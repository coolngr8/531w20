%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{9}


\newcommand\eqspace{\quad\quad\quad}
\newcommand\eqskip{\vspace{2.5mm}}

\newcommand\ev{u}

% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{{\color{blue}{#2}}} % to show answers
\newcommand\answer[2]{#1} % to show blank space

<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
 ANS=FALSE
@

\usepackage{bbm} % for blackboard bold 1

\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping

@

<<setup,echo=F,results=F,cache=F>>=
myround<- function (x, digits = 1) {
  # taken from the broman package
  if (digits < 1) 
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

set.seed(2050320976)

options(
  keep.source=TRUE,
  encoding="UTF-8"
)

@

\begin{frame}[fragile]
\frametitle{Chapter \CHAPTER. Introduction to partially observed Markov process models}

\hspace{3cm} {\large \bf Objectives}

\vspace{3mm}

\begin{enumerate}

\item Develop a framework for thinking about models that consist of a stochastic dynamic system observed with noise.

\item In the linear Gaussian case, develop matrix operations to find an exact and computationally fast algorithm for the likelihood function. This algorithm is called the \myemph{Kalman filter}.

\item Understand how the Kalman filter is used to compute the likelihood for ARMA models.

\item See how the Kalman filter also facilitates forecasting and estimation of the state of the unobserved process.

\item Start to investigate the general nonlinear filtering equations.

\end{enumerate}

\end{frame}


\begin{frame}


\frametitle{Partially observed Markov processes (POMP) models}

\bi

\item  Uncertainty and variability are ubiquitous features of processes in the biological and social sciences. A physical system obeying Newton's laws is fully predictable, but complex systems are in practice not perfectly predictable---we can only forecast weather reliably in the near future.

\item  Basic time series models of deterministic trend plus colored noise imply perfect predictability if the trend function enables extrapolation.

\item  To model variability and unpredictability in low frequency components, we may wish to specify a random process model for how the system evolves. We could call this a ``stochastic trend'' approach, though that is an oxymoron since we've defined trend to be expected value for the model.

\item  As in the deterministic signal plus noise model, we will model the observations as random variables conditional on the trajectory of a \myemph{latent process}. It can also be called a \myemph{state process} or a \myemph{hidden process}. 

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{The Markov property}

\bi

\item  A standard class of latent process models is characterized by the requirement that the future evolution of the system depends only on the current state, plus randomness introduced in future.

\item A model of this type is called a \myemph{Markov chain} for a discrete time model or a  \myemph{Markov process} in continuous time.

\item We use the term Markov process for both discrete and continous time.

\item  \myemph{Partial observations} here mean either or both of (i) measurement noise; (ii) entirely unmeasured latent variables. Both these features are present in many systems.

\item  A \myemph{partially observed Markov process} (POMP) model is defined by putting together a latent process model and an observation model.

\ei

\end{frame}

\begin{frame}[fragile]

\bi

\item  Often, much of the scientific interest is in understanding what models for the behavior of this latent process are consistent with the data. 

\vspace{3mm}

\item  A good model for the underlying, but imperfectly observed, dynamics of a system can also lead to a skillful forecast.

\vspace{3mm}

\item  We are going to introduce a general framework for specifying POMP models. This generality will give us the flexibility to develop models and methods appropriate to a range of applications.
\ei

\end{frame}

\begin{frame}

\frametitle{Discrete time Markov processes}


A time series model $X_{0:N}$ is a \myemph{Markov process} model if the conditional densities satisfy the \myemph{Markov property} that, for all $n\in 1:N$. 

%\eqskip
\vspace{5mm}

[MP1] $\eqspace f_{X_n|X_{1:n-1}}(x_n\given x_{1:n-1}) = f_{X_n|X_{n-1}}(x_n\given x_{n-1})$,


\eqskip
\vspace{5mm}

 We suppose that the random process $X_n$ occurs at time $t_n$ for $n\in 0:N$, so the discrete time process corresponds to time points in continuous time. 


\end{frame}

\begin{frame}[fragile]

\frametitle{Initial conditions}

\bi

\item  We have \myemph{initialized} the Markov process model at a time $t_0$, although we will suppose that data are collected only at times $t_{1:N}$.

\item The initialization model could be deterministic (a fixed value) or a random variable. 

\item Formally, a fixed initial value is a special case of a discrete distribution having a point mass with probability one at the fixed value. Therefore, fixed initial values are covered in our framework since we use probability density functions to describe both discrete and continuous probability distributions.

\item Mathematically, a probability mass function (for discrete distributions) is a probability density on a discrete space.  We avoid getting sidetracked on to that topic, but it is worth noting that there is a proper mathematical justification for treating a probability mass function as a type of probability density function.

\item It is not important whether to adopt the convention that the Markov process model is intialized at time $t_1$ or at some previous time $t_0$. Here, we follow the choice to use $t_0$.

\ei

\end{frame}


\begin{frame}[fragile]

\frametitle{The process nodel and one-step predictions}

\bi

\item  The probability density function $f_{X_n|X_{n-1}}(x_n\given x_{n-1})$ is called the \myemph{one-step transition density} of the Markov process.

\item  In words, the Markov property says that the next step taken by a Markov process follows the one-step transition density based on the current state, whatever the previous history of the process.

\item  For a POMP model, the full joint distribution of the latent process  is entirely specified by the one-step transition densities, given the initial value.
We show this below.

\item Therefore, we also call $f_{X_n|X_{n-1}}(x_n\given x_{n-1})$ the \myemph{process model}.

\ei

\end{frame}



\begin{frame}


%\frametitle{Writing the joint distribution in terms of the one-step transition densities.}



\myquestion. Use [MP1] to derive an expression for the joint distribution of a Markov process as a product of the one-step transition densities. In other words, derive 

\eqskip

[MP2] $\eqspace f_{X_{0:N}}(x_{0:N}) = f_{X_0}(x_0)\prod_{n=1}^N f_{X_n|X_{n-1}}(x_n\given x_{n-1})$.

\answer{\vspace{50mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\myquestion. Show that a causal Gaussian AR(1) process is a Markov process.

\answer{\vspace{60mm}}{todo}

\end{frame}

\begin{frame}

\frametitle{Time homogeneous transitions and stationarity}

 In general, the one step transition probability density in a POMP model can depend on $n$. 
 A latent process model $X_{0:N}$ is \myemph{time-homogeneous} if the  one step transition probability density does not depend on $n$, so there is a conditional density $f(y\given x)$ such that, for all $n\in 1:N$,
$$  f_{X_n|X_{n-1}}(x_n\given x_{n-1})= f(x_n\given x_{n-1}).$$


\myquestion. If $X_{0:N}$ is stationary then it is time homogeneous. Why?

\answer{\vspace{20mm}}{todo}

\myquestion. Time homogeneity does not necessarily imply stationarity. Find a counter-example. What has to be added to time homogeneity to get stationarity?

\answer{\vspace{40mm}}{todo}

\end{frame}

\begin{frame}

\frametitle{The measurement model}

\bi
\item  We model the observation process random variables $Y_{1:N}$.

\item  For state space models, we will generally write the data as $\data{y_{1:N}}$.

\item  We model the measurement at time $t_n$ to depend only on the value of the latent process at time $t_n$, conditionally independent of all other latent process and observation process variables. Formally, this assumption is,

\ei

\eqskip

[MP3] $ \hspace{2mm}
%\eqspace
f_{Y_n|X_{0:N},Y_{1:n-1},Y_{n+1:N}}(y_n\given x_{0:N},y_{1:n-1},y_{n+1:N}) = f_{Y_n|X_n}(y_n\given x_{n})
$.

\eqskip

\bi
\item  We call $f_{Y_n|X_n}(y_n\given x_{n})$ the \myemph{measurement model}.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Time-inhomoegeneous measurement models}

\bi
\item  In general, the measurement model can depend on $n$ or on any covariate time series.


\item  The measurement model is \myemph{time-homogeneous} if there is a conditional probability density function $g(y\given x)$ such that, for all $n\in 1:N$,
$$ f_{Y_n|X_n}(y_n\given x_{n})= g(y_n\given x_n).$$

\ei

\end{frame}

\begin{frame}

\frametitle{Four basic calculations for working with POMP models}

Many time series models in science, engineering and industry can be written as POMP models.
A reason that POMP models form a useful tool for statistical work is that there are convenient recursive formulas to carry out four basic calculations:

\begin{enumerate}
\item Prediction
\item Filtering
\item Smoothing
\item Likelihood calculation
\end{enumerate}

\end{frame}

\begin{frame}

\frametitle{Prediction}

\bi

\item  One-step prediction of the latent process at time $t_{n+1}$ given data up to time $t_n$ involves finding
$$ f_{X_{n+1}|Y_{1:n}}(x_{n+1}\given \data{y_{1:n}}).$$

\item  We may want to carry out prediction (also called forecasting) more than one time step ahead. However, unless specified otherwise, the prediction calculation will be one-step prediction. 

\item  One-step prediction turns out to be closely related to computing the likelihood function, and therefore central to statistical inference.

\item  We have required our prediction to be a conditional probability density, not a point estimate. In the context of forecasting, this is called a \myemph{probabilistic forecast}, and has advantages over a point estimate forecast. What are they? Are there any disadvantages to probabilistic forecasting?

\ei

\end{frame}

\begin{frame}


\frametitle{Filtering}

\bi

\item  The filtering calculation at time $t_n$ is to find the conditional distribution of the latent process $X_n$ given currently available data, $\data{y_{1:n}}$.

\vspace{3mm}

\item  Filtering therefore involves calculating
$$f_{X_{n}|Y_{1:n}}(x_n\given \data{y_{1:n}}).$$

\vspace{3mm}

\item This can be calculated numerically or algebraically. We will also see that Monte Carlo methods can be a good tool.

\ei

\end{frame}



\begin{frame}

\frametitle{Smoothing}

\bi
\item  In the context of a POMP model, smoothing involves finding the conditional distribution of $X_n$ given all the data, $\data{y_{1:N}}$.

\vspace{3mm}

\item  So, the smoothing calculation is to find
$$f_{X_{n}|Y_{1:N}}(x_n\given \data{y_{1:N}}).$$

\ei

\end{frame}

\begin{frame}


\frametitle{The likelihood}

\bi

\item  The model may depend on a parameter vector $\theta$.

\vspace{3mm}

\item  Since we have not explicitly written this dependence above, the likelihood calculation is to evaluate the joint density of $Y_{1:N}$ at the data,
$$f_{Y_{1:N}}(\data{y_{1:N}}).$$

\vspace{3mm}

\item  If we can compute this at any value of $\theta$ we choose, we can perform numerical optimization to get a maximum likelihood estimate

\item Likelihood evaluation and maximization lets us compute profile likelihood confidence intervals, carry out likelihood ratio tests, and make AIC model comparisons.

\ei

\end{frame}

\begin{frame}

\frametitle{The prediction and filtering formulas}

\bi
\item One-step prediction of the latent process at time $t_{n}$ given data up to time $t_{n-1}$ can be computed in terms of the filtering problem at time $t_{n-1}$, via the \myemph{prediction formula} for $n\in 1:N$,

\ei


\eqskip

[MP4] $\eqspace \displaystyle f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}})$

\eqskip

$\eqspace \displaystyle = \int f_{X_{n-1}|Y_{1:n-1}}(x_{n-1}\given \data{y_{1:n-1}}) 
f_{X_{n}|X_{n-1}}(x_{n}\given x_{n-1})\, dx_{n-1}$.

\eqskip

\bi
\item To make this formula work for $n=1$, we need the convention that $1:k$ is the empty set when $k=0$. Conditioning on an empty collection of random variables is the same as not conditioning at all! In this case, we have by definition that
$$ f_{X_{0}|Y_{1:0}}(x_{0}\given \data{y_{1:0}}) = f_{X_0}(x_0).$$

\item In other words, the filtering calcuation at time $t_0$ is the initial density for the latent process. This makes sense, since at time $t_0$ we have no data to condition on.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Hints for homework: deriving the recursion formulae}

 Any general identity holding for densities must also hold when we condition everything on a new variable.

\eqskip

{\bf Example 1}. From
$$f_{XY}(x,y) = f_X(x)\, f_{Y|X}(y\given x)$$
we can condition on $Z$ to obtain 
$$f_{XY|Z}(x,y\given z) = f_{X|Z}(x\given z)\, f_{Y|XZ}(y\given x,z).$$


{\bf Example 2}. the prediction formula is a special case of the identity
$$ f_{X|Y}(x\given y)= \int f_{XZ|Y}(x,z\given y)\, dz.$$

\myquestion. Why is the following identity true?
$$f_{X|YZ}(x\given y,z)= \frac{ f_{Y|XZ}(y\given x,z)\, f_{X|Z}(x\given z)}{f_{Y|Z}(y\given z)}.$$


\end{frame}

\begin{frame}[fragile]

\bi
\item  Filtering at time $t_n$ can be computed by combining the new information in the datapoint $\data{y_{n}}$ with the calculation of the one-step prediction of the latent process at time $t_{n}$ given data up to time $t_{n-1}$.

\item

This is carried out via the \myemph{filtering formula} for $n\in 1:N$,

\ei
\eqskip

[MP5] $\eqspace \displaystyle f_{X_{n}|Y_{1:n}}(x_{n}\given \data{y_{1:n}})
= \frac{
f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}})\, 
f_{Y_n|X_n}(\data{y_n}\given x_n)
}{
f_{Y_{n}|Y_{1:n-1}}(\data{y_n}\given \data{y_{1:n-1}})
}$.

\eqskip



\end{frame}

\begin{frame}[fragile]

\bi

\item  The denominator in the filtering formula [MP5] is the \myemph{conditional likelihood} of $\data{y_{n}}$ given  $\data{y_{1:n-1}}$.

\item It can be computed in terms of the one-step prediction density, via the \myemph{conditional likelihood formula},

\ei
\eqskip

[MP6] $\eqspace  f_{Y_{n}|Y_{1:n-1}}(\data{y_n}\given \data{y_{1:n-1}})
= $

\eqskip

$ \hspace{42mm} \displaystyle
\int 
f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}})\, f_{Y_n|X_n}(\data{y_n}\given x_n)\, dx_n$.

\eqskip
\bi

\item  To make this formula work for $n=1$, we again take advantage of the convention that $1:k$ is the empty set when $k=0$. 

\ei

\end{frame}

\begin{frame}

\bi

\item  The prediction and filtering formulas are \myemph{recursive}. If they can be computed for time $t_n$ then they provide the foundation for the following computation at time $t_{n+1}$.

\ei

\myquestion. Give a detailed derivation of [MP4], [MP5] and [MP6], being careful to note when you use the Markov property [MP1].

\answer{\vspace{40mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\frametitle{Computation of the likelihood}

\bi

\item  The likelihood of the entire dataset, $\data{y_{1:N}}$ can be found from [MP6], using the identity

\eqskip

[MP7] $\eqspace\displaystyle f_{Y_{1:N}}(\data{y_{1:N}})
= \prod_{n=1}^N  f_{Y_{n}|Y_{1:n-1}}(\data{y_n}\given \data{y_{1:n-1}})$.

\eqskip

\item  As above, this formula [MP7] requires the convention that $1:k$ is the empty set when $k=0$, so the first term in the product is
$$f_{Y_{1}|Y_{1:0}}(\data{y_1}\given \data{y_{1:0}}) = 
f_{Y_{1}}(\data{y_1}).$$

\item  If our model has an unknown parameter $\theta$, the likelihood identity [MP7] lets us evaluate the log likelihood function,
$$\loglik(\theta)=\log f_{Y_{1:N}}(\data{y_{1:N}}\params\theta).$$

\ei

\end{frame}

\begin{frame}

\frametitle{The smoothing formulas}

\bi

\item  Smoothing is less fundamental for likelihood-based inference than filtering and one-step prediction. 

\item  Nevertheless, sometimes we want to compute the smoothing density, so we develop some necessary formulas.

\item  The filtering and prediction formulas are recursions forwards in time (we use the solution at time $t_{n-1}$ to carry out the computation at time $t_{n}$).

\item  There are similar \myemph{backwards recursion formulas},

\ei

\vspace{4mm}

[MP8] $\hspace{5mm} f_{Y_{n:N}|X_{n}}(\data{y_{n:N}}\given x_n)= f_{Y_n|X_n}(\data{y_n}\given x_n)
f_{Y_{n+1:N}|X_{n}}(\data{y_{n+1:N}}\given x_n)$.

\vspace{5mm}

[MP9] $\hspace{3mm} 
f_{Y_{n+1:N}|X_{n}}(\data{y_{n+1:N}}\given x_n)$

\eqskip

$ \hspace{20mm}\displaystyle = \int f_{Y_{n+1:N}|X_{n+1}}(\data{y_{n+1:N}}\given x_{n+1}) \, 
f_{X_{n+1}|X_n}(x_{n+1}\given x_n)\, dx_{n+1}$.

\eqskip

\end{frame}


\begin{frame}

The forwards and backwards recursion formulas together allow us to compute the \myemph{smoothing formula},


\vspace{4mm}

[MP10] $ \hspace{3mm} \displaystyle
f_{X_{n}|Y_{1:N}}(x_n\given \data{y_{1:N}})
= \frac{
f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}}) \, f_{Y_{n:N}|X_{n}}(\data{y_{n:N}}\given x_n)
}{
f_{Y_{n:N}|Y_{1:n-1}}(\data{y_{n:N}}\given \data{y_{1:n-1}})
}
$.

\vspace{4mm}

\myquestion. Show how [MP8], [MP9] and [MP10] follow from the basic properties of conditional densities combined with the Markov property.

\answer{\vspace{40mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\frametitle{An algebraic trick: Using  un-normalized identities}

\bi

%\item  Some common Monte Carlo algorithms (Markov chain Monte Carlo and self-normalized importance sampling) need probability density functions only up to an unknown constant factor. These algorithms depend on ratios of densities, for which a constant factor cancels out and so does not have to be computed.

\item  Sometimes we can avoid calculating a normalizing constant for a density, since it can be worked out later using the property that the probability density function must integrate to 1.

\item  The denominators $f_{Y_{n}|Y_{1:n-1}}(\data{y_n}\given \data{y_{1:n-1}})$ and $f_{Y_{n:N}|Y_{1:n-1}}(\data{y_{n:N}}\given \data{y_{1:n-1}})$, in equations [MP5] and [MP10]  respectively, may sometimes be hard to compute. 

\item   We can simplify [MP5] and [MP10] using the proportionality relationship $\propto$. This gives,

\ei

\eqskip

[MP5$^\prime$] $\vspace{7mm}
f_{X_{n}|Y_{1:n}}(x_{n}\given \data{y_{1:n}})
\propto 
f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}})\, f_{Y_n|X_n}(\data{y_n}\given x_n)$,

\eqskip

[MP10$^\prime$] $\vspace{3mm}
f_{X_{n}|Y_{1:N}}(x_n\given \data{y_{1:N}})
\propto
f_{X_{n}|Y_{1:n-1}}(x_{n}\given \data{y_{1:n-1}}) \, f_{Y_{n:N}|X_{n}}(\data{y_{n:N}}\given x_n)
$.

\eqskip

\bi
\item The normalizing ``constant'' avoided in equations [MP5'] and [MP10'] does depend on $\data{y_{1:N}}$. However, the data are fixed constants. The variable in these equations is $x_n$.
\ei

\end{frame}


\begin{frame}

\frametitle{Linear Gaussian POMP (LG-POMP) models}

\bi

\item  Linear Gaussian partially observed Markov process (LG-POMP) models have many applications

\item Gassian ARMA models are LG-POMP models. The POMP recursion formulas give a computationally efficient way to obtain the likelihood of a Gaussian ARMA model. 

\item The computations for smoothing splines can be written as an LG-POMP model, enabling computationally efficient spline smooothing.

\item The \myemph{Basic Structural Model} is an LG-POMP used for econometric forecasting. It models a stochastic trend, seasonality, and measurement error, in a framework with econometrically interpretable parameters. This is more interpretable than fitting SARIMA.

\item LG-POMP models are widely used in engineering, especially for control applications. 
 If a scientific and engineering application is not too far from linear and Gaussian, you save a lot of effort if an LG-POMP model is appropriate. General nonlinear POMP models usually involve intensive Monte Carlo computation.

\ei

\end{frame}

\begin{frame}

\frametitle{The general LG-POMP model}


Suppose the latent process, $X_{0:N}$, and the observation process $\{Y_n\}$, takes vector values with dimension $d^{}_X$ and $d^{}_Y$. A general mean zero LG-POMP model is specified by 

\bi

\item A sequence of $d_X\times d_X$ matrices, $\matA_{1:N}$,

\item A sequence of  $d_X\times d_X$ covariance matrices, $\covmatX_{0:N}$,

\item A sequence of $d_Y\times d_X$ matrices, $\matB_{1:N}$

\item A sequence of  $d_Y\times d_Y$ covariance matrices, $\covmatY_{1:N}$.

\ei

We initialize with $X_0\sim N[0,\covmatX_0]$ and then define the entire LG-POMP model by a recursion for $n\in 1:N$,

\eqskip

[LG1] $\eqspace X_{n} = \matA_n X_{n-1} + \epsilon_n$, $\eqspace \epsilon_n\sim N[0,\covmatX_n]$,

\eqskip

[LG2] $\eqspace Y_{n} = \matB_n X_n + \eta_n$, $\eqspace \eta_n\sim N[0,\covmatY_n]$.

\eqskip

Often, but not always, we will have a \myemph{time-homogeneous} LG-POMP model, with $\matA_n=\matA$, $\;\matB_n=\matB$, $\;\covmatX_n=\covmatX$ and $\covmatY_n=\covmatY$ for $n\in 1:N$.

\end{frame}

\begin{frame}

\frametitle{The LG-POMP representation of a Gaussian ARMA}

 Suppose $\{Y_n\}$ is a Gaussian ARMA(p,q) model with noise process $\omega_n\sim N[0,\sigma^2]$ and specification

\eqskip

[LG3] $\eqspace\displaystyle Y_n = \sum_{j=1}^p \ar_j Y_{n-j} +  \omega_n + \sum_{k=1}^q \ma_q \omega_{n-k}.$

\eqskip

Set $r=\max(p,q+1)$ so that $\{Y_n\}$ is also ARMA(r,r-1).
Our LG-POMP representation has $d_X=r$, with
$$\matB_n = \matB = (1,0,0,\dots,0)$$
and 
$$\covmatY_n = \covmatY = 0.$$

Therefore, $Y_n$ is the first component of $X_n$, observed without measurement error.

\end{frame}

\begin{frame}[fragile]

  Now, define
$$ X_n = \left(\begin{array}{l}
Y_n \\
\ar_2 Y_{n-1} + \dots + \ar_r Y_{n-r+1} + \ma_1 \omega_n + \dots +\ma_{r-1} \omega_{n-r+2}
\\
\ar_3 Y_{n-1} + \dots + \ar_r Y_{n-r+1} + \ma_2 \omega_n + \dots +\ma_{r-1} \omega_{n-r+3}
\\
\vdots
\\
\ar_r Y_{n-1} + \ma_{r-1} \omega_t
\end{array}\right)$$

We can check that the ARMA equation [LG3] corresponds to the matrix equation

%\vspace{3mm}

$\displaystyle X_{n} = \matA X_{n-1} + \left(\begin{array}{l}
1 \\
\ma_1\\
\ma_2\\
\vdots\\
\ma_{r-1}
\end{array}\right) \omega_n.
\mbox{
where 
}
\matA = 
\left(\begin{array}{ccccc}
\ar_1    & 1     & 0      & \ldots & 0 \\
\ar_2    & 0     & 1      & \ddots       & \vdots \\
\vdots   & \vdots& \ddots & \ddots & 0 \\
\ar_{r-1}& 0     & \ldots & 0      & 1 \\
\ar_{r}  & 0     & \ldots & 0      & 0
\end{array}\right)$

\vspace{2mm}

This is in the form of a time-homogenous LG-POMP, with $\matA$, $\matB$ and $\covmatY$ defined above, and
$$\covmatX_n = \covmatX = \sigma^2 (1, \ma_1,\ma_2,\dots,\ma_{r-1})^\transpose(1, \ma_1,\ma_2,\dots,\ma_{r-1}).$$

\end{frame}

\begin{frame}[fragile]

\frametitle{Different POMPs can give the same model for $Y_{1:N}$}

\bi

\item  There are other LG-POMP representations giving rise to the same ARMA model.

\item  When only one component of a latent process is observed, any model giving rise to the same observed component is indistinguishable from the data.

\item  Here, the LG-POMP model has order $r^2$ parameters and the ARMA model has order $r$ parameters, so we might expect there are many ways to parameterize the ARMA model as a special case of the much larger LG-POMP model.

\item The same can be true of non-Gaussian POMPs, but it is easier to see in the Gaussian case.

\ei

\end{frame}

\begin{frame}

\frametitle{The basic structural model and its LG-POMP representation}

\bi

\item  The \myemph{basic structural model} is an econometric model used for forecasting.

\item  The basic stuctural model supposes that the observation process $Y_{1:N}$ is the sum of a \myemph{level} ($L_n$),  a \myemph{trend} ($T_n$) describing the rate of change of the level, and a monthly \myemph{seasonal component} ($S_n$).

\item The model supposes that all these quantities are perturbed with Gaussian white noise at each time point. So, we have the following model equations

\eqskip

$\begin{array}{lrcl}
\mbox{[BSM1]} \eqspace & Y_n &=& L_n + S_n + \epsilon_n \\
\mbox{[BSM2]} \eqspace & L_{n} &=& L_{n-1} + T_{n-1} + \xi_n \\
\mbox{[BSM3]} \eqspace & T_{n} &=& T_{n-1} + \zeta_n \\
\mbox{[BSM4]} \eqspace & S_{n} &=& -\sum_{k=1}^{11} S_{n-k} + \eta_n
\end{array}$

\eqskip

\item  We suppose $\epsilon_n\sim N[0,\sigma^2_\epsilon]$,  $\xi_n\sim N[0,\sigma^2_\xi]$,  $\zeta_n\sim N[0,\sigma^2_\zeta]$, and $\eta_n\sim N[0,\sigma^2_\eta]$.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Two common special cases of the basic structural model}

\bi

\item  The \myemph{local linear trend} model is the basic structural model without the seasonal component, $\{S_n\}$

\vspace{5mm}

\item  The \myemph{local level model} is the basic structural model without either the seasonal component, $\{S_n\}$, or the trend component, $\{T_n\}$. The local level model is therefore a random walk observed with measurement error.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Initial values for the basic structural model}

\bi
\item  To complete the model, we need to specify initial values.

\item We have an example of the common problem of failing to specify initial values: these are not explained in the documentation of the  R implementation of the basic structural model, `StructTS`. We could go through the source code to find out what it does.

\item  Incidentally, `?StructTS` does give some advice which resonates with our experience earlier in the course that optimization for ARMA models is often imperfect.

\ei

\vspace{2mm}

     ``Optimization of structural models is a lot harder than many of the
     references admit.  For example, the `AirPassengers’ data are
     considered in Brockwell \& Davis (1996): their solution appears to
     be a local maximum, but nowhere near as good a fit as that
     produced by `StructTS’.  It is quite common to find fits with one
     or more variances zero, and this can include sigma$^2_{\mathrm{eps}}$.''



\end{frame}

\begin{frame}[fragile]

To put [BSM1-4] in the form of an LG-POMP model, we set

\eqskip

[BSM5] $\eqspace X_n = (L_n,T_n,S_n, S_{n-1}, S_{n-2}, \dots,S_{n-10})^\transpose$.

\eqskip

Then, we have

\eqskip

[BSM6] $\eqspace Y_n = (1,0,1,0,0,\dots, 0) X_n + \epsilon_n$,

\eqskip

$\hspace{-1mm} \displaystyle
 \left(\begin{array}{l}
L_{n} \\
T_{n} \\
S_{n} \\
S_{n-1}\\
S_{n-2} \\
 \vdots \\
S_{n-10}
\end{array} \! \right)
=
\left(\begin{array}{ccccccc}
1      & 1 & 0  & 0     & 0      & \ldots & 0 \\
0      & 1 & 0  & 0     & 0      & \ldots & 0 \\
0      & 0 & -1 & -1    &  -1    & \ldots & -1\\
0      & 0 & 1  & 0     &  0     & \ldots & 0 \\
0      & 0 & 0  & 1     &  0     & \ldots & 0 \\
\vdots &   &    &\ddots & \ddots & \ddots & \vdots \\
0      & 0 & 0  &\ldots & 0      & 1      & 0   
\end{array}\right)
\left(\begin{array}{l}
L_{n-1} \\
T_{n-1} \\
S_{n-1} \\
S_{n-2}\\
S_{n-3} \\
 \vdots \\
S_{n-11}
\end{array} \! \right)
+
\left(\begin{array}{l}
\xi_n \\
\zeta_n \\
\eta_n \\
0 \\
0 \\
 \vdots \\
0
\end{array}\right)$

\eqskip

From [BSM5] and [BSM6], we can read off the matrices $\matA$, $\matB$, $\covmatX$ and $\covmatY$ in the LG-POMP representation of the basic structural model.

\end{frame}

\begin{frame}

\frametitle{Spline smoothing and its LG-POMP representation}

\bi

\item  Spline smoothing is a standard method to smooth scatter plots and time plots. For example, \code{smooth.spline} in R.

\vspace{2mm}

\item  A \myemph{smoothing spline} for an equally spaced time series $\data{y_{1:N}}$ collected at times $t_{1:N}$ is the sequence $x_{1:N}$ minimizing the \myemph{penalized sum of squares (PSS)}, which is defined as

\ei

\vspace{2mm}

[SS1] $\eqspace{
  \mathrm{PSS}(x_{1:N}\params\lambda)
  = \sum_{n=1}^N(\data{y_n}-x_n)^2 + \lambda\sum_{n=3}^N(\Delta^2 x_n)^2
}$.

\vspace{2mm}

\bi

\item  The spline is defined for all times, but here we are only concerned with its value at the times $t_{1:N}$. 

\vspace{2mm}

\item  Here, $\Delta x_n = (1-B)x_n = x_n - x_{n-1}.$

\ei

\end{frame}

\begin{frame}[fragile]
\bi
\item  The \myemph{smoothing parameter}, $\lambda$, penalizes $x_{1:N}$ to prevent the spline from interpolating the data.

\item  If $\lambda=0$, the spline will go through each data point, i.e, $x_{1:N}$ will interpolate $\data{y_{1:N}}$.

\item  If $\lambda=\infty$, the spline will be the ordinary least squares regression fit,
$$ x_n = \alpha + \beta n,$$
since $\Delta^2(\alpha + \beta n) = 0$.

\item  Now consider the model,

\ei

\eqskip

[SS2] $\eqspace 
\begin{array}{rclcl}
X_n &=& 2X_{n-1}-X_{n-2} + \epsilon_n, & & \epsilon_n\sim \mathrm{iid}\; N[0,\sigma^2/\lambda]\\
Y_n &=& X_n + \eta_n & & \eta_n\sim \mathrm{iid}\; N[0,\sigma^2].
\end{array}$

\eqskip
\bi
\item  Note that $\Delta^2 X_n = \epsilon_n$.
\ei
\end{frame}

\begin{frame}

\frametitle{Constructing a linear Gaussian POMP (LG-POMP) model from [SS2]}

\myquestion. $\{X_n,Y_n\}$ defined in [SS2] is not quite an LG-POMP model. However, we can use  $\{X_n\}$ and $\{Y_n\}$ to build an LG-POMP model. How?

\answer{\vspace{50mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\bi

\item  The joint density of $X_{1:N}$ and $Y_{1:N}$ in [SS2] can be written as
$$ f_{X_{1:N}Y_{1:N}}(x_{1:N},y_{1:N})= f_{X_{1:N}}(x_{1:N}) \, f_{Y_{1:N}|X_{1:N}}(y_{1:N}\given x_{1:N}).$$

\item  Taking logs, 
$$ \hspace{-2mm} \log f_{X_{1:N}Y_{1:N}}(x_{1:N},y_{1:N})= \log f_{X_{1:N}}(x_{1:N}) + \log f_{Y_{1:N}|X_{1:N}}(y_{1:N}\given x_{1:N}).$$

\item  Suppose the initial conditions are irrelevant (either unknown parameters or an improper Gaussian distribution with infinite variance).  Noting that $\{\Delta^2 X_{n}, n\in 1:N\}$ and $\{Y_n-X_n, n\in 1:N\}$ are collections of independent Normal random variables with mean zero and variances $\sigma^2/\lambda$ and $\sigma^2$ respectively, we have

\ei

\eqskip

[SS3] $ \hspace{3mm}
\log f_{X_{1:N}Y_{1:N}}(x_{1:N},y_{1:N} \params \sigma,\lambda) =
$

\eqskip

$\hspace{35mm}
\frac{-1}{2\sigma^2} \sum_{n=1}^N(y_n-x_n)^2 +\frac{-\lambda}{2\sigma^2}  \sum_{n=3}^N(\Delta^2 x_n)^2 + C
$.

\eqskip

\bi

\item  In [SS3], $C$ is a constant depending on $\sigma$ and $\lambda$ but not $x_{1:N}$ or $y_{1:N}$.

\item  Comparing [SS3] with [SS1], we see that maximizing the density $f_{X_{1:N}Y_{1:N}}(x_{1:N},\data{y_{1:N}} \params \sigma,\lambda)$ as a function of $x_{1:N}$ is the same problem as finding the smoothing spline by minimizing the penalized sum of squares in [SS1].


\ei

\end{frame}

\begin{frame}[fragile]

\bi

\item  For a Gaussian density, the mode (i.e., the maximum of the density) is equal to the expected value. Therefore, we have

\begin{eqnarray*}
\arg\min_{x_{1:N}} \mathrm{PSS}(x_{1:N}\params\lambda),
&=& 
\arg\max_{x_{1:N}} f_{X_{1:N}Y_{1:N}}(x_{1:N},\data{y_{1:N}}\params \sigma,\lambda),
\\
&=&
\arg\max_{x_{1:N}} \frac{
  f_{X_{1:N}Y_{1:N}}(x_{1:N},\data{y_{1:N}} \params \sigma,\lambda)
}{
  f_{Y_{1:N}}(\data{y_{1:N}} \params \sigma,\lambda)
},
\\
&=& 
\arg\max_{x_{1:N}} f_{X_{1:N}|Y_{1:N}}(x_{1:N}\given \data{y_{1:N}}\params \sigma,\lambda),
\\
&=& \E\big[X_{1:N}\given Y_{1:N}=\data{y_{1:N}} \params  \sigma,\lambda\big].
\end{eqnarray*}

\ei

\end{frame}

\begin{frame}

\bi

\item  The smoothing calculation for an LG-POMP model involves finding the mean and variance of $X_{n}$ given $Y_{1:N}=\data{y_{1:N}}$.

\item  We conclude that the smoothing problem for this LG-POMP model is the same as the spline smoothing problem defined by [SS1].

\item  If you have experience using smoothing splines, this connection may help you transfer that experience to POMP models.

\item  Once you have experience with POMP models, this connection helps you understand spline smoothers that are commonly used in many applications.

\item  For example, we might propose that the smoothing parameter $\lambda$ could be selected by maximum likelihood for the POMP model.

\ei

\end{frame}


\begin{frame}

\myquestion. Why do we use $\Delta^2 X_n=\epsilon_n$ for our smoothing model?

\bi

\item  Seeing that the smoothing spline arrives from the particular choice of LG-POMP model in equation [SS2] could make you wonder why we choose that model.
Any ideas?
\ei

\answer{\vspace{30mm}}{todo}
\bi
\item  Even if this LG-POMP model is sometimes reasonable, presumably there are other occasions when a different LG-POMP model would be a superior choice for smoothing.
\ei

\end{frame}

\begin{frame}

\frametitle{The Kalman filter}

\bi

\item  We find exact versions of the prediction, filtering and smoothing formulas [MP4--10] for the linear Gaussian partially observed Markov process (LG-POMP) model [LG1,LG2].

\vspace{5mm}

\item  In the linear Gaussian case, the conditional probability density functions in [MP4--10] are specified by the conditional mean and conditional variance.
\ei

\end{frame}


\begin{frame}[fragile]

\frametitle{Review of the multivariate normal distribution}



\bi
\item  A random variable $X$ taking values in $\R^{d_X}$ is \myemph{multivariate normal} with mean $\mu_X$ and variance $\Sigma_X$ if we can write
\begin{equation*}X = \matH Z + \mu_X,\end{equation*}
where $Z$ is a vector of $d_X$ independent identically distributed $N[0,1]$ random variables and $\matH$ is a $d_X\times d_X$ matrix square root of $\Sigma_X$, i.e.,
\begin{equation*}\matH\matH^\transpose = \Sigma_X.\end{equation*}

\item  The choice of $\matH$ is not unique, and a matrix square root of this type exists for any covariance matrix because covariance matrices are positive semi-definite.

\item  We write $X\sim N\big[\mu_X,\Sigma_X\big]$.

\item $X\sim N\big[\mu_X,\Sigma_X\big]$ has a probability density function if and only if $\Sigma_X$ is invertible. This density is given by

\vspace{-2mm}
\begin{equation*}
f_X(x) = \frac{1}{(2\pi)^{d_X/2}|\Sigma_X|}
\exp
  \left\{-
     \frac{ (x - \mu_X)\, \big[\Sigma_X\big]^{-1} \, (x - \mu_X)^\transpose}{2}
  \right\}.
\end{equation*}

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Jointly multivariate normal vectors}


$X$ and $Y$ are \myemph{jointly multivariate normal} if the combined vector 
\begin{equation*}
W=\left(
\begin{array}{l}
X \\
Y
\end{array}
\right)
\end{equation*}
is multivariate normal. In this case, we write

\begin{equation*}
\mu_W = \left( \begin{array}{l}
\mu_X \\
\mu_Y
\end{array}\right),
\eqspace
\Sigma_W = \left(\begin{array}{cc}
\Sigma_X & \Sigma_{XY}\\
\Sigma_{YX} & \Sigma_Y
\end{array}\right),
\end{equation*}
where 
\begin{equation*}\Sigma_{XY}= \cov(X,Y) = \E\big[(X-\mu_X)(Y-\mu_Y)^\transpose\big].\end{equation*}

\end{frame}


\begin{frame}

\bi

\item  For jointly multivariate normal random variables $X$ and $Y$, we have the useful property that the conditional distribution of $X$ given $Y=y$ is multivariate normal, with conditional mean and variance

\eqskip

[KF1] $\eqspace \begin{array}{rcl}
\mu_{X|Y}(y) &=& \mu_X + \Sigma_{XY}\Sigma_Y^{-1}\big(y-\mu_Y\big),
\vspace{3mm}
\\
\Sigma_{X|Y} &=& \Sigma_X - \Sigma_{XY}\Sigma_Y^{-1}\Sigma_{YX}.
\end{array}$

\item  We write this as
\begin{equation*} X\given Y=y \sim N\big[\, \mu_{X|Y}(y)\, ,\Sigma_{X|Y}\big]. \end{equation*}

\item  In general, the conditional variance of $X$ given $Y=y$ will depend on $y$ (remind yourself of the definition of conditional variance). In the special case where $X$ and $Y$ are jointly multivariate normal, this conditional variance happens not to depend on the value of $y$.

\item  If $\Sigma_Y$ is not invertible, to make [KF1] work we have to interpret $\Sigma_Y^{-1}$ as a generalized inverse.

\ei

\end{frame}

\begin{frame}

\frametitle{Notation for the Kalman filter recursions}

To write the Kalman filter, we define the following notation,
Since the system is Gaussian, the filtering and prediction distributions are defined by their mean and variance.

\vspace{4mm}

[KF2] $\eqspace \begin{array}{lcl}
X_{n}\given Y_{1:n-1}\equals y_{1:n-1} &\sim& N\big[\, \mu_{n}^P(y_{1:n-1}),\, \Sigma_n^P \, \big],
\vspace{3mm}
\\
X_n\given Y_{1:n}\equals y_{1:n} &\sim& N\big[\, \mu_n^F(y_{1:n}),\,\Sigma_n^F\, \big],
\vspace{3mm}
\\
X_n\given Y_{1:N}\equals y_{1:N} &\sim& N\big[\, \mu_n^S(y_{1:N}),\,\Sigma_n^S \, \big].
\end{array}$

\vspace{4mm}


 To relate this notation to the general POMP recursion formulas, given data $\data{y_{1:N}}$, we define the following terminology:

\vspace{5mm}

$\mu_n^P(\data{y_{1:n-1}})=\E\big[X_n\given Y_{1:n-1}\equals \data{y_{1:n-1}}\big]$ is the \myemph{one-step prediction mean} for time $t_n$. It is an arbitrary decision we have made to call this the prediction for time $t_n$ (the time for which the prediction is being made) rather than for time $t_{n-1}$ (the time at which the prediction for time $t_n$ becomes available).

\end{frame}



\begin{frame}

$\Sigma_n^P(\data{y_{1:n-1}})=\var\big(X_n\given Y_{1:n-1}\equals \data{y_{1:n-1}}\big)$ is the \myemph{one-step prediction variance} for time $t_n$.

\vspace{3mm}

For a Gaussian model, this conditional variance does not depend on $\data{y_{1:n}}$. To make this terminology work for general POMP models we see later, we include possible dependence on $\data{y_{1:n-1}}$.

\vspace{3mm}

Other related quantities use the same notation:

\vspace{3mm}

\bi

\item
$\mu_n^F(\data{y_{1:n}})=\E\big[X_n\given Y_{1:n}\equals \data{y_{1:n}}\big]$ is the \myemph{filter mean} for time $t_n$.

\vspace{3mm}


\item 
$\Sigma_n^F(\data{y_{1:n}})=\var\big(X_n\given Y_{1:n}\equals \data{y_{1:n}}\big)$ is the \myemph{filter variance} for time $t_n$.

\vspace{3mm}

\item 
$\mu_n^S(\data{y_{1:N}})=\E\big[X_n\given Y_{1:N}\equals \data{y_{1:N}}\big]$ is the \myemph{smoothing mean} for time $t_n$. 

\vspace{3mm}

\item 
$\Sigma_n^S(\data{y_{1:N}})=\var\big(X_n\given Y_{1:N}\equals \data{y_{1:N}}\big)$ is the \myemph{smoothing variance} for time $t_n$.

\vspace{3mm}

\ei

\end{frame}


\begin{frame}

\frametitle{The Kalman matrix recursions}

Applying the properties of linear combinations of Normal random variables, we get the Kalman filter and prediction recursions:

\vspace{3mm}


[KF3] $\eqspace
  \mu_{n+1}^P(y_{1:n}) = \matA_{n+1} \mu_{n}^F(y_{1:n})
$,

\vspace{3mm}


[KF4] $\eqspace
  \Sigma_{n+1}^P = \matA_{n+1} \Sigma_{n}^F \matA_{n+1}^\transpose + \covmatX_{n+1}
$.


\vspace{3mm}


[KF5] $\eqspace
  \Sigma_{n}^F = \big([\Sigma_n^P]^{-1} + \matB_n^\transpose\covmatY_n^{-1}\matB_n\big)^{-1}
$.

\vspace{3mm}

[KF6] $\eqspace
  \mu_{n}^F(y_{1:n}) = \mu_{n}^P(y_{1:n-1}) + \Sigma_n^F \matB^\transpose_n\covmatY_n^{-1}\big\{y_n - \matB_n\mu_n^P(y_{1:n-1})\big\}
$.

\vspace{2mm}

\end{frame}

\begin{frame}[fragile]
\frametitle{Toward deriving the Kalman matrix recursions}

\bi

\item  The prediction recursions [KF3] and [KF4] are relatively easy to demonstrate, but it is a good exercise to go through the algebra to your own satisfaction. 

\item  A useful trick for the algebra is to notice that the conditioning identities [KF1] for joint Gaussian random variables continue to hold if left and right are both conditioned on some additional jointly Gaussian variable, such as $Y_{1:n-1}$.

\item\   
[KF5] and [KF6] can be deduced by completing the square in an expression for the joint density,
$$f_{X_nY_n|Y_{1:n-1}} (
  x_n,y_n\given y_{1:n-1}
)$$
and noticing that the marginal density of $X_n$ given $Y_{1:n}$ is proportional to the joint density, with a normalizing constant that must allow the marginal density to integrate to one. 

\ei


\end{frame}

\begin{frame}[fragile]
\myquestion. Derive some or all of these equations.

\answer{\vspace{100mm}}{todo}

\end{frame}

\begin{frame}

\bi
\item  These Kalman filter matrix equations are easy to code, and quick to compute unless the dimension of the latent space is very large.

\item In numerical weather forecasting, with careful programming, they are solved with latent variables having dimension $d_X\approx 10^7$.

\item  A similar computation gives backward Kalman recursions. Putting the forward and backward Kalman recursions together, as in [MP10], is called \myemph{Kalman smoothing}.

\ei


\end{frame}


\begin{frame}[fragile]
\frametitle{Acknowledgments and License}

\bi
\item These notes build on previous versions at \url{ionides.github.io/531w16} and \url{ionides.github.io/531w18}.
\item
Licensed under the Creative Commons attribution-noncommercial license, \url{http://creativecommons.org/licenses/by-nc/3.0/}.
Please share and remix noncommercially, mentioning its origin.  
\includegraphics[width=2cm]{cc-by-nc.png}
\ei

\end{frame}


%\begin{frame}[allowframebreaks]
%\frametitle{References}
%\bibliography{notes03.bib}
%\end{frame}

\end{document}

