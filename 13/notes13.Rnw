\documentclass{beamer}
\usepackage{natbib}
\bibliographystyle{dcu}
\input{../header.tex}

\newcommand\CHAPTER{13}

\setbeamertemplate{footline}[frame number]

\newcommand\eqspace{\quad\quad\quad}
\newcommand\eqskip{\vspace{2.5mm}}

\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta{\Delta}

\newcommand\myeq{\hspace{10mm}}

% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{{\color{blue}{#2}}} % to show answers
\newcommand\answer[2]{#1} % to show blank space

<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
 ANS=FALSE
@

\usepackage{bbm} % for blackboard bold 1

\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
  cache=FALSE,
#  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping

@

<<setup,echo=F,results=F,cache=F>>=
myround<- function (x, digits = 1) {
  # taken from the broman package
  if (digits < 1) 
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

set.seed(2050320976)

options(
  keep.source=TRUE,
  encoding="UTF-8"
)

@



<<prelims,echo=F,cache=F>>=
set.seed(594709947L)
library(ggplot2)
theme_set(theme_bw())
library(tidyverse)
library(plyr)
library(reshape2)
library(foreach)
#library(doMC)
library(pomp)
stopifnot(packageVersion("pomp")>="2.0")
@


\begin{frame}[fragile]
\frametitle{Chapter \CHAPTER. Time series models with covariates, and a case study of polio}

\hspace{3cm} {\large \bf Objectives}

\vspace{3mm}

\begin{enumerate}

\item Discuss covariates in POMP models as a generalization of regression with ARMA errors.

\item Demonstrate the use of covariates in \package{pomp} to add demographic data (birth rates and total population) and  seasonality to an epidemiological model.

\item Present a case study, developing and fitting a POMP model with covariates.

\end{enumerate}

\end{frame}

\begin{frame}[fragile]


\frametitle{Covariates in time series analysis}

\bi

\item Suppose our time series of primary interest is $\data{y_{1:N}}$.

\item A \myemph{covariate} time series is an additional time series ${z_{1:N}}$ which is used to help explain $\data{y_{1:N}}$.

\item When we talk about covariates, it is often implicit that we think of ${z_{1:N}}$ as a measure of an \myemph{external forcing} to the system producing $\data{y_{1:N}}$. This means that the process generating the data ${z_{1:N}}$ affects the process generating $\data{y_{1:N}}$, but not vice versa. 

 \item For example, the weather might affect human health, but human health has negligible effect on weather: weather is an external forcing to human health processes.

\item When we make an assumption of external forcing, we should try to make it explicit.

\item In regression analysis, we usually \myemph{condition} on covariates. Equivalently, we model them as fixed numbers, rather than modeling them as the outcome of random variables.

\ei

\end{frame}

\begin{frame}[fragile]

\bi

\item When the process leading to  ${z_{1:N}}$ is not external to the system generating it, we must be alert to the possibility of \myemph{reverse causation} and \myemph{confounding variables}.

\item Issues involved in inferring causation from fitting statistical models are essentially the same whether the model is linear and Gaussian or not.

\item Any experience you have with causal interpretation of linear regression coefficients also applies to POMP models.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Covariates in linear time series analysis}

\bi

\item The main tool we have seen previously for investigating dependence on covariates is regression with ARMA errors.

\item This tool can also be used to identify lag relationships, where $\data{y_{n}}$ depends on $z_{n-L}$. 

\item Another way to investigate associations at different lags is by computing the sample correlation between $\data{y_n}$ and $z_{n-L}$, for $n\in L+1:N$, and plotting this against $L$. 

\item This is called the \myemph{cross-correlation function} and can be computed with the R function \code{ccf}.

 \item An example of the use of the cross-correlation function in a midterm project: The Association between Recent Cholera Epidemics and Rainfall in Haiti
(\url{https://ionides.github.io/531w16/midterm_project/project3/midterm_project.html}).

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Covariates in nonlinear POMP models}

\bi
\item The general POMP modeling framework allows essentially arbitrary modeling of covariates.

\item Scientific considerations may suggest sensible ways to model the relationship.

 \item In an epidemiological model for malaria, rainfall might affect the number of mosquitoes (and hence the disease transmission rate) but not the duration of infection.

 \item In an economic model, geopolitical shocks to the oil supply might have direct influence on energy prices, secondary direct effects on inflation and investment, and indirect consequences for unemployment.

 \item In a hydrology model, precipitation is a covariate explaining river flow, but the exact nature of the relationship is a question of interest.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Including covariates in the general POMP framework}

\bi

\item Recall that a POMP model is specified by defining the following:
$$\begin{array}{l}
f_{X_{0}}(x_0\params\theta),
\\
f_{X_{n}|X_{n-1}}(x_{n}\given x_{n-1}\params\theta),
\\
f_{Y_{n}|X_n}(y_{n}\given x_n\params\theta),
\end{array}$$
for $n=1:N$

\item The possibility of a general dependence on $n$ includes the possibility that there is some covariate time series $z_{0:N}$ such that
$$\begin{array}{lcl}
f_{X_{0}}(x_0\params\theta)&=& f_{X_{0}}(x_0\params\theta,z_0)
\\
f_{X_{n}|X_{n-1}}(x_{n}\given x_{n-1}\params\theta) &=& f_{X_{n}|X_{n-1}}(x_{n}\given x_{n-1}\params\theta,z_n),
\\
f_{Y_{n}|X_n}(y_{n}\given x_n\params\theta) &=& f_{Y_{n}|X_n}(y_{n}\given x_n\params\theta,z_n),
\end{array}$$
for $n=1:N$

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Seasonality in a POMP model}

\bi

\item One specific choice of covariates is to construct $z_{0:N}$ so that it fluctuates periodically, once per year. This allows \myemph{seasonality} enter the POMP model in whatever way is appropriate for the system under investigation.

\item All that remains is to hypothesize what is a reasonable way to include covariates for your system, and to fit the resulting model.

\item Now we can evaluate and maximize the log likelihood, we can construct AIC or likelihood ratio tests to see if the covariate helps describe the data.

\item This also lets us compare alternative ways the covariates might enter the process model and/or the measurement model.

\ei

\end{frame}

\begin{frame}[fragile]


\frametitle{Covariates in the \package{pomp} package}

\bi

\item \package{pomp} provides facilities for including covariates in a pomp object.

\item Named covariate time series entered via the \code{covar} argument to \code{pomp} are automatically defined within Csnippets used for the \code{rprocess}, \code{dprocess}, \code{rmeasure}, \code{dmeasure} and \code{rinit} arguments.

\item We see this in practice in the following epidemiological model, which has  population census, birth data and seasonality as covariates.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Case study: polio in Wisconsin}

\bi

\item The massive global polio eradication initiative (GPEI) has brought polio from a major global disease to the brink of extinction. 

\item Finishing this task is proving hard, and improved understanding polio ecology might assist.  

\item \citet{martinez-bakker15} investigated this using extensive state level pre-vaccination era data in USA. 

\item We will follow the approach of \citet{martinez-bakker15} for one state (Wisconsin). In the context of their model, we can quantify seasonality of transmission, the role of the birth rate in explaining the transmission dynamics, and the persistence mechanism of polio. 

\item \citet{martinez-bakker15} carrried out this analysis for all 48 contigous states and District of Columbia, and their data and code are publicly available. The data we study, in \code{polio_wisconsin.csv}, consist of \code{cases}, the monthly reported polio cases; \code{births}, the  monthly recorded births; \code{pop}, the annual census; \code{time}, date in years.

\ei

\end{frame}

\begin{frame}[fragile]

<<data>>=
polio_data <- read.csv("polio_wisconsin.csv",comment="#")
head(polio_data,4)
@

\end{frame}

\begin{frame}[fragile]

<<data_plot,echo=F,out.width="12cm">>=
polio_data %>%
  gather(variable,value,-time) %>%
  ggplot(aes(x=time,y=value))+
  geom_line()+
  facet_wrap(~variable,ncol=1,scales='free_y',strip.position = "left")+
  theme_bw()+
  theme(
    strip.background=element_rect(fill=NA,color=NA),
    strip.placement="outside"
  )+
  labs(x="",y="")
@


\end{frame}

\begin{frame}[fragile]

\bi

\item We use the compartment model of \citet{martinez-bakker15}.

\item Compartments representing susceptible babies in each of six one-month birth cohorts ($S^B_1$,...,$S^B_6$), susceptible older individuals ($S^O$), infected babies ($I^B$), infected older individuals ($I^O$), and recovered with lifelong immunity ($R$). 

\item The state vector of the disease transmission model consists of numbers of individuals in each compartment at each time, 
$$X(t)=\big(S^B_1(t),...,S^B_6(t), I^B(t),I^O(t),R(t) \big).$$

\item Babies under six months are modeled as fully protected from symptomatic poliomyelitis.

\item Older infections lead to reported cases (usually paralysis) at a rate $\rho$. 

\item The flows through the compartments are graphically represented on the following slide (Figure 1A of \citet{martinez-bakker15}):

\ei

\end{frame}

\begin{frame}[fragile]

\includegraphics[width=6cm]{polio_fig1A.png} \hspace{-5mm}\parbox[b]{5.5cm}{
SBk, suscepbible babies $k$ months\\
IB, infected babies\\
SO, susceptible older people\\
IO, infected older people

\vspace{3cm}

}

\end{frame}

\begin{frame}[fragile]

\frametitle{Setting up the model}

\bi

\item Duration of infection is comparable to the one-month reporting aggregation, so a discrete time model may be appropriate.

\item \citet{martinez-bakker15} fitted monthly reported cases, May 1932 through January 1953, so we set $t_n=1932+ (4+n)/12$ and
$$X_n=X(t_n)=\big(S^B_{1,n},...,S^B_{6,n}, I^B_n,I^O_n,R_n \big).$$

\item The mean force of infection, in units of $\mathrm{yr}^{-1}$, is modeled as
$$\bar\lambda_n=\left( \beta_n \frac{I^O_n+I^B_n}{P_n} + \psi \right)$$
where $P_n$ is census population interpolated to time $t_n$ and seasonality of transmission is modeled as
$$\beta_n=\exp\left\{ \sum_{k=1}^K b_k\xi_k(t_n) \right\},$$
with $\{\xi_k(t),k=1,\dots,K\}$ being a periodic B-spline basis with $K=6$.

\item $P_n$ and $\xi_k(t_n)$ are \myemph{covariate time series}.

\ei

\end{frame}

\begin{frame}[fragile]

\bi
\item The force of infection has a stochastic perturbation,
$$\lambda_n = \bar\lambda_n \epsilon_n,$$
where $\epsilon_n$ is a Gamma random variable with mean 1 and variance $\sigma^2_{\mathrm{env}} + \sigma^2_{\mathrm{dem}}\big/\bar\lambda_n$. These two terms capture variation on the environmental and demographic scales, respectively. All compartments suffer a mortality rate, set at $\delta=1/60\mathrm{yr}^{-1}$. 

\item Within each month, all susceptible individuals are modeled as having exposure to constant competing hazards of mortality and polio infection.  The chance of remaining in the susceptible population when exposed to these hazards for one month is therefore
$$p_n = \exp\big\{ -(\delta+\lambda_n)/12\big\},$$
with the chance of polio infection being 
$$q_n = (1-p_n)\lambda_n\big/(\lambda_n+\delta).$$

\ei

\end{frame}

\begin{frame}

\bi
\item We employ a continuous population model, with no demographic stochasticity. Writing $B_n$ for births in month $n$, we obtain the dynamic model of \citet{martinez-bakker15}:
$$\begin{array}{rcl}
S^B_{1,n+1}&=&B_{n+1}\\
S^B_{k,n+1}&=&p_nS^B_{k-1,n} \quad\mbox{for $k=2,\dots,6$}\\
S^O_{n+1}&=& p_n(S^O_n+S^B_{6,n})\\
I^B_{n+1}&=& q_n \sum_{k=1}^6 S^B_{k,n}\\
I^O_{n+1}&=& q_n S^O_n
\end{array}$$
\ei

\end{frame}

\begin{frame}[fragile]
\frametitle{The measurement model}

\bi
\item 
The model for the reported observations, conditional on the state, is a discretized normal distribution truncated at zero, with both environmental and Poisson-scale contributions to the variance:
$$Y_n= \max\{\mathrm{round}(Z_n),0\}, \quad Z_n\sim\mathrm{normal}\left(\rho I^O_n, \big(\tau  I^O_n\big)^2 + \rho I^O_n\right).$$
\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Initial conditions}

\bi
\item 
Additional parameters are used to specify initial state values at time $t_0=1932+ 4/12$.
\item 
We will suppose there are parameters $\big(\tilde S^B_{1,0},...,\tilde S^B_{6,0}, \tilde I^B_0,\tilde I^O_0,\tilde S^O_0\big)$ that specify the population in each compartment at time $t_0$ via
$$ S^B_{1,0}= {\tilde S}^B_{1,0} ,...,S^B_{6,0}= \tilde S^B_{6,0}, \quad I^B_{0}= P_0 \tilde I^B_{0},\quad S^O_{0}= P_0 \tilde S^O_{0}, \quad I^O_{0}= P_0 \tilde I^O_{0}.$$
\item
Following \citet{martinez-bakker15}, we make an approximation for the initial conditions of ignoring infant infections at time $t_0$. 
Thus, we set $\tilde I^B_{0}=0$ and use monthly births in the preceding months (ignoring infant mortality) to fix $\tilde S^B_{k,0}=B_{1-k}$ for $k=1,\dots,6$. The estimated initial conditions are then defined by the two parameters $\tilde I^O_{0}$ and $\tilde S^O_{0}$, since the initial recovered population, $R_0$, is specified by subtraction of all the other compartments from the total initial population, $P_0$.
\item 
It is convenient to parameterize the estimated initial states as fractions of the population, whereas the initial states fixed at births are parameterized directly as a count.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Building a \code{pomp} object for the polio model}

\bi
\item
We code the state and observation variables, and the choice of $t_0$, as
\ei

<<statenames>>=
polio_statenames <- c("SB1","SB2","SB3","SB4","SB5","SB6",
  "IB","SO","IO")
polio_obsnames <- "cases"
polio_t0 <- 1932+4/12
@
\bi
\item 
We do not explictly code $R$, since it is defined implicitly as the total population minus the sum of the other compartments. Due to lifelong immunity, individuals in $R$ play no role in the dynamics. Even occasional negative values of $R$ (due to a discrepancy between the census and the mortality model) would not be a fatal flaw.
\ei
\end{frame}

\begin{frame}[fragile]

\frametitle{Setting up the covariate table}
\bi
\item
\code{time} gives the time at which the covariates are defined.
\item
\code{P} is a smoothed interpolation of the annual census.
\item
\code{B} is monthly births.
\item \code{xi1,...,xi6}  is a periodic B-spline basis
\ei

<<covariates>>=
polio_K <- 6
polio_covar <- covariate_table(
  t=polio_data$time,
  B=polio_data$births,
  P=predict(smooth.spline(x=1931:1954,
    y=polio_data$pop[seq(12,24*12,by=12)]))$y,
  periodic.bspline.basis(t,nbasis=polio_K,
    degree=3,period=1,names="xi%d"),
  times="t"
)
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Regular parameters and initial value parameters}

\bi
\item
The parameters $b_1,\dots,b_\mathrm{K},\psi,\rho,\tau,\sigma_\mathrm{dem}, \sigma_\mathrm{env}$  in the model above are {regular parameters} (RPs), coded as
\ei
<<rp_names>>=
polio_rp_names <- c("b1","b2","b3","b4","b5","b6",
  "psi","rho","tau","sigma_dem","sigma_env")
@

\bi
\item
The {initial value parameters} (IVPs), $\tilde I^O_{0}$ and  $\tilde S^O_{0}$, are coded for each state named by adding \code{_0} to the state name:
\ei
<<ivp_names>>=
polio_ivp_names <- c("SO_0","IO_0")
polio_paramnames <- c(polio_rp_names,polio_ivp_names)
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Fixed parameters (FPs)}
\bi
\item 
Two quantities in the dynamic model specification, $\delta=1/60 \mathrm{yr}^{-1}$ and $\mathrm{K}=6$, are not estimated.
\item Six other initial value quantities, $\{\tilde S^B_{1,0},\dots,\tilde S^B_{6,0}\}$, are treated as fixed.

\item Fixed quantities could be coded as constants using the \code{globals} argument of \code{pomp}, but here we pass them as fixed parameters (FPs).
\ei

<<fixed_names>>=
polio_fp_names <- c("delta","K",
  "SB1_0","SB2_0","SB3_0","SB4_0","SB5_0","SB6_0")
polio_paramnames <- c(polio_rp_names,
  polio_ivp_names,polio_fp_names)
covar_index_t0 <- which(abs(polio_covar@times-polio_t0)<0.01)
polio_initial_births <- polio_covar@table["B",covar_index_t0-0:5]
names(polio_initial_births) <- c("SB1_0","SB2_0",
  "SB3_0","SB4_0","SB5_0","SB6_0") 
polio_fixed_params <- c(delta=1/60,K=polio_K,
  polio_initial_births)
@

\end{frame}

\begin{frame}[fragile]

\frametitle{A starting value for the parameters}

\bi
\item We have to start somewhere for our search in parameter space.
\item The following parameter vector is based on informal model exploration and prior research:
\ei

<<polio_read_mle>>=
polio_params_guess <- c(b1=3,b2=0,b3=1.5,b4=6,b5=5,b6=3,
  psi=0.002,rho=0.01,tau=0.001,sigma_dem=0.04,sigma_env=0.5,
  SO_0=0.12,IO_0=0.001,polio_fixed_params)
@

\end{frame}

\begin{frame}[fragile]

<<rprocess>>=
polio_rprocess <- Csnippet("
  double beta = exp(dot_product( (int) K, &xi1, &b1));
  double lambda = (beta * (IO+IB) / P + psi);
  double var_epsilon = pow(sigma_dem,2)/ lambda +  
    pow(sigma_env,2);
  lambda *= (var_epsilon < 1.0e-6) ? 1 : 
    rgamma(1/var_epsilon,var_epsilon);
  double p = exp(- (delta+lambda)/12);
  double q = (1-p)*lambda/(delta+lambda);
  SB1 = B;
  SB2= SB1*p;
  SB3=SB2*p;
  SB4=SB3*p;
  SB5=SB4*p;
  SB6=SB5*p;
  SO= (SB6+SO)*p;
  IB=(SB1+SB2+SB3+SB4+SB5+SB6)*q;
  IO=SO*q;
")
@

\end{frame}

\begin{frame}[fragile]

<<measure>>=
polio_dmeasure <- Csnippet("
  double tol = 1.0e-25;
  double mean_cases = rho*IO;
  double sd_cases = sqrt(pow(tau*IO,2) + mean_cases);
  if(cases > 0.0){
    lik = pnorm(cases+0.5,mean_cases,sd_cases,1,0)
      - pnorm(cases-0.5,mean_cases,sd_cases,1,0) + tol; 
  } else{
    lik = pnorm(cases+0.5,mean_cases,sd_cases,1,0) + tol;
  }
  if (give_log) lik = log(lik);
")

polio_rmeasure <- Csnippet("
  cases = rnorm(rho*IO, sqrt( pow(tau*IO,2) + rho*IO ) );
  if (cases > 0.0) {
    cases = nearbyint(cases);
  } else {
    cases = 0.0;
  }
")
@

\end{frame}

\begin{frame}[fragile]

The map from the initial value parameters to the initial value of the states at time $t_0$ is coded by the \code{rinit} function:

<<initializer>>=
polio_rinit <- Csnippet("
  SB1 = SB1_0;
  SB2 = SB2_0;
  SB3 = SB3_0;
  SB4 = SB4_0;
  SB5 = SB5_0;
  SB6 = SB6_0;
  IB = 0;
  IO = IO_0 * P;
  SO = SO_0 * P;
")
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Parameter transformations}
\bi
\item For parameter estimation, it is helpful to have transformations that map each parameter into the whole real line and which put uncertainty close to a unit scale
\ei
<<trans>>=
polio_partrans <- parameter_trans(
  log=c("psi","rho","tau","sigma_dem","sigma_env"),
  logit=c("SO_0","IO_0")
)
@
\bi
\item Since the seasonal splines are exponentiated, the \code{beta} parameters are already defined on the real line with unit scale uncertainty.
\ei

\end{frame}

\begin{frame}[fragile]

We now put these pieces together into a pomp object. 
<<pomp>>=
polio <- pomp(
  data=subset(polio_data, 
    (time > polio_t0 + 0.01) & (time < 1953+1/12+0.01),
    select=c("cases","time")),
  times="time",
  t0=polio_t0,
  params=polio_params_guess,
  rprocess = euler(step.fun = polio_rprocess, delta.t=1/12),
  rmeasure= polio_rmeasure,
  dmeasure = polio_dmeasure,
  covar=polio_covar,
  obsnames = polio_obsnames,
  statenames = polio_statenames,
  paramnames = polio_paramnames,
  rinit=polio_rinit,
  partrans=polio_partrans
)
plot(polio)
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Setting run levels to control computation time}

\bi

\item \code{run_level=1} will set all the algorithmic parameters to the first column of values in the following code, for debugging.

\item Here, \code{Np} is the number of particles, and \code{Nmif} is the number of iterations of the optimization procedure carried. 

\item \code{run_level=2} uses \code{Np=1000} and \code{Nmif=100}. enough effort in this case to gives reasonably stable results for a moderate amount of computational time.

\item Larger values give more refined computations, implemented here by \code{run_level=3} which was run on a computing node.

\ei

<<run_level>>=
run_level=3
polio_Np <-          switch(run_level,100, 1e3, 5e3)
polio_Nmif <-        switch(run_level, 10, 100, 200)
polio_Nreps_eval <-  switch(run_level,  2,  10,  20)
polio_Nreps_local <- switch(run_level, 10,  20,  40)
polio_Nreps_global <-switch(run_level, 10,  20, 100)
polio_Nsim <-        switch(run_level, 50, 100, 500) 
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Comments on setting algorithmic parameters}

\bi

\item \code{run_level} is a facility that is convenient for when you are editing the source code. It plays no fundamental role in the final results. If you are not editing the source code, or using the code as a template for developing your own analysis, it has no function.

\item When you edit a document with different \code{run_level} options, you can debug your code by editing \code{run_level=1}. Then, you can get preliminary assessment of whether your results are sensible with \code{run_level=2} and get finalized results, with reduced Monte Carlo error, by editing \code{run_level=3}.

\item In practice, you probably want \code{run_level=1} to run in minutes, \code{run_level=2} to run in tens of minutes, and \code{run_level=3} to run in hours.

\item You can increase or decrease the numbers of particles, or the number of mif2 iterations, or the number of global searches carried out, to make sure this procedure is practical on your machine.
 
\item Appropriate values of the algorithmic parameters for each run-level are context dependent.

\ei

\end{frame}

\begin{frame}[fragile]

\myquestion. \myemph{Choosing algorithmic parameters}.\\
Discuss how you choose the algorithmic parameters for each run level when building a new likelihood-based data analysis using \code{pfilter()} and \code{mif2()} in \package{pomp}.

\answer{\vspace{40mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\frametitle{Likelihood evaluation at the starting parameter estimate}

<<parallel-setup,cache=FALSE>>=
require(doParallel)
registerDoParallel()
@

<<pf1>>=
stew(file=sprintf("pf1-%d.rda",run_level),{
  t1 <- system.time(
    pf1 <- foreach(i=1:20,.packages='pomp') %dopar% pfilter(
      polio,Np=polio_Np)
  )
},seed=493536993,kind="L'Ecuyer")
L1 <- logmeanexp(sapply(pf1,logLik),se=TRUE)
@

\bi

\item In  \Sexpr{round(t1["elapsed"],1)} seconds, we obtain a log likelihood estimate of \Sexpr{round(L1[1],2)} with a Monte standard error of \Sexpr{round(L1[2],2)}.

\ei

\end{frame}


\begin{frame}[fragile]

\frametitle{Simulation to investigate local persistence}

\bi
\item 
The scientific purpose of fitting a model typically involves analyzing properties of the fitted model, often investigated using simulation.
\item Following \citet{martinez-bakker15}, we are interested in how often months with no reported cases ($Y_n=0$) correspond to months without any local asymptomatic cases, defined for our continuous state model as $I^B_n+I^O_n<1/2$.
\item For Wisconsin, using our model at the estimated MLE, we simulate in parallel as follows:
\ei
<<persistence-sim>>=
stew(sprintf("persistence-%d.rda",run_level),{
  t_sim <- system.time(
    sim <- foreach(i=1:polio_Nsim,
      .packages='pomp') %dopar% simulate(polio)
  )
},seed=493536993,kind="L'Ecuyer")
@

\end{frame}
\begin{frame}[fragile]

<<persistence-analysis>>=
no_cases_data <- sum(obs(polio)==0)
no_cases_sim <- sum(sapply(sim,obs)==0)/length(sim)
fadeout1_sim <- sum(sapply(sim,function(po)states(po)["IB",]
  +states(po)["IO",]<1))/length(sim)
fadeout100_sim <- sum(sapply(sim,function(po)states(po)["IB",]
  +states(po)["IO",]<100))/length(sim)
imports_sim <- coef(polio)["psi"]*mean(sapply(sim,
  function(po) mean(states(po)["SO",]+states(po)["SB1",]
    +states(po)["SB2",]+states(po)["SB3",]+states(po)["SB4",]
    +states(po)["SB5",]+states(po)["SB6",])))/12
@

\vspace{-1mm}

 For the data, there were \Sexpr{no_cases_data} months with no reported cases, similar to the mean of \Sexpr{round(no_cases_sim,1)} for simulations from the fitted model. Months with no asyptomatic infections for the simulations were rare, on average \Sexpr{round(fadeout1_sim,1)} months per simulation. Months with fewer than 100 infections averaged \Sexpr{round(fadeout100_sim,1)} per simulation, which in the context of a reporting rate of \Sexpr{signif(coef(polio)["rho"],3)} can explain the absences of case reports. The mean monthly infections due to importations, modeled by $\psi$, is \Sexpr{round(imports_sim,1)}. This does not give much opportunity for local elimination of poliovirus. A profile over $\psi$ would show how sensitive this conclusion is to values of $\psi$ consistent with the data.


\end{frame}
\begin{frame}[fragile]

\bi
\item It is also good practice to look at simulations from the fitted model:

\ei

<<plot_simulated>>=
mle_simulation <- simulate(polio,seed=127)
plot(mle_simulation)
@

\end{frame}

\begin{frame}[fragile]

\bi

\item We see from this simulation that the fitted model can generate report histories that look qualitatively similar to the data. However, there are things to notice in the reconstructed latent states. Specifically, the pool of older susceptibles, $S^O(t)$, is mostly increasing. The reduced case burden in the data in the time interval 1932--1945 is explained by a large initial recovered ($R$) population, which implies much higher levels of polio before 1932. There were large epidemics of polio in the USA early in the 20th century, so this is not implausible.

\item A liklihood profile over the parameter $\tilde S^O_0$ could help to clarify to what extent this is a critical feature of how the model explains the data.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Local likelihood maximization}

\bi

\item Let's see if we can improve on the previous MLE. We use the IF2 algorithm. We set a constant random walk standard deviation for each of the regular parameters and a larger constant for each of the initial value parameters:

\ei
<<rwsd>>=
polio_rw.sd_rp <- 0.02
polio_rw.sd_ivp <- 0.2
polio_cooling.fraction.50 <- 0.5
polio_rw.sd <- rw.sd(
  b1=polio_rw.sd_rp, b2=polio_rw.sd_rp,
  b3=polio_rw.sd_rp, b4=polio_rw.sd_rp,
  b5=polio_rw.sd_rp, b6=polio_rw.sd_rp,
  psi=polio_rw.sd_rp, rho=polio_rw.sd_rp,
  tau=polio_rw.sd_rp, sigma_dem=polio_rw.sd_rp,
  sigma_env=polio_rw.sd_rp,
  IO_0=ivp(polio_rw.sd_ivp), SO_0=ivp(polio_rw.sd_ivp)
)
@			 

\end{frame}

\begin{frame}[fragile]

<<mif>>=
stew(sprintf("mif-%d.rda",run_level),{
  t2 <- system.time({
    m2 <- foreach(i=1:polio_Nreps_local,
      .packages='pomp', .combine=c) %dopar% mif2(polio,
           Np=polio_Np,
           Nmif=polio_Nmif,
           cooling.fraction.50=polio_cooling.fraction.50,
           rw.sd=polio_rw.sd)	
    lik_m2 <- foreach(i=1:polio_Nreps_local,
      .packages='pomp', .combine=rbind) %dopar% logmeanexp(
        replicate(polio_Nreps_eval,logLik(
	  pfilter(polio,params=coef(m2[[i]]),Np=polio_Np))),
        se=TRUE)
  })
},seed=318817883,kind="L'Ecuyer")
@

\end{frame}

\begin{frame}[fragile]

<<search-save>>=
r2 <- data.frame(logLik=lik_m2[,1],logLik_se=lik_m2[,2],
  t(sapply(m2,coef)))
if (run_level>1) 
  write.table(r2,file="polio_params.csv",append=TRUE,
  col.names=FALSE,row.names=FALSE)
summary(r2$logLik,digits=5)
@

\bi

\item This investigation took  \Sexpr{round(t2["elapsed"]/60,1)} minutes.
\item
These repeated stochastic maximizations can also show us the geometry of the likelihood surface in a neighborhood of this point estimate:

\ei

\end{frame}

\begin{frame}[fragile]
<<pairs>>=
pairs(~logLik+psi+rho+tau+sigma_dem+sigma_env,
  data=subset(r2,logLik>max(logLik)-20))
@

\end{frame}

\begin{frame}[fragile]

\bi

\item We see strong tradeoffs between $\psi$, $\rho$ and $\sigma_\mathrm{dem}$. By itself, in the absence of other assumptions, the pathogen immigration rate $\psi$ is fairly weakly identified. However, the reporting rate $\rho$ is essentially the fraction of poliovirus infections leading to acute flaccid paralysis, which is known to be around 1\%. This plot suggests that fixing an assumed value of $\rho$ might lead to much more precise inference on $\psi$; the rate of pathogen immigration presumably being important for understanding disease persistence. These hypotheses could be investigated more formally by construction of profile likelihood plots and likelihood ratio tests.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Comparison  of our implementation with \citet{martinez-bakker15}}

\bi
\item 
This setup has minor differences in notation, model construction and code compared to \citet{martinez-bakker15}.
\item
The maximized likelihood reported for these data by \citet{martinez-bakker15} was -794.34, with Monte Carlo evaluation error of 0.18.
\item This is similar to the value \Sexpr{myround(max(r2$logLik),digits=2)} found by this search.
\item The differences between the two implementations do not substantially improve or decrease the fit of our model compared to  \citet{martinez-bakker15}, demonstrating reproducibility of both results.
\ei
\end{frame}

\begin{frame}[fragile]

\frametitle{Global likelihood maximization}

Practical parameter estimation involves trying many starting values for the parameters. One can specify a large box in parameter space that contains all parameter vectors which seem remotely sensible. If an estimation method gives stable conclusions with starting values drawn randomly from this box, this gives some confidence that an adequate global search has been carried out. 

For our polio model, a box containing reasonable parameter values might be

<<box>>=
polio_box <- rbind(
  b1=c(-2,8), b2=c(-2,8),
  b3=c(-2,8), b4=c(-2,8),
  b5=c(-2,8), b6=c(-2,8),
  psi=c(0,0.1), rho=c(0,0.1), tau=c(0,0.1),
  sigma_dem=c(0,0.5), sigma_env=c(0,1),
  SO_0=c(0,1), IO_0=c(0,0.01)
)
@


\end{frame}


\begin{frame}[fragile]

We then carry out a search identical to the local one except for the starting parameter values. This can be succinctly coded by calling \code{mif2} on the previously constructed object, \code{m2[[1]]}, with a reset starting value:

<<box_eval>>=
stew(file=sprintf("box_eval-%d.rda",run_level),{
  t3 <- system.time({
    m3 <- foreach(i=1:polio_Nreps_global,.packages='pomp',
      .combine=c) %dopar% mif2(
        m2[[1]],
        params=c(apply(polio_box,1,function(x)runif(1,x[1],x[2])),
	  polio_fixed_params)
      )
    lik_m3 <- foreach(i=1:polio_Nreps_global,.packages='pomp',
      .combine=rbind) %dopar% logmeanexp(
        replicate(polio_Nreps_eval,
        logLik(pfilter(polio,
	  params=coef(m3[[i]]),Np=polio_Np))), 
        se=TRUE)
  })
},seed=290860873,kind="L'Ecuyer")
@

\end{frame}

\begin{frame}[fragile]

<<global-results>>=
r3 <- data.frame(logLik=lik_m3[,1],logLik_se=lik_m3[,2],t(sapply(m3,coef)))
if(run_level>1) write.table(r3,file="polio_params.csv",append=TRUE,col.names=FALSE,row.names=FALSE)
summary(r3$logLik,digits=5)
@

\bi

\item Evaluation of the best result of this search gives a likelihood of \Sexpr{round(max(r3$logLik),1)} with a standard error of \Sexpr{round(r3$logLik_se[which.max(r3$logLik)],1)}. We see that optimization attempts from diverse remote starting points can approach our MLE, but do not exceed it. This gives us some reasonable confidence in our MLE. 

\item Plotting these diverse parameter estimates can help to give a feel for the global geometry of the likelihood surface 

\ei

\end{frame}


\begin{frame}[fragile]

<<pairs_global>>=
pairs(~logLik+psi+rho+tau+sigma_dem+sigma_env,
  data=subset(r3,logLik>max(logLik)-20))
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Benchmark likelihoods for non-mechanistic models}

\bi
\item To understand these global searches, many of which may correspond to parameter values having no meaningful scientific interpretation, it is helpful to put the log likelihoods in the context of some non-mechanistic benchmarks.

\item The most basic statistical model for data is independent, identically distributed (IID). Picking a negative binomial model, 

\ei

<<nbinom>>=
nb_lik <- function(theta) -sum(dnbinom(as.vector(obs(polio)),
  size=exp(theta[1]),prob=exp(theta[2]),log=TRUE))
nb_mle <- optim(c(0,-5),nb_lik)
-nb_mle$value
@

\bi

\item We see that a model with likelihood below \Sexpr{round(-nb_mle$value,1)} is unreasonable. This explains a cutoff around this value in the global searches: in these cases, the model is finding essentially IID explanations for the data.

\ei

\end{frame}


\begin{frame}[fragile]
\frametitle{ARMA models as benchmarks}
\bi
\item Linear, Gaussian auto-regressive moving-average (ARMA) models provide non-mechansitic fits to the data including flexible dependence relationships.
\item We fit to $\log(y_n^*+1)$ and correct the likelihood back to the scale appropriate for the untransformed data:

\ei

<<arma>>=
log_y <- log(as.vector(obs(polio))+1)
arma_fit <- arima(log_y,order=c(2,0,2),
  seasonal=list(order=c(1,0,1),period=12))
arma_fit$loglik-sum(log_y)
@

\bi

\item This 7-parameter model, which knows nothing of susceptible depletion, attains a likelihood of \Sexpr{round( arma_fit$loglik-sum(log_y),1)}.
\item Although the goal of mechanistic modeling here is not to beat non-mechanstic models, it is comforting that we're competitive with them.

\ei

\end{frame}


\begin{frame}[fragile]

\frametitle{Mining previous investigations of the likelihood}

<<param_file,out.width="10cm">>=
polio_params <- read.table("polio_params.csv",row.names=NULL,
  header=TRUE)
pairs(~logLik+psi+rho+tau+sigma_dem+sigma_env,
  data=subset(polio_params,logLik>max(logLik)-20))
@

\end{frame}

\begin{frame}[fragile]

\bi

\item Here, we see that the most successful searches have always led to models with reporting rate around 1-2\%. This impression can be reinforced by looking at results from the global searches:

\ei

<<global_rho_code,eval=F,echo=T>>=
plot(logLik~rho,data=subset(r3,logLik>max(r3$logLik)-10),log="x")
@

\vspace{-2mm}

<<global_rho_plot,eval=T,echo=F,out.width="7cm",fig.width=5,fig.height=3>>=
par(mai=c(0.8,0.8,0.1,0.1))
<<global_rho_code>>
@



\bi

\item Reporting rates close to 1\% provide a small but clear advantage (several units of log likelihood) in explaining the data. For these reporting rates, depletion of susceptibles can help to explain the dynamics.

\ei

\end{frame}

\begin{frame}[fragile]

\myquestion. \myemph{Parameter estimation using randomized starting values}.
Comment on the computations above, for parameter estimation using randomized starting values. Propose and try out at least one modification of the procedure. How could one make a formal statement quantifying the error of the optimization procedure?

\answer{\vspace{40mm}}{todo}
\end{frame}

\begin{frame}[fragile]

\myquestion. \myemph{Demography and discrete time}.
It can be surprisingly hard to include birth, death, immigration, emmigration and aging into a disease model in satisfactory ways. Consider the strengths and weaknesses of the analysis presented. For example, how does it compare to a continuous-time model? In an imperfect world, it is nice to check the extent to which the conclusions are insensitive to alternative modeling decisions. If you have some ideas to change the treatmentof demography (or an other aspect of the model) you could have a go at coding it up to see if it makes a difference.

\answer{\vspace{40mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\myquestion. \myemph{Diagnosing filtering and maximization convergence}.
Are there outliers in the data (i.e., observations that do not fit well with our model)? Are we using unnecessarily large amounts of computer time to get our results? Are there indications that we would should run our computations for longer? Or maybe with different choices of algorithmic settings?
In particular, \code{cooling.fraction.50} gives the fraction by which the random walk standard deviation is decreased ("cooled") in 50 iterations. If \code{cooling.fraction.50} is too small, the search will "freeze" too soon, evidenced by flat parallel lines in the convergence diagnostics. If \code{cooling.fraction.50} is too large, the researcher may run of of time, patience or computing budget (or all three) before the parameter trajectories approach an MLE.

\answer{\vspace{20mm}}{todo}
\end{frame}

\begin{frame}[fragile]

<<mif_diagnostics_code,echo=T,eval=F>>=
plot(m3[r3$logLik>max(r3$logLik)-10])
@

\vspace{-5mm}

<<mif_diagnostics_plot,out.width="7.5cm",echo=F>>=
<<mif_diagnostics_code>>
@

\end{frame}

\begin{frame}[fragile]

\bi
\item
The likelihood is particularly important to keep in mind. If parameter estimates are numerically unstable, that could be a consequence of a weakly identified parameter subspace.
\item
The presence of some weakly identified combinations of parameters is not fundamentally a scientific flaw; rather, our scientific inquiry looks to investigate which questions can and cannot be answered in the context of a set of data and modeling assumptions.
\item As long as the search is demonstrably approaching the maximum likelihood region we should not necessarily be worried about the stability of parameter values (at least, from the point of diagnosing successful maximization).
\item
So, we zoom in on the likelihood convergence:
\ei

\end{frame}

\begin{frame}[fragile]

<<likelihood_convergence,out.width="10cm">>=
loglik_convergence <- do.call(cbind,
  traces(m3[r3$logLik>max(r3$logLik)-10],"loglik"))
matplot(loglik_convergence,type="l",lty=1,
  ylim=max(loglik_convergence,na.rm=T)+c(-10,0))
@

<<save, include=FALSE,purl=FALSE,eval=FALSE>>=
if(run_level>1) save(list = ls(all = TRUE), file = "Rout.rda")
@

\end{frame}  

\begin{frame}[fragile]
\frametitle{Acknowledgments and License}

\bi
\item Produced with R version \Sexpr{ getRversion()} and \package{pomp} version \Sexpr{ packageVersion("pomp")}.

\item These notes build on previous versions at \url{ionides.github.io/531w16} and \url{ionides.github.io/531w18}. 
\item Those notes draw on material developed for a short course on Simulation-based Inference for Epidemiological Dynamics (\url{http://kingaa.github.io/sbied/}) by Aaron King and Edward Ionides, taught at the University of Washington Summer Institute in Statistics and Modeling in Infectious Diseases, from 2015 through 2019.
\item
Licensed under the Creative Commons attribution-noncommercial license, \url{http://creativecommons.org/licenses/by-nc/3.0/}.
Please share and remix noncommercially, mentioning its origin.  
\includegraphics[width=2cm]{cc-by-nc.png}
\ei

\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliography{notes13.bib}
\end{frame}

\end{document}
