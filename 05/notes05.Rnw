%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{5}


\newcommand\eqspace{\quad\quad\quad}
\newcommand\ar{\phi}
\newcommand\ma{\psi}
%\newcommand\eqspace{\hspace{10mm}}

% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{{\color{blue}{#2}}} % to show answers
\newcommand\answer[2]{#1} % to show blank space

<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
 ANS=FALSE
@

\usepackage{bbm} % for blackboard bold 1

\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping

@

<<setup,echo=F,results=F,cache=F>>=
myround<- function (x, digits = 1) {
  # taken from the broman package
  if (digits < 1) 
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

set.seed(2050320976)

options(
  keep.source=TRUE,
  encoding="UTF-8"
)

@

\begin{frame}[fragile]
\frametitle{Chapter \CHAPTER. Parameter estimation and model identification for ARMA models}

\hspace{3cm} {\large \bf Objectives}

\vspace{3mm}

\begin{enumerate}
\item   Develop likelihood-based inference in the context of ARMA models.

\item Discuss maximum likelihood parameter estimation and alternative methods.

\item Investigate strategies for model selection, also known as model identification, in the context of ARMA models.

\item  Work on practical computational approaches for implementing these methods.

\end{enumerate}

\end{frame}

\begin{frame}[fragile]

\frametitle{Background on likelihood-based inference}

\bi

\item For any data $\data{y_{1:N}}$ and any probabilistic model $f_{Y_{1:N}}(y_{1:N}\params\theta)$ we define the likelihood function to be
$$ \lik(\theta) = f_{Y_{1:N}}(\data{y_{1:N}}\params\theta).$$

\item It is often convenient to work with the logarithm to base $e$ of the likelihood, which we write as
$$\loglik(\theta) = \log \lik(\theta).$$

\item Using the likelihood function as a statistical tool is a very general technique, widely used since Fisher (1922)] (\url{wikipedia.org/wiki/Likelihood_function}).

\item Time series analysis involves various situations where we can, with sufficient care, compute the likelihood function and take advantage of the general framework of likelihood-based inference.

\ei

\end{frame}

\begin{frame}[fragile]


\bi
\item Computation of the likelihood function for ARMA models is not entirely straightforward. 

 \item Computationally efficient algorithms exist, using a state space model representation of ARMA models that will be developed later in this course. 

 \item For now, it is enough that software exists to evaluate and maximize the likelihood function for a Gaussian ARMA model. Our immediate task is to think about how to use that capability.

\ei

\end{frame}

\begin{frame}[fragile]

\bi
\item Before evaluation of the ARMA likelihood became routine, it was popular to use a method of moments estimator called \myemph{Yule-Walker} estimation. This is described by Shumway and Stoffer (Section 3.6) but is nowadays mostly of historical interest. 

\item There are occasionally time series situations where massively long data or massively complex models mean that it is computationally infeasible to work with the likelihood function. However, we are going to focus on the common situation where we can (with due care) work with the likelihood.

\item Likelihood-based inference (meaning statistical tools based on the likelihood function) provides tools for parameter estimation, standard errors, hypothesis tests and diagnosing model misspecification. 

\item Likelihood-based inference often (but not always) has favorable theoretical properties. Here, we are not especially concerned with the underlying theory of likelihood-based inference. On any practical problem, we can check the properties of a statistical procedure by simulation experiments.

\ei

\end{frame}   

\begin{frame}[fragile]


\frametitle{The maximum likelihood estimator (MLE)}

\bi

\item A maximum likelihood estimator (MLE) is
$$ \hat\theta(y_{1:N}) = \arg\max_\theta f_{Y_{1:N}}(y_{1:N}\params\theta),$$
where $\arg\max_\theta g(\theta)$ means a value of argument $\theta$ at which the maximum of the function $g$ is attained, so $g\big(\arg\max_\theta g(\theta)\big) = \max_\theta g(\theta)$.

\item If there are many values of $\theta$ giving the same maximum value of the likelihood, then an MLE still exists but is not unique.
 

\item The maximum likelihood estimate (also known as the MLE) is
\begin{eqnarray} \estimate{\hat\theta} &=& \hat\theta(\data{y_{1:N}})
\\
&=& \arg\max_\theta \lik(\theta)
\\
&=& \arg\max_\theta \loglik(\theta).
\end{eqnarray}
\ei

\end{frame}   

\begin{frame}[fragile]

\myquestion. Why are $\arg\max_\theta \lik(\theta)$ and $\arg\max_\theta \loglik(\theta)$ the same?

\answer{\vspace{20mm}}{todo}

\bi

\item We can write $\hat\theta_{MLE}$ and $\estimate{\hat\theta_{MLE}}$ if we are considering various alternative estimation methods. However, in this course, we will most often be using maximum likelihood estimation so we let $\hat\theta$ and $\estimate{\hat\theta}$ correspond to this approach.

\ei

\end{frame}  

 \begin{frame}[fragile]

\frametitle{Standard errors for the MLE}
\bi
\item As statisticians, it would be irresponsible to present an estimate without a measure of uncertainty!

\item Usually, this means obtaining a confidence interval, or an approximate confidence interval. 

 \item It is good to say \myemph{approximate} when you present something that is not exactly a confidence interval with the claimed coverage. For example, remind yourself of the definition of a 95% confidence interval. 

 \item Saying "approximate" reminds you that there is some checking that could be done to assess how accurate the approximation is in your particular situation.

 \item It also helps to remind you that it may be interesting and relevant to explain why the interval you present is an approximate confidence interval rather than an exact one.

\item There are three main approaches to estimating the statistical uncertainty in an MLE.

1. The Fisher information. This is computationally quick, but works well only when $\hat\theta(Y_{1:N})$ is well approximated by a normal distribution.

2. Profile likelihood estimation. This is a bit more computational effort, but generally is preferable to the Fisher information.

3. A simulation study, also known as a bootstrap. 

 \item If done carefully and well, this can be the best approach.

 \item A confidence interval is a claim about reproducibility. You claim, so far as your model is correct, that on 95% of realizations from the model, a 95% confidence interval you have constructed will cover the true value of the parameter.

 \item A simulation study can check this claim fairly directly, but requires the most effort. 

 \item The simulation study takes time for you to develop and debug, time for you to explain, and time for the reader to understand and check what you have done. We usually carry out simulation studies to check our main conclusions only.
\ei
\end{frame}   

\begin{frame}[fragile]

\frametitle{Standard errors via the observed Fisher information}

\bi
\item We suppose that $\theta\in\R^D$ and so we can write $\theta=\theta_{1:D}$.

\item The Hessian matrix
%% (https://en.wikipedia.org/wiki/Hessian_matrix) 
of a function is the matrix of its second partial derivatives. We write the Hessian matrix of the log likelihood function as $\nabla^2\loglik(\theta)$, a $D\times D$ matrix whose $(i,j)$ element is
$$ \big[\nabla^2\loglik(\theta)\big]_{ij} =  \frac{\partial^2}{\partial\theta_i\partial\theta_j}\loglik(\theta).$$

\item The observed Fisher information is
$$ \estimate{\hat{I}} = - \nabla^2\loglik(\estimate{\hat\theta}).$$

\item A standard asymptotic approximation to the distribution of the MLE for large $N$ is
\[
\hat\theta(Y_{1:N}) \approx N\left[\theta, [\estimate{\hat{I}}]^{-1}\right],
\]
where $\theta$ is the true parameter value.
This asserts that the MLE is asymptotically unbiased, with variance asymptotically attaining the Cramer-Rao lower bound. Thus, we say the MLE is \myemph{asymptotically efficient}.
Here, we interpret $\approx$ to mean "one could write a limit statement formally justifying this approximation in a suitable limit."

\item A corresponding approximate 95% confidence interval for $\theta_d$ is
$$ \estimate{\hat\theta_d} \pm 1.96 \big[{\estimate{\hat{I}}}^{-1}\big]_{dd}^{1/2}.$$

\item The R function \code{arima} computes standard errors for the MLE of an ARMA model in this way.

\item We usually only have one time series, with some fixed $N$, and so we cannot in practice take $N\to\infty$. When our time series model is non-stationary it may not even be clear what it would mean to take $N\to\infty$. These asymptotic results should be viewed as nice mathematical reasons to consider computing an MLE, but not a substitute for checking how the MLE behaves for our model and data. 
\ei

\end{frame}



\begin{frame}[fragile]
\frametitle{Confidence intervals via the profile likelihood}
\bi
\item Let's consider the problem of obtaining a confidence interval for $\theta_d$, the $d$th component of $\theta_{1:D}$. 

\item The \myemph{profile log likelihood function} of $\theta_d$ is defined to be 
$$ \profileloglik{d}(\theta_d) = \max_{\phi\in\R^D: \phi_d=\theta_d}\loglik(\phi).$$
In general, the profile likelihood of one parameter is constructed by maximizing the likelihood function over all other parameters.

\item Check that $\max_{\theta_d}\profileloglik{d}(\theta_d) = \max_{\theta_{1:D}}\loglik(\theta_{1:D})$. Maximizing the profile likelihood $\profileloglik{d}(\theta_d)$ gives the MLE, $\estimate{\hat\theta_d}$.

\item An approximate 95% confidence interval for $\theta_d$ is given by
$$ \big\{\theta_d : \loglik(\estimate{\hat\theta}) - \profileloglik{d}(\theta_d)< 1.92\big\}.$$

\item This is known as a profile likelihood confidence interval. The cutoff $1.92$ is derived using \myemph{Wilks's theorem}, which we will discuss in more detail when we develop likelihood ratio tests.

\item Although the asymptotic justification of Wilks's theorem is the same limit that justifies the Fisher information standard errors, profile likelihood confidence intervals tend to work better than Fisher information confidence intervals when $N$ is not so large---particularly when the log likelihood function is not close to quadratic near its maximum.

\ei

\end{frame}   

\begin{frame}[fragile]

\frametitle{Bootstrap methods for constructing standard errors and confidence intervals}
\bi
\item Suppose we want to know the statistical behavior of the estimator $\hat\theta({y_{1:N}})$
for models in a neighborhood of the MLE, $\data{\theta}=\hat\theta(\data{y_{1:N}})$.

\item In particular, let's consider the problem of estimating uncertainty about $\theta_1$. We want to assess the behavior of the maximum likelihood estimator, $\hat\theta({y_{1:N}})$, and possibly the coverage of an associated confidence interval estimator, $\big[\hat\theta_{1,\mathrm lo}({y_{1:N}}),\hat\theta_{1,\mathrm hi}({y_{1:N}})\big]$. The confidence interval estimator could be constructed using either the Fisher information method or the profile likelihood approach.


\item The following simulation study lets us address the following goals: 
  
(A) Evaluate the coverage of a proposed confidence interval estimator, $[\hat\theta_{1,\mathrm lo},\hat\theta_{1,\mathrm hi}]$,
  
(B) Construct a standard error for $\estimate{\hat\theta_1}$,
  
(C) Construct a confidence interval for $\theta_1$ with exact local coverage.

1. Generate $J$ independent Monte Carlo simulations, 
$$Y_{1:N}^{[j]} \sim f_{Y_{1:N}}(y_{1:N}\params\estimate{\hat\theta})\mbox{ for } j\in 1:J.$$

2. For each simulation, evaluate the maximum likelihood estimator,
$$ \theta^{[j]} = \hat\theta\big(Y_{1:N}^{[j]}\big)\mbox{ for } j\in 1:J,$$
and, if desired, the confidence interval estimator,
$$ \big[\theta^{[j]}_{1,\mathrm lo},\theta^{[j]}_{1,\mathrm hi}\big] = \big[\hat\theta_{1,\mathrm lo}({Y^{[j]}_{1:N}}),\hat\theta_{1,\mathrm hi}({Y^{[j]}_{1:N}})\big].$$

3. We can use these simulations to obtain solutions to our goals for uncertainty assessment:
  
(A) For large $J$, the coverage of the proposed confidence interval estimator is well approximated, for models in a neighborhood of $\estimate{\hat\theta}$, by the proportion of the intervals $\big[\theta^{[j]}_{1,\mathrm lo},\theta^{[j]}_{1,\mathrm hi}\big]$ that include $\estimate{\hat\theta_1}$.
  
(B) The sample standard deviation of $\{ \theta^{[j]}_1, j\in 1:J\}$ is a natural standard error to associate with $\estimate{\hat \theta_1}$.
  
(C) For large $J$, one can empirically calibrate a 95% confidence interval for $\theta_1$ with exactly the claimed coverage in a neighborhood of $\estimate{\hat\theta}$. For example, using profile methods, one could replace the cutoff 1.92 by a constant $\alpha$ chosen such that 95% of the profile confidence intervals computed for the simulations cover $\estimate{\hat\theta_1}$.

\ei

\end{frame}

\begin{frame}[fragile]

\myquestion. Local coverage as an approximation to actual coverage for a confidence interval

\bi
\item A true 95% confidence interval covers $\theta$ with probability 0.95 whatever the value of $\theta$.  

\item The local coverage probability at a value $\theta=\tilde\theta$ is the chance that the confidence interval covers $\tilde\theta$ when the true parameter value is $\tilde\theta$.
Typically, we compute local coverage at $\theta=\estimate{\hat\theta}$.

\item Local coverage can be evaluated or calibrated via simulation; the actual (global) coverage is usually hard to work with.

\item What properties of the model and data make local coverage a good substitute for global coverage? How would you check whether or not these properties hold?

\ei

\end{frame} 

 \begin{frame}[fragile]

\frametitle{Likelihood-based model selection and model diagnostics}

1. Likelihood ratio tests for nested hypotheses
\bi
\item The whole parameter space on which the model is defined is $\Theta\subset\R^D$. 

\item Suppose we have two \myemph{nested} hypotheses
\begin{eqnarray}
H^{\langle 0\rangle} &:& \theta\in \Theta^{\langle 0\rangle},
\\
H^{\langle 1\rangle} &:& \theta\in \Theta^{\langle 1\rangle},
\end{eqnarray}
defined via two nested parameter subspaces, $\Theta^{\langle 0\rangle}\subset \Theta^{\langle 1\rangle}$, with respective dimensions $D^{\langle 0\rangle}< D^{\langle 1\rangle}\le D$.

\item We consider the log likelihood maximized over each of the hypotheses,
\begin{eqnarray}
\ell^{\langle 0\rangle} &=& \sup_{\theta\in \Theta^{\langle 0\rangle}} \ell(\theta),
\\
\ell^{\langle 1\rangle} &=& \sup_{\theta\in \Theta^{\langle 1\rangle}} \ell(\theta).
\end{eqnarray}

\ei
\end{frame}   

\begin{frame}[fragile]
\bi
\item A useful approximation asserts that, under the hypothesis $H^{\langle 0\rangle}$,
$$ 
\ell^{\langle 1\rangle} - \ell^{\langle 0\rangle} \approx (1/2) \chi^2_{D^{\langle 1\rangle}- D^{\langle 0\rangle}},
$$
where $\chi^2_d$ is a chi-squared random variable on $d$ degrees of freedom and $\approx$ means "is approximately distributed as."

\item We will call this the \myemph{Wilks approximation}.

\item The Wilks approximation can be used to construct a hypothesis test of the null hypothesis  $H^{\langle 0\rangle}$ against the alternative  $H^{\langle 1\rangle}$. 

\item This is called a \myemph{likelihood ratio test} since a difference of log likelihoods corresponds to a ratio of likelihoods.

\item When the data are IID, $N\to\infty$, and the hypotheses satisfy suitable regularity conditions, this approximation can be derived mathematically and is known as \myemph{Wilks's theorem}. 

\item We therefore have two different interpretations of $\approx$. One is "one could write a limit statement formally justifying this approximation in a suitable limit" and another is "this approximation is useful in the finite sample situation at hand." These interpretations may both be appropriate!

\item The chi-squared approximation to the likelihood ratio statistic may be useful, and can be assessed empirically by a simulation study, even in situations that do not formally satisfy any known theorem.
\ei

\end{frame} 

\begin{frame}[fragile]  

\frametitle{Using a likelihood ratio test to construct profile likelihood confidence intervals}
\bi

\item Recall the duality between hypothesis tests and confidence intervals:
  
The estimated parameter $\data{\theta}$ does not lead us to reject a null hypothesis of $\theta=\theta^{\langle 0\rangle}$ at the 5% level
$$\Updownarrow$$
$\theta^{\langle 0\rangle}$ is in a 95% confidence interval for $\theta$.

\item We can check what the 95\% cutoff is for a chi-squared distribution with one degree of freedom,
<<chi_squared>>=
qchisq(0.95,df=1)
@

\item We can now see how the Wilks approximation suggests a confidence interval constructed from parameter values having a profile likelihood within 1.92 log units of the maximum. 

\item It is a exercise to write out more details (to your own satisfaction) on how to use the Wilks approximation, together with the duality between hypothesis tests and confidence intervals, to derive a profile likelihood confidence interval.

\ei

\end{frame} 

  \begin{frame}[fragile]

\frametitle{Akaike's information criterion (AIC)}

\bi
\item Likelihood ratio tests provide an approach to model selection for nested hypotheses, but what do we do when models are not nested?

\item A more general approach is to compare likelihoods of different models by penalizing the likelihood of each model by a measure of its complexity. 

\item Akaike's information criterion \myemph{AIC} is given by
$$ AIC = -2 \times \loglik(\data{\theta}) + 2D$$
"Minus twice the maximized log likelihood plus twice the number of parameters."

\item We are invited to select the model with the lowest AIC score.

\item AIC was derived as an approach to minimizing prediction error. Increasing the number of parameters leads to additional \myemph{overfitting} which can decrease predictive skill of the fitted model. 

\item Viewed as a hypothesis test, AIC may have weak statistical properties. It can be a mistake to interpret AIC by making a claim that the favored model has been shown to provides a superior explanation of the data. However, viewed as a way to select a model with reasonable predictive skill from a range of possibilities, it is often useful.

\ei

\end{frame}   


\begin{frame}[fragile]

\frametitle{Comparing AIC with likelihood ratio tests}

\bi

\item Suppose we are in a situation in which we wish to choose between two nested hypotheses, with dimensions $D^{\langle 0\rangle}< D^{\langle 1\rangle}$. Suppose the Wilks approximation is valid.

\item Consider the strategy of selecting the model with the lowest AIC value. 

\item We can view this model selection approach as a formal statistical test. 

\item Find an expression for the size of this AIC test (i.e, the probability of rejecting the null hypothesis,  $H^{\langle 0\rangle}$, when this null hypothesis is true).

\item Evaluate this expression for $D^{\langle 1\rangle} - D^{\langle 0\rangle}=1$.

\ei

\end{frame} 

\begin{frame}[fragile]


\frametitle{Implementing likelihood-based inference for ARMA models in R}

\bi

\item The Great Lakes are an important resource for leisure, agriculture and industry in this region. 

\item A past concern has been whether human activities such as water diversion or channel dredging might be leading to a decline in lake levels. 

\item An additional current concern is the effects of climate change. The physical mechanisms are not always obvious: for example, evaporation tends to be highest when the weather is cold but the lake is not ice-covered. 

\item We look at monthly time series data on the depth of Lake Huron. 
\ei

\end{frame}   


\begin{frame}[fragile]


\frametitle{Reading in the data}

Here is the head of the file \url{huron_depth.csv}

\begin{verbatim}
# downloaded on 1/24/16 from
# http://www.glerl.noaa.gov/data/dashboard/data/levels/mGauge/miHuronMog.csv
# Lake Michigan-Huron:, Monthly Average Master Gauge Water Levels (1860-Present)
# Source:, NOAA/NOS
Date, Average
01/01/1860,177.285
02/01/1860,177.339
03/01/1860,177.349
04/01/1860,177.388
05/01/1860,177.425
\end{verbatim}


\end{frame}


\begin{frame}[fragile]

\bi
\item A bit of work has to be done manipulating the \code{Date} variable. 

 \item Moving between date formats is a necessary skill for time series analysis!

 \item A standard representation of time is \code{POSIXct}, which is a signed real number representing the number of seconds since the beginning of 1970.

 \item The raw data have a character string representing date. We convert this into the standard format using \code{strptime}. Than we can extract whatever we need. See \code{?DateTimeClasses} for more on manipulating date and time formats in R.


\ei

<<read_data>>=
dat <- read.table(file="huron_depth.csv",sep=",",header=TRUE)
dat$Date <- strptime(dat$Date,"%m/%d/%Y")
dat$year <- as.numeric(format(dat$Date, format="%Y"))
dat$month <- as.numeric(format(dat$Date, format="%m"))
head(dat)
@


\end{frame}


\begin{frame}[fragile]

\bi

\item For now, let's avoid monthly seasonal variation by considering an annual series of January depths. We will investigate seasonal variation later in the course, but sometimes it is best avoided.
\ei

<<select_annual>>=
dat <- subset(dat,month==1)
huron_depth <- dat$Average
year <- dat$year
plot(huron_depth~year,type="l")
@


\end{frame}  


 \begin{frame}[fragile]

\frametitle{Fitting an ARMA model}

\bi
\item Later, we will consider hypotheses of trend. For now, let's start by fitting a stationary ARMA$(p,q)$ model under the null hypothesis that there is no trend. This hypothesis, which asserts that nothing has substantially changed in this system over the last 150 years, is not entirely unreasonable from looking at the data.

\item We seek to fit a stationary Gaussian ARMA(p,q) model with parameter vector $\theta=(\ar_{1:p},\ma_{1:q},\mu,\sigma^2)$ given by
$$ \ar(B)(Y_n-\mu) = \ma(B) \epsilon_n,$$
where 
\begin{eqnarray}
\mu &=& \E[Y_n]
\\
\ar(x)&=&1-\ar_1 x-\dots -\ar_px^p,
\\ 
\ma(x)&=&1+\ma_1 x+\dots +\ma_qx^q, 
\\
\epsilon_n&\sim&\mathrm{ iid }\, N[0,\sigma^2].
\end{eqnarray}

\item We need to decide where to start in terms of values of $p$ and $q$. Let's tabulate some AIC values for a range of different choices of $p$ and $q$.

\item In the code below, note the use of \code{kable} for formatting HTML tables when using Rmarkdown. The \code{"<b>"} and \code{""</b>"} tags in \code{dimnames} make the rownames boldface in HTML. By default, only column names are boldface in standard HTML.

\ei

\end{frame}   


\begin{frame}[fragile]

<<aic_table>>=
aic_table <- function(data,P,Q){
  table <- matrix(NA,(P+1),(Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
       table[p+1,q+1] <- arima(data,order=c(p,0,q))$aic
    }
  }
  dimnames(table) <- list(paste("<b> AR",0:P, "</b>", sep=""),paste("MA",0:Q,sep=""))
  table
}
huron_aic_table <- aic_table(huron_depth,4,5)
require(knitr)
kable(huron_aic_table,digits=2)
@



\myquestion. What do we learn by interpreting the results in the above table of AIC values? 

\end{frame}   



\begin{frame}[fragile]

\myquestion. In what ways might we have to be careful not to over-interpret the results of this table?

\answer{\vspace{30mm}}{todo}

\end{frame} 

 \begin{frame}[fragile]

\bi

\item Let's fit the ARMA(2,1) model recommended by consideration of AIC.
\ei

<<arma21fit>>=
huron_arma21 <- arima(huron_depth,order=c(2,0,1))
huron_arma21
@
\bi
\item We can examine the roots of the AR polynomial,
\ei

<<huron_roots>>=
AR_roots <- polyroot(c(1,-coef(huron_arma21)[c("ar1","ar2")]))
AR_roots
@

\bi
\item These are just outside the unit circle, suggesting we have a stationary causal fitted ARMA.

\item However, the MA root is $\Sexpr{round(-1/coef(huron_arma21)["ma1"],2)}$, showing that the fitted model is at the threshold of non-invertibility.

\item Is this non-invertibility a problem? Let's investigate a little, using profile and bootstrap methods. The claimed standard error on the MA1 coefficient, from the Fisher information approach used by \code{arima} is small. 

\item First, we can see if the approximate confidence interval constructed using profile likelihood is in agreement with the approximate confidence interval constructed using the observed Fisher information. 

\item To do this, we need to maximize the ARMA likelihood while fixing the MA1 coefficient at a range of values. This can be done using \code{arima} as follows. Note that the \code{fixed} argument expects a vector of length $p+q+1$ corresponding to a concatenated vector $(\ar_{1:p},\ma_{1:q}, \mu)$. Somehow, the Gaussian white noise variance, $\sigma^2$, is not included in this representation. Parameters with \code{NA} entries in \code{fixed} are estimated.

\ei

\end{frame}   


\begin{frame}[fragile]

<<huron_profile>>=
K <- 500
ma1 <- seq(from=0.2,to=1.1,length=K)
profile_loglik <- rep(NA,K)
for(k in 1:K){
   profile_loglik[k] <- logLik(arima(huron_depth,order=c(2,0,1),
      fixed=c(NA,NA,ma1[k],NA)))
}
plot(profile_loglik~ma1,ty="l")
@

\myquestion. Interpret the profile likelihood plot for $\ma_1$. 

\bi

\item What do you conclude about the Fisher information confidence interval proposed by \code{arima}?

\item When do you think the Fisher information confidence interval may be reliable?

\item Is this profile likelihood plot, and its statistical interpretation, reliable? How do you support your opinion on this?
\ei

\end{frame} 


  \begin{frame}[fragile]

\frametitle{A simulation study}

<<simA>>=
set.seed(57892330)
J <- 1000
params <- coef(huron_arma21)
ar <- params[grep("^ar",names(params))]
ma <- params[grep("^ma",names(params))]
intercept <- params["intercept"]
sigma <- sqrt(huron_arma21$sigma2)
theta <- matrix(NA,nrow=J,ncol=length(params),dimnames=list(NULL,names(params)))
for(j in 1:J){
   Y_j <- arima.sim(
      list(ar=ar,ma=ma),
      n=length(huron_depth),
      sd=sigma
   )+intercept
   theta[j,] <- coef(arima(Y_j,order=c(2,0,1)))
}
hist(theta[,"ma1"],freq=FALSE) 
@

\bi
\item This seems consistent with the profile likelihood plot.

\item A density plot shows this similarity even more clearly.

\ei

\end{frame}   

\begin{frame}[fragile]

<<density>>=
plot(density(theta[,"ma1"],bw=0.05))
@

\bi
\item Here, We look at the raw plot for instructional purposes. For a report, one should improve the default axis labels and title.

\item Note that \code{arima} transforms the model to invertibility. Thus, the estimated value of $\theta_1$ can only fall in the interval $(-1,1)$ but can be arbitrarily close to $-1$ or $1$. 
\ei

<<range>>=
range(theta[,"ma1"])
@

\bi
 \item Estimated densities outside $[-1,1]$ are artifacts of the density estimation procedure. 

 \item How would you refine this procedure to get a density estimate respecting the range of the parameter estimation procedure?

\item To understand what is going on better, it is helpful to do another simulation study for which we fit ARMA(2,1) when the true model is AR(1).

\item When doing simulation studies, it is helpful to use multicore computing, which most of us have on our machines nowadays. 

\item A basic approach to multicore statistical computing is to tell R you want it to look for available processors, using the \code{doParallel} package.
\ei

\end{frame}   

\begin{frame}[fragile]

<<parallel-setup,cache=FALSE>>=
require(doParallel)
registerDoParallel()
@

We can use \code{foreach} to carry out a parallel \code{for} loop where jobs are sent to different processors.


<<simB>>=
J <- 1000
huron_ar1 <- arima(huron_depth,order=c(1,0,0))
params <- coef(huron_ar1)
ar <- params[grep("^ar",names(params))]
intercept <- params["intercept"]
sigma <- sqrt(huron_ar1$sigma2)
t1 <- system.time(
  huron_sim <- foreach(j=1:J) %dopar% {
     Y_j <- arima.sim(list(ar=ar),n=length(huron_depth),sd=sigma)+intercept
     try(coef(arima(Y_j,order=c(2,0,1))))
  }
) 
@

\bi
\item Some of these \code{arima} calls did not successfully produce parameter estimates. The \code{try} function lets the simulation proceed despite these errors. Let's see how many of them fail:
\ei

<<out, cache=FALSE>>=
sum(sapply(huron_sim, function(x) inherits(x,"try-error"))) 
@


\end{frame}   

\begin{frame}[fragile]

\bi

\item Now, for the remaining ones, we can look at the resulting estimates of the MA1 component:

\ei

<<histB, cache=FALSE>>=  
ma1 <- unlist(lapply(huron_sim,function(x) if(!inherits(x,"try-error"))x["ma1"] else NULL ))
hist(ma1,breaks=50)  
@

\bi
\item When the true model is AR1 and we fit ARMA(2,1), it seems that we often obtain a model with estimated MA1 coefficient on the boundary of invertibility.

\item It is clear from this that we cannot reject an AR1 hypothesis, even though the Fisher information based analysis appears to give strong evidence that the data should be modeled with a nonzero MA1 coefficient. 

\item It may be sensible to avoid fitted models too close to the boundary of invertibility. This is a reason not to blindly accept whatever model AIC might suggest. 
\ei

\end{frame} 

\begin{frame}[fragile]

\myquestion. What else could we look for to help diagnose, and understand, this kind of model fitting problem? Hint: pay some more attention to the roots of the fitted ARMA(2,1) model.

\answer{\vspace{40mm}}{todo}

\end{frame}   

\begin{frame}[fragile]

\frametitle{Assessing the numerical correctness of evaluation and maximization of the likelihood function}

\bi
\item We can probably suppose that \code{arima} has negligible numerical error in evaluating the likelihood. 

 \item Likelihood evaluation is a linear algebra computation which should be numerically stable away from singularities. 

 \item Possibly, numerical problems could arise for models very close to reducibility (canceling AR and MA roots).

\item Numerical optimization is more problematic. 

 \item \code{arima} calls the general purpose optimization routine \code{optim}. 

 \item We know the likelihood surface can be multimodal and have nonlinear ridges; both these are consequences of the possibility of reducibility or near reducibility (AR and MA roots which almost cancel). 

 \item No optimization procedure is reliable for maximizing awkward, non-convex functions.
 
 \item Evidence for imperfect maximization (assuming negligible likelihood evaluation error) can be found in the above AIC table, reproduced here:
\ei

\end{frame} 



\begin{frame}[fragile]


<<repeated_aic,echo=FALSE>>=
require(knitr)
kable(huron_aic_table,digits=2)
@


\myquestion. How is this table inconsistent with perfect maximization?
Here are two hints:
\bi
    \item Recall that, for nested hypotheses $H^{\langle 0\rangle}\subset H^{\langle 1\rangle}$, the likelihood maximized over $H^{\langle 1\rangle}$ cannot be less than the likelihood maximized over $H^{\langle 0\rangle}$. 

    \item Recall also the definition of AIC,

AIC = -2$\times$ maximized log likelihood $+$ 2$\times$ number of parameters
\ei

\answer{\vspace{30mm}}{todo}

\end{frame}   


\begin{frame}[fragile]
\frametitle{Acknowledgments and License}

\bi
\item These notes build on previous versions at \url{ionides.github.io/531w16} and \url{ionides.github.io/531w18}.
\item
Licensed under the Creative Commons attribution-noncommercial license, \url{http://creativecommons.org/licenses/by-nc/3.0/}.
Please share and remix noncommercially, mentioning its origin.  
\includegraphics[width=2cm]{cc-by-nc.png}
\ei

\end{frame}


%\begin{frame}[allowframebreaks]
%\frametitle{References}
%\bibliography{notes03.bib}
%\end{frame}

\end{document}


