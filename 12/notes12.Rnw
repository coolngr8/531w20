%\documentclass[handout]{beamer}
\documentclass{beamer}
\usepackage{natbib}
\bibliographystyle{dcu}
\input{../header.tex}

\newcommand\CHAPTER{12}


\newcommand\eqspace{\quad\quad\quad}
\newcommand\eqskip{\vspace{2.5mm}}

\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta{\Delta}

\newcommand\myeq{\hspace{10mm}}

% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{{\color{blue}{#2}}} % to show answers
\newcommand\answer[2]{#1} % to show blank space

<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
 ANS=FALSE
@

\usepackage{bbm} % for blackboard bold 1

\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping

@

<<setup,echo=F,results=F,cache=F>>=
myround<- function (x, digits = 1) {
  # taken from the broman package
  if (digits < 1) 
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}

set.seed(2050320976)

options(
  keep.source=TRUE,
  encoding="UTF-8"
)

@



<<prelims,echo=F,cache=F>>=
set.seed(594709947L)
library(ggplot2)
theme_set(theme_bw())
library(plyr)
library(reshape2)
library(foreach)
#library(doMC)
library(pomp)
stopifnot(packageVersion("pomp")>="2.0")
@


\begin{frame}[fragile]
\frametitle{Chapter \CHAPTER. Practical likelihood-based inference for POMP models}

\hspace{3cm} {\large \bf Objectives}

\vspace{3mm}

\begin{enumerate}

\item Understanding the simplest \myemph{particle filter} and how it enables Monte Carlo solution of the POMP filtering and prediction recursions and computation of a Monte Carlo evaluation of the likelihood.

\item Using the particle filter to visualize and exploring likelihood surfaces 

\item Understanding how iterated filtering algorithms carry out repeated particle filtering operations, with randomly perturbed parameter values, in order to maximize the likelihood.

\item Carrying out likelihood-based inferences for dynamic models using simulation-based statistical methodology in the R package \package{pomp}, demonstrated by fitting an SIR model to a boarding school flu outbreak.

\end{enumerate}

\end{frame}

\begin{frame}[fragile]

\frametitle{Indirect specification of the statistical model via a simulation procedure}

\bi

\item  For simple statistical models, we may describe the model by explicitly writing the density function $f_{Y_{1:N}}(y_{1:N}\params\theta)$. 
One may then ask how to simulate a random variable $Y_{1:N}\sim f_{Y_{1:N}}(y_{1:N}\params\theta)$.

\item  For many dynamic models it is convenient to define the model via a procedure to simulate the random variable $Y_{1:N}$. 
This implicitly defines the corresponding density $f_{Y_{1:N}}(y_{1:N}\params\theta)$. 
For a complicated simulation procedure, it may be difficult or impossible to write down $f_{Y_{1:N}}(y_{1:N}\params\theta)$ exactly. 

\item  It is important for us to bear in mind that the likelihood function exists even when we don't know what it is! We can still talk about the likelihood function, and develop numerical methods that take advantage of its statistical properties.

\ei

\end{frame}   

\begin{frame}[fragile]

\frametitle{Special case: a deterministic unobserved state process}

\bi

\item  Suppose $X_{n}=x_n(\theta)$ is a known function of $\theta$ for each $n$.
Equivalently, consider fitting the deterministic skeleton for a POMP. What is the likelihood?

\item  Since the distribution of the observable random variable, $Y_n$, depends only on $X_n$ and $\theta$, and since, in particular $Y_{m}$ and $Y_{n}$ are independent given $X_{m}$ and $X_{n}$, we have 
$$\lik(\theta) = \prod_{n} f_{Y_n|X_n}(y_n^*\given x_n(\theta)\params \theta)$$ 
or 
$$\loglik(\theta) = \log\lik(\theta) = \sum_{n} \log f_{Y_n|X_n}(y_n^*\given x_n(\theta)\params \theta).$$

\item This situation includes linear or nonlinear regression: with a Gaussian measurement model, maximum likelihood corresponds to least squares.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{The skeleton of an SIR compared to the flu dataset}

<<det-example,echo=FALSE,results="hide",purl=FALSE,out.width="12cm">>=
source("bsflu.R")
 coef(sir) <- c(Beta=1.8,mu_IR=1,rho=0.9,N=2600)
 x <- trajectory(sir) 
 y <- cbind(as.data.frame(sir),x=x["H",1,])
 mutate(y,xlab=sprintf("H[%d]",day),
       ylab=sprintf("B[%d]",day)) -> y

 ggplot(data=y,
       mapping=aes(x=day,xend=day))+
  geom_point(aes(y=B),color='black',alpha=0.5)+
  geom_point(aes(y=x),color='red',alpha=0.5)+
  geom_line(aes(y=B),color='black',alpha=0.5)+
  geom_line(aes(y=x),color='red',alpha=0.5)+
  geom_text(aes(y=B,label=ylab,vjust=ifelse(day>=10,2,-1)),
    parse=TRUE,color='black')+
  geom_text(aes(y=x,label=xlab,vjust=ifelse(day>=10,-1,2)),
    parse=TRUE,color='red')+
  geom_segment(aes(y=x,yend=B),color='blue',linetype=2,alpha=0.3,
               arrow=grid::arrow(length=grid::unit(0.02,"npc")))+
  expand_limits(y=c(-20,320))+
  labs(y="")
@

\vspace{-2mm}

\bi
\item Minimizing a sum of squared errors for the skeleton is a simple comparison. We want to do better, and fit the full POMP model.
\ei

\end{frame} 

\begin{frame}[fragile]

\frametitle{An ineffective method: Likelihood by direct simulation}

To motivate the particle filter, we first introduce a simpler method which usually does not work on anything but very short time series. We calculate:
\begin{eqnarray*}
\lik(\theta)
&=&
f_{Y_{1:N}}(\data{y_{1:N}}\params\theta)
\\
&=& \! \int_{x_{0:N}} \!\! f_{X_0}(x_0\params\theta)\prod_{n=1}^{N}\!f_{Y_n|X_n}(\data{y_n}\given x_n\params \theta)\, f_{X_n|X_{n-1}}(x_n|x_{n-1}\params\theta)\, dx_{0:N}
\\
&=& \E\left[ \prod_{n=1}^{N}\!f_{Y_n|X_n}(\data{y_n}\given X_n\params \theta) \right]
\\
&\approx& \frac{1}{J}\sum_{j=1}^J \prod_{n=1}^{N}\!f_{Y_n|X_n}(\data{y_n}\given X_{n,j}\params \theta) 
\end{eqnarray*}
where we have $J$ independent simulated trajectories $\{X_{nj},n=1,\dots,N\}$, and $\approx$ is justified by the laws of large numbers.

\vspace{2mm}

\myquestion. Why is this approach ineffective unless time series is very short?

\answer{\vspace{20mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\frametitle{The particle filter}

\bi
\item Fortunately, we can compute the likelihood for a POMP model much more efficiently by using Monte Carlo representations of the prediction and filtering recursions.
\item This gives the \myemph{particle filter} algorithm, also known as sequential Monte Carlo (SMC):
\ei

\begin{enumerate}
\item Suppose $X_{n-1,j}^{F}$, $j=1,\dots,J$ is a set of $J$ points drawn from the filtering distribution at time $n-1$.

\item We obtain a sample $X_{n,j}^{P}$ of points drawn from the prediction distribution at time $t$ by simply simulating the process model:
$$X_{n,j}^{P} \sim \mathrm{process}(X_{n-1,j}^{F},\theta), \qquad j=1,\dots,J.$$

\item Having obtained $x_{n,j}^{P}$, we obtain a sample of points from the filtering distribution at time $t_n$ by \myemph{resampling} from $\big\{X_{n,j}^{P},j\in 1:J\big\}$ with weights 
$$w_{n,j}=f_{Y_n|X_n}(y^*_{n}|X^P_{n,j}\params\theta).$$

\end{enumerate}

%%\bi
%%\item  References: \citet{kitagawa87,arulampalam02,doucet01}.
%%\ei
\end{frame}

\begin{frame}[fragile]

\frametitle{The likelihood via the particle filter}

\bi
\item The Monte Carlo principle tells us that the conditional likelihood
\begin{eqnarray*}
\lik_n(\theta) &=& f_{Y_n|Y_{1:n-1}}(y^*_n|y^*_{1:n-1}\params\theta)
\\
&=& 
\int
f_{Y_n|X_n}(y^*_{n}|x_{n}\params\theta)\,f_{X_n|Y_{1:n-1}}(x_{n}|y^*_{1:n-1}\params\theta)\, dx_n
\end{eqnarray*}
is approximated by
$$\hat\lik_n(\theta)  \approx \frac{1}{N}\,\sum_j\, f_{Y_n|X_n}(y^*_{n}|X_{n,j}^{P}\params\theta).$$

%% \item We can iterate this procedure through the data, one step at a time, alternately simulating and resampling, until we reach $n=N$.

\item The full log likelihood then has approximation
\begin{eqnarray*}\loglik(\theta) 
&=& \log{\lik(\theta)} 
\\
&=& \sum_n \log{\lik_n(\theta)}
\\
&\approx& \sum_n\log\hat\lik_n(\theta).
\end{eqnarray*}


\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Recall the SIR model from Chapter 11}
<<sir-sim1,out.width="10cm">>=
sims <- simulate(sir,params=c(Beta=1.8,mu_IR=1,rho=0.9,N=2600),
  nsim=20,format="data.frame",include=TRUE)
ggplot(sims,mapping=aes(x=day,y=B,group=.id,color=.id=="data"))+
  geom_line()+guides(color=FALSE)
@

\end{frame}



\begin{frame}[fragile]


\frametitle{Sequential Monte Carlo in \package{pomp}}

 In \package{pomp}, the basic particle filter is implemented in the command \code{pfilter}.
We must choose the number of particles to use by setting the \code{Np} argument.

<<sir-pfilter-1,results='markup',cache=T>>=
pf <- pfilter(sir,Np=5000,params=c(Beta=2,mu_IR=1,rho=0.8,N=2600))
logLik(pf)
@

Running a few particle filters gives an estimate of Monte Carlo variability:

<<sir-pfilter-2,results='markup',cache=T>>=
pf <- replicate(10,
  pfilter(sir,Np=5000,params=c(Beta=2,mu_IR=1,rho=0.8,N=2600))
)
print(ll <- sapply(pf,logLik))
@



\end{frame}


\begin{frame}[fragile]

\frametitle{Unbiasedness of the particle filter likelihood estimate}
\bi
\item  A theoretical property of the particle filter is that it gives us an unbiased Monte Carlo estimate of the likelihood.

\item  This theoretical property, combined with Jensen's inequality and the observation that $\log(x)$ is a concave function, ensures that the average of the log likelihoods from many particle filter replications will have negative bias as a Monte Carlo estimator of the log likelihood.

\item For other quantities, the particle filter has bias which decreases to zero as the number of particles increases. It is a special property of the likelihood that the bias in this case is zero. 

\ei

\end{frame}  

\begin{frame}[fragile]
\frametitle{Averaging log likelihood estimates}
\bi
\item The unbiased property of the particle filter for the \myemph{likelihood} suggests we average log likelihood estimates on the natural scale.
\item After returning to the log scale, a standard error is available from the delta method.
\item \code{logmeanexp()} does these computations.
\ei
<<logmeanexp>>=
logmeanexp(ll,se=TRUE)
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Graphing the likelihood function: The likelihood surface}

\bi

\item  We can think of the geometric surface defined by the likelihood function. 

\item  If $\Theta$ is two-dimensional, then the surface $\ell(\theta)$ has features like a landscape: local maxima of $\ell(\theta)$ are peaks,  local minima are valleys,  peaks may be separated by a valley or may be joined by a ridge. 

\item Moving along a ridge, you may be able to go from one peak to the other without losing much elevation. Narrow ridges can be easy to fall off, and hard to get back on to.

\item  In higher dimensions, one can still think of peaks and valleys and ridges. However, as the dimension increases it quickly becomes hard to imagine the surface.

\item  To get an idea of what the likelihood surface looks like in the neighborhood of the default parameter set supplied by \code{sir}, we can construct a \myemph{likelihood slice}. A slice varies one parameter at a time, fixing the others.


\ei
\end{frame}

\begin{frame}[fragile]

\frametitle{Parallel statistical computing in \Rlanguage}

Parallelization is helpful for computational statistics. You can tell {\Rlanguage} to access multiple processors on your machine.

<<parallel-setup,cache=FALSE>>=
library(doParallel)
registerDoParallel()
library(doRNG)
registerDoRNG(3899882)
@

\bi
\item \code{registerDoRNG} sets up a parallel random number generator.
\item Most statistical computing is \myemph{embarrassingly parallel}.
\item This means we simply have to learn to use a parallel for loop via \code{foreach()}
\ei
\end{frame}

\begin{frame}[fragile]
\frametitle{Slicing in the $\beta$ and $\mu_{IR}$ directions}

<<sir-like-slice,cache=TRUE,results='hide'>>=
p <- sliceDesign(
  c(Beta=2,mu_IR=1,rho=0.8,N=2600),
  Beta=rep(seq(from=0.5,to=4,length=40),each=3),
  mu_IR=rep(seq(from=0.5,to=2,length=40),each=3)) 

foreach (theta=iter(p,"row"),
  .combine=rbind,.inorder=FALSE) %dopar% {
    pfilter(sir,params=unlist(theta),Np=5000) -> pf
    theta$loglik <- logLik(pf)
    theta
  } -> p
@
\bi
\item \code{sliceDesign()} builds a dataframe where each row is a parameter vector.
\item \code{foreach()} carries out particle filters for each row, distributed across processors
\ei

\end{frame}  

 \begin{frame}[fragile]

\myquestion. Write down the definition of a likelihood slice in mathematical notation.

\answer{\vspace{20mm}}{todo}

\myquestion. Explain the difference between a likelihood slice and a likelihood profile,

(a) from a computational perspective.

\answer{\vspace{20mm}}{todo}

(b) from the perspective of constructing confidence intervals and hypothesis tests.

\answer{\vspace{20mm}}{todo}

\end{frame}   

\begin{frame}[fragile]

<<sir-like-slice-plot,cache=TRUE,results="hide",echo=F,out.width="8cm">>=
foreach (v=c("Beta","mu_IR")) %do% 
{
  x <- subset(p,slice==v)
  plot(x[[v]],x$loglik,xlab=v,ylab="loglik")
}

@

\end{frame}

\begin{frame}[fragile]

\frametitle{A two-dimensionsional likelihood cross-section}

\bi
\item
Slices offer a limited perspective on the geometry of the likelihood surface.
With two parameters, we can evaluate the likelihood at a grid of points and visualize the surface.
\item
We compute a likelihood cross-section in the $\beta$ and $\mu_{IR}$ directions.
\ei

<<sir-grid1,cache=TRUE>>=
expand.grid(Beta=seq(from=1,to=4,length=50),
            mu_IR=seq(from=0.7,to=3,length=50),
            rho=0.8,
            N=2600) -> p

foreach (theta=iter(p,"row"),.combine=rbind,
  .inorder=FALSE) %dopar% {
     pfilter(sir,params=unlist(theta),Np=5000) -> pf
     theta$loglik <- logLik(pf)
     theta
  } -> p
@

\end{frame}

\begin{frame}[fragile]


<<sir-grid1-plot,cache=TRUE,out.width="10cm">>=
pp <- mutate(p,loglik=ifelse(loglik>max(loglik)-100,loglik,NA))
ggplot(data=pp,mapping=aes(x=Beta,y=mu_IR,z=loglik,fill=loglik))+
  geom_tile(color=NA)+
  geom_contour(color='black',binwidth=3)+
  scale_fill_gradient()+
  labs(x=expression(beta),y=expression(mu[IR]))
@

\end{frame}


\begin{frame}[fragile]

<<sir-grid2,eval=FALSE,include=FALSE,purl=FALSE,fig.show=FALSE,cache=TRUE>>=
expand.grid(Beta=seq(from=1,to=3,length=50),
            mu_IR=1,
            rho=0.8,
            N=seq(from=1600,to=3000,length=50)) -> p

foreach (theta=iter(p,"row"),.combine=rbind,
         .inorder=FALSE) %dopar% 
 {
   pfilter(sir,params=unlist(theta),Np=5000) -> pf
   theta$loglik <- logLik(pf)
   theta
 } -> p

@
<<sir-grid2-plot,eval=FALSE,include=FALSE,purl=FALSE>>=
pp <- mutate(p,loglik=ifelse(loglik>max(loglik)-100,loglik,NA))
ggplot(data=pp,mapping=aes(x=Beta,y=N,z=loglik,fill=loglik))+
  geom_tile(color=NA)+
  geom_contour(color='black',binwidth=3)+
  scale_fill_gradient()+
  labs(x=expression(beta),y=expression(N))
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Likelihood maximization for the boarding school flu example}
\bi
\item  We saw above that the default parameter set for the 'bsflu' pomp object is not particularly close to the MLE.

\item  One way to find the MLE is to try optimizing the estimated likelihood, computed by the particle filter, directly.

\item  There are many optimization algorithms to choose from, and many implemented in R.

\item  Three issues arise immediately (discussed more on following slides):
\ei

\begin{enumerate}

\item The particle filter gives us a stochastic estimate of the likelihood.

\item Lack of derivatives.

\item Constrained parameters.

\end{enumerate}

\end{frame}  

\begin{frame}[fragile]
\frametitle{1. The particle filter gives us a stochastic estimate of the likelihood}
\bi
\item  We can reduce this variability by making the number of particles, \code{Np}, larger. 
However, we cannot make it go away.
\item  We can use deterministic optimization, by fixing the seed of the pseudo-random number generator, a side effect is that the objective function can become jagged, with many small local maxima and minima.
\item If we use stochastic optimization, the underlying surface may be smoother but we see it only with Monte Carlo noise.
\item 
This is the trade-off between a noisy and a rough objective function.
\ei
\end{frame}

\begin{frame}[fragile]
\frametitle{2. Lack of derivatives}
\bi
\item Because the particle filter gives us just an estimate of the likelihood and no information about the derivative, we must choose an algorithm that is ``derivative-free''.
\item
There are many such, but we can expect less efficiency than would be possible with derivative information.
\item
Note that finite differencing (i.e., a direct numerical estimate of the derivative) is not an especially promising way of constructing derivatives in the presence of Monte Carlo noise.
\ei
\end{frame}


\begin{frame}[fragile]
\frametitle{3. Constrained parameters}

\bi
\item For the boarding school flu example, the parameters are constrained to be positive, and $\rho < 1$.
\item Such constraints are common, especially for rate parameters.
\item We must select an optimizer that can solve this \myemph{constrained maximization problem}, or figure out some of way of turning it into an unconstrained maximization problem.
\item 
For the latter, we transform the parameters onto a scale on which there are no constraints.
\ei

\end{frame}


\begin{frame}[fragile]

\frametitle{Cautions about parameter estimation for dynamic models}

\bi
\item  When we propose a mechanistic model for a system, we have some idea of what we intend parameters to mean. In our epidemiology example, we interpret parameters as  a reporting rate, a contact rate between individuals, an immigration rate, a duration of immunity, etc. 

\item  The data and the parameter estimation procedure do not know about our intended interpretation of the model. Parameter estimates statistically consistent with the data may be absurd according to the scientific reasoning used to build the model. 

\item  This can arise as a consequence of weak identifiability, or it can be a warning that the data show our model does not represent reality in the way we had hoped.

\item  
Fixing some parameters at known, scienficially reasonable values is tempting. However, this can suppress warnings that the data were giving about weaknesses in the model, or in our biological interpretation of it.

\ei

\end{frame}  

\begin{frame}[fragile]

\frametitle{An iterated filtering algorithm (IF2)}

\bi

\item  We use the IF2 algorithm of \citep{ionides15}.

\item  A particle filter is carried out with the parameter vector for each particle doing a random walk.

\item  At the end of the time series, the collection of parameter vectors is recycled as starting parameters for a new particle filter with a smaller random walk variance.

\item  Theoretically, this procedure converges toward the region of parameter space maximizing the maximum likelihood.

\item  Empirically, we can test this claim on examples.
\ei

\end{frame}


\begin{frame}[fragile]

\frametitle{IF2 algorithm input and output}

\myemph{model input}:
Simulators for $f_{X_0}(x_0;\theta)$ and $f_{X_n|X_{n-1}}(x_n| x_{n-1}; \theta)$;
evaluator for $f_{Y_n|X_n}(y_n| x_n;\theta)$;
data, $y^*_{1:N}$ 

\vspace{3mm}

\myemph{algorithmic parameters}:
Number of iterations, $M$;
number of particles, $J$;
initial parameter swarm, $\{\Theta^0_j, j=1,\dots,J\}$;
perturbation density, $h_n(\theta|\varphi;\sigma)$;
perturbation scale, $\sigma_{1{:}M}$ 

\vspace{3mm}

\myemph{output}:
Final parameter swarm, $\{\Theta^M_j, j=1,\dots,J\}$ 

\vspace{10mm}

\bi
\item This algorithm requires \code{rprocess} but not \code{dprocess}. It is \myemph{simulation-based}, also known as \myemph{plug-and-play}.
\ei
\end{frame}


\begin{frame}[fragile]

\frametitle{IF2 algorithm pseudocode}

1. $\quad$ For $m$ in $1{:} M$
\\
2. $\quad\quad\quad$ $\Theta^{F,m}_{0,j}\sim h_0(\theta|\Theta^{m-1}_{j}; \sigma_m)$ for $j$ in $1{:} J$
\\
3. $\quad\quad\quad$ $X_{0,j}^{F,m}\sim f_{X_0}(x_0 ; \Theta^{F,m}_{0,j})$ for $j$ in $1{:} J$
\\
4. $\quad\quad\quad$ For $n$ in $1{:} N$
\\
5. $\quad\quad\quad\quad\quad$ $\Theta^{P,m}_{n,j}\sim h_n(\theta|\Theta^{F,m}_{n-1,j},\sigma_m)$ for $j$ in $1{:} J$
\\
6. $\quad\quad\quad\quad\quad$ $X_{n,j}^{P,m}\sim f_{X_n|X_{n-1}}(x_n | X^{F,m}_{n-1,j}; \Theta^{P,m}_j)$ for $j$ in $1{:} J$
\\
7. $\quad\quad\quad\quad\quad$ $w_{n,j}^m = f_{Y_n|X_n}(y^*_n| X_{n,j}^{P,m} ; \Theta^{P,m}_{n,j})$ for $j$ in $1{:} J$
\\
8. $\quad\quad\quad\quad\quad$ Draw $k_{1{:}J}$ with $P[k_j=i]=  w_{n,i}^m\Big/\sum_{u=1}^J w_{n,u}^m$
\\
9.  $\quad\quad\quad\quad\quad$ $\Theta^{F,m}_{n,j}=\Theta^{P,m}_{n,k_j}$ and $X^{F,m}_{n,j}=X^{P,m}_{n,k_j}$ for $j$ in $1{:} J$
\\
10. $\quad\quad\quad$ End For
\\
11. $\quad\quad\quad$ Set $\Theta^{m}_{j}=\Theta^{F,m}_{N,j}$ for $j$ in $1{:} J$
\\
12. $\quad$ End For

\end{frame}



\begin{frame}[fragile]
\frametitle{Comments on the IF2 algorithm}

\bi

\item  The $N$ loop (lines 4 through 10) is a basic particle filter applied to a model with stochastic perturbations to the parameters.

\item  The $M$ loop repeats this particle filter with decreasing perturbations.

\item  The superscript $F$ in $\Theta^{F,m}_{n,j}$ and $X^{F,m}_{n,j}$ denote solutions to the filtering problem, with the particles $j=1,\dots,J$ providing a Monte Carlo representation of the conditional distribution at time $n$ given data $y^*_{1:n}$ for filtering iteration $m$.

\item  The superscript $P$ in $\Theta^{P,m}_{n,j}$ and $X^{P,m}_{n,j}$ denote solutions to the prediction problem, with the particles $j=1,\dots,J$ providing a Monte Carlo representation of the conditional distribution at time $n$ given data $y^*_{1:n-1}$ for filtering iteration $m$.

\item  The weight $w^m_{n,j}$ gives the likelihood of the data at time $n$ for particle $j$ in filtering iteration $m$.

\ei

\end{frame}

\begin{frame}[fragile]
\frametitle{Choosing the algorithmic settings for IF2}

\bi
\item  The initial parameter swarm, $\{ \Theta^0_j, j=1,\dots,J\}$, usually consists of $J$ identical replications of some starting parameter vector.

\item  $J$ should be sufficient for particle filtering. By the last iteration ($m=M$) one should not have effective sample size close to 1. 

\item  Perturbations are usually chosen to be Gaussian, with $\sigma_m$ being a scale factor for iteration $m$:
$$h_n(\theta|\varphi;\sigma) \sim N[\varphi, \sigma^2_m V_n].$$

\item $V_n$ is usually taken to be diagonal,
$$ V_n = \left( \begin{array}{ccccc}
v_{1,n}^2 & 0 & 0 & \rightarrow & 0 \\
0 & v_{2,n}^2 &  0 & \rightarrow & 0 \\
0 & 0 & v_{3,n}^2 & \rightarrow & 0 \\
\downarrow & & & \searrow & \downarrow \\
0 & 0 & 0 & \rightarrow & v_{p,n}^2 \end{array}\right).$$
\item If $\theta_i$ is a parameter that affects the dynamics or observations throughout the timeseries, it is called a \myemph{regular parameter} (RP) and we can set $ v_{i,n} = v_i$.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Initial value parameters (IVPs)}

\bi

\item  If $\theta_j$ is a parameter that affects only the initial conditions of the dynamic model, it is called an \myemph{initial value parameter} (IVP) and it is appropriate to specify
$$ v_{j,n} = \left\{\begin{array}{ll} v_j & \mbox{if $n=0$} \\
0 & \mbox{if $n>0$} \end{array}\right.$$

\item  If $\theta_k$ is a break-point parameter that models how the system changes at time $t_q$ then $\theta_k$ is like an IVP at time $t_q$ and it is appropriate to specify
$$ v_{j,n} = \left\{\begin{array}{ll} v_j & \mbox{if $n=q$} \\
0 & \mbox{if $n\neq q$} \end{array}\right.$$

\ei

\end{frame}

\begin{frame}[fragile]
\frametitle{Choosing the cooling schedule}
\bi

\item  $\sigma_{1:M}$ is called a \myemph{cooling schedule}, following a thermodynamic analogy popularized by \myemph{simulated annealing}. As $\sigma_m$ becomes small, the system cools toward a \myemph{freezing point}.

\item The freezing point should be close to the lowest-energy state of the system, i.e., the MLE.


\item  We aim to transform parameters so that (on the estimation scale) they are unconstrained and have uncertainty on the order of 1 unit.
\item Usually, we do a logarithmic transformation of positive parameters and a logistic transformation of $[0,1]$ valued parameters.
\item On this scale, it is surprisingly often effective to take
$$\begin{array}{rcll}
v_i &=& 0.02 &\mbox{for regular parameters (RPs)}
\\
v_j &=& 0.1 &  \mbox{for initial value parameters (IVPs)}
\end{array}
$$

\item  We suppose that $\sigma_1=1$, since the scale of the parameters is addressed by the matrix $V_n$ . Early on in an investigation, one might take $M=100$ and $\sigma_M=0.1$. Later on, consideration of diagnostic plots may suggest refinements. 

\ei

\end{frame}

\begin{frame}[fragile]


\frametitle{Applying IF2 to a boarding school influenza outbreak}

\bi

\item  We redevelop a model for the boarding school flu data, as template for the cases studies to follow.


\item  We use an $SIR_1R_2R_3$ model with state $X(t)=(S(t),I(t),R_1(t),R_2(t),R_3(t) )$ giving the number of individuals in the susceptible and infectious categories, and three stages of recovery. 

\item  The recovery stages, $R_1$, $R_2$ and $R_3$, are all modeled to be non-contagious. 

\item  $R_1$ consists of individuals who are bed-confined if they show symptoms. 

\item  $R_2$ consists of individuals who are convalescent if they showed symptoms.

\item  $R_3$ consists of recovered individuals who have returned to schoolwork if they were symtomatic.  


\ei

\end{frame}

\begin{frame}[fragile]

<<bsflu_rprocess>>=
bsflu_rprocess <- "
  double dN_SI = rbinom(S,1-exp(-Beta*I*dt));
  double dN_IR1 = rbinom(I,1-exp(-dt*mu_IR));
  double dN_R1R2 = rbinom(R1,1-exp(-dt*mu_R1));
  double dN_R2R3 = rbinom(R2,1-exp(-dt*mu_R2));
  S -= dN_SI;
  I += dN_SI - dN_IR1;
  R1 += dN_IR1 - dN_R1R2;
  R2 += dN_R1R2 - dN_R2R3;
"
@

We do not need a representation of $R_3$ since the total population size is fixed at $P=763$ and hence $R_3(t)=P-S(t)-I(t)-R_1(t)-R_2(t)$. 

\end{frame}

\begin{frame}[fragile]

\frametitle{The measurement model}

\bi

\item  The observation on day $n$ of the observed epidemic (with $n=1$ being 22 January) has two measurements: the numbers of children who are bed-confined and convalescent. 

\item To start simply, we will just take $Y_n= B_n$ with $B_n\sim\mathrm{Poisson}(\rho R_1(t_n))$.

\item  Here, $\rho$ is a reporting rate corresponding to the chance of being symptomatic.

\item  Multivariate measurement models can be coded in \package{pomp}.
\ei

\end{frame}

\begin{frame}[fragile]

<<bsflu_measure>>=
bsflu_dmeasure <- "
  lik = dpois(B,rho*R1+1e-10,give_log);
"

bsflu_rmeasure <- "
  B = rpois(rho*R1+1e-10);
"
@

\bi
\item The \code{1e-10} \myemph{tolerance} value stops the code crashing when all particles have \code{R1=0}.
\ei

\end{frame}


\begin{frame}[fragile]

\frametitle{Initial conditions}

\bi

\item  The index case for the epidemic was proposed to be a boy returning to Britain from Hong Kong, who was reported to have a transient febrile illness
from 15 to 18 January.
\item
It would therefore be reasonable to initialize the epidemic with $I(t_0)=1$ at $t_0=-6$.
\item This is tricky to reconcile with the rest of the data; we simply initialize with  $I(t_0)=1$ at $t_0=0$.

\ei

<<bsflu_rinit>>=
bsflu_rinit <- "
 S=762;
 I=1;
 R1=0;
 R2=0;
"
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Limitations and weaknesses}

\bi

\item  All models have limitations and weaknesses. Writing down and fitting a model is a starting point for data analysis, not an end point. In particular, one should try model variations.

\item One could include a latency period for infections.

\item One could modify the model to give a better description of the bed-confinement and convalescence processes.

\item Ten individuals received antibiotics for secondary infections, and they had longer bed-confinement and convalescence times. A model including the convalescence data might have to address this.

\ei

\end{frame}

\begin{frame}[fragile]

<<Csnippets_bsflu1>>=
bsflu_statenames <- c("S","I","R1","R2")
bsflu_paramnames <- c("Beta","mu_IR","rho","mu_R1","mu_R2")
@

\bi
\item The names are needed only for compiling the Csnippets, but writing them down also helps clarify the map from the mathematical representation of the model to the computational representation.
\ei

\end{frame}

\begin{frame}[fragile]

<<Csnippets_bsflu2>>=

bsflu_data <- read.table("bsflu_data.txt")
bsflu_data <- subset(bsflu_data,select=c(day,B))

bsflu2 <- pomp(
  data=bsflu_data,
  times="day",
  t0=0,
  rprocess=euler(
    step.fun=Csnippet(bsflu_rprocess),
    delta.t=1/12),
  rmeasure=Csnippet(bsflu_rmeasure),
  dmeasure=Csnippet(bsflu_dmeasure),
  partrans=parameter_trans(
    log=c("Beta","mu_IR","mu_R1","mu_R2"),
    logit="rho"),
  statenames=bsflu_statenames,
  paramnames=bsflu_paramnames,
  rinit=Csnippet(bsflu_rinit)
)

@

\end{frame}

\begin{frame}[fragile]

\frametitle{Controlling the run time}

\bi
\item  To develop and debug code, we want a version that runs extra quickly. Setting \code{run_level=1} gives a low number of particles, \code{Np}, etc.

\item For this model and data, \code{Np=5000} and \code{Nmif=200} are empirically around the minimum to get stable results with an error in the likelihood of order 1 log unit for this example. This is done by \code{run_level=2}.

\item For more precise time-consuming computations, \code{run_level=3}.
\ei
<<run_level>>=
run_level <- 1
switch(run_level, {
  bsflu_Np=100; bsflu_Nmif=10; bsflu_Neval=10;
  bsflu_Nglobal=10; bsflu_Nlocal=10
  },{
  bsflu_Np=20000; bsflu_Nmif=100; bsflu_Neval=10;
  bsflu_Nglobal=10; bsflu_Nlocal=10
  },{
  bsflu_Np=60000; bsflu_Nmif=300; bsflu_Neval=10;
  bsflu_Nglobal=100; bsflu_Nlocal=20}
)
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Running a particle filter}

\bi
\item 
Before running iterated filtering, we check that the basic particle filter is working.
\item We test \code{pfilter} on a previously computed point estimate read in from \code{bsflu_params.csv}
\ei
<<bsflu_params>>=
bsflu_params <- data.matrix(
  read.table("mif_bsflu_params.csv",
  row.names=NULL,header=TRUE))
which_mle <- which.max(bsflu_params[,"logLik"])
bsflu_mle <- bsflu_params[which_mle,][bsflu_paramnames]
@

\bi
\item
We treat $\mu_{R_1}$ and  $\mu_{R_2}$ as known, fixed at the empirical mean of the bed-confinement and convalescence times for symptomatic cases:
\ei

<<fixed_params>>=
bsflu_fixed_params <- c(mu_R1=1/(sum(bsflu_data$B)/512),
  mu_R2=1/(sum(bsflu_data$C)/512) )
@

\end{frame}


\begin{frame}[fragile]

\bi

\item  We proceed to carry out replicated particle filters at this tentative MLE:
\ei

<<pf>>=
stew(file=sprintf("pf-%d.rda",run_level),{
  t_pf <- system.time(
    pf <- foreach(i=1:20,.packages='pomp') %dopar% try(
                    pfilter(bsflu2,params=bsflu_mle,Np=bsflu_Np)
                  )
  )
  
},seed=1320290398,kind="L'Ecuyer")

(L_pf <- logmeanexp(sapply(pf,logLik),se=TRUE))
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Caching computations in Rmarkdown}

\bi

\item  In  \Sexpr{round(t_pf["elapsed"],1)} seconds, we obtain an unbiased likelihood estimate of \Sexpr{round(L_pf[1],2)} with a Monte standard error of \Sexpr{round(L_pf[2],2)}.

\item  It is not unusual for computations in a POMP analysis to take hours to run on many cores.

\item  The computations for a final version of a manuscript may take days.

\item  Usually, we use some mechanism like the different values of \code{run_level} so that preliminary versions of the manuscript take less time to run.

\item  However, when editing the text or working on a different part of the manuscript, we don't want to re-run long pieces of code.

\item  Saving results so that the code is only re-run when necessary is called \myemph{caching}.

\ei

\end{frame}

\begin{frame}[fragile]
\bi
\item  You may already be familiar with Rmarkdown's own version of caching. 

\item  In the notes, we tell Rmarkdown to cache. For example, in (notes13.Rmd) the first R chunk, called \code{knitr-opts}, contains the following:

\ei

<<set_cache, eval=FALSE>>=
opts_chunk$set(
  cache=TRUE,
  )
@

\bi
\item  Rmarkdown uses a library called \code{knitr} to process the Rmd file, so options for Rmarkdown are formally options for knitr.

\item  Having set the option \code{cache=TRUE}, Rmarkdown caches every chunk, meaning that a chunk will only be re-run if code in that chunk is edited.

\item  You can force Rmarkdown to recompute all the chunks by deleting the \code{cache} subdirectory.
\ei
\end{frame}


\begin{frame}[fragile]
\frametitle{Practical advice for caching}
\bi
\item  What if changes elsewhere in the document affect the proper evaluation of your chunk, but you didn't edit any of the code in the chunk itself?
Rmarkdown will get this wrong. \myemph{It will not recompute the chunk}.

\item  A perfect caching system doesn't exist. \myemph{Always delete the entire cache and rebuild a fresh cache before finishing a manuscript.}

\item  Rmarkdown caching is good for relatively small computations, such as producing figures or things that may take a minute or two and are annoying if you have to recompute them every time you make any edits to the text.

\item  For longer computations, it is good to have full manual control. In \package{pomp}, this is provided by two related functions, \code{stew} and \code{bake}.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{\code{stew} and \code{bake}}
\bi
\item  Notice the function \code{stew} in the replicated particle filter code above.

\item  Here, \code{stew} looks for a file called \code{pf-[run_level].rda}. 

\item  If it finds this file, it simply loads the contents of this file.

\item  If the file doesn't exist, it carries out the specified computation and saves it in a file of this name.

\item  \code{bake} is similar to \code{stew}. The difference is that \code{bake} uses \code{readRDS} and \code{saveRDS}, whereas \code{stew} uses \code{load} and \code{save}. 

\item  either way, the computation will not be re-run unless you manually delete \code{pf-[run_level].rda}.

\item  \code{stew} and \code{bake} reset the seed appropriately whether or not the computation is recomputed. Othewise, caching risks adverse consequences for reproducibility.

\ei

\end{frame}   

\begin{frame}[fragile]


\frametitle{A local search of the likelihood surface}
%\bi
%\item  Let's carry out a local search using \code{mif2} around this previously identified MLE. For that, we need to set the \code{rw.sd} and \code{cooling.fraction.50} algorithmic parameters:
%\ei

\vspace{-2mm}

<<box_search_local>>=
bsflu_rw.sd <- 0.02; bsflu_cooling.fraction.50 <- 0.5
stew(file=sprintf("local_search-%d.rda",run_level),{
  t_local <- system.time({
  mifs_local <- foreach(i=1:bsflu_Nlocal,
    .packages='pomp', .combine=c) %dopar%  {
      mif2(bsflu2,
        params=bsflu_mle,
        Np=bsflu_Np,
        Nmif=bsflu_Nmif,
        cooling.fraction.50=bsflu_cooling.fraction.50,
        rw.sd=rw.sd(
          Beta=bsflu_rw.sd,
          mu_IR=bsflu_rw.sd,
          rho=bsflu_rw.sd)
      )
    }
  })
},seed=900242057,kind="L'Ecuyer")
@

\end{frame}
\begin{frame}[fragile]
\bi
\item  The final filtering iteration carried out by \code{mif2} generates an approximation to the likelihood at the resulting point estimate.
\item This approximation is not usually good enough for reliable inference.
Partly, because some parameter perturbations remain in the last filtering iteration. Partly, because \code{mif2} may be carried out with fewer particles than necessary for a good likelihood evaluation.

\item  Therefore, we evaluate the likelihood, together with a standard error, using replicated particle filters at each point estimate:
\ei

<<lik_local_eval>>=
stew(file=sprintf("lik_local-%d.rda",run_level),{
  t_local_eval <- system.time({
  liks_local <- foreach(i=1:bsflu_Nlocal,.combine=rbind)%dopar% {
    evals <- replicate(bsflu_Neval, logLik(
      pfilter(bsflu2,params=coef(mifs_local[[i]]),Np=bsflu_Np)))
    logmeanexp(evals, se=TRUE)
    }
  })
},seed=900242057,kind="L'Ecuyer")

results_local <- data.frame(logLik=liks_local[,1],
  logLik_se=liks_local[,2],t(sapply(mifs_local,coef)))
@


\end{frame}

\begin{frame}[fragile]

<<lik_local_summary>>=
summary(results_local$logLik,digits=5)
@

\bi
\item  This investigation took  \Sexpr{round(t_local["elapsed"]/60,1)} minutes for the maximization and \Sexpr{round(t_local_eval["elapsed"]/60,1)} minutes for the likelihood evaluation.

\item These repeated stochastic maximizations can show us the geometry of the likelihood surface in a neighborhood of this point estimate.

\item A pairs plot is helpful to interpret these results.
\ei

<<pairs_local_code,eval=FALSE,echo=T>>=
pairs(~logLik+Beta+mu_IR+rho,
  data=subset(results_local,logLik>max(logLik)-50))
@

\end{frame}   

\begin{frame}[fragile]

<<pairs_local_plot,eval=TRUE,echo=FALSE,out.width="12cm">>=
<<pairs_local_code>>
@

\myquestion. What do you conclude from this pairs plot?

\answer{\vspace{20mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\frametitle{A global likelihood search using random starting values}

\bi

\item  When carrying out parameter estimation for dynamic systems, we need to specify beginning values for both the dynamic system (in the state space) and the parameters (in the parameter space).
\item By convention, we use  \myemph{initial values} for the initialization of the dynamic system and \myemph{starting values} for initialization of the parameter search.

\item  Practical parameter estimation involves trying many starting values. One can specify a large box in parameter space that contains all parameter vectors which seem remotely sensible.

\item If an estimation method gives stable conclusions with starting values drawn randomly from this box, we have some confidence that an adequate global search has been carried out. 

\ei

\end{frame}

\begin{frame}[fragile]

\bi
\item  For our flu model, a box containing reasonable parameter values might be
\ei

<<box>>=
bsflu_box <- rbind(
  Beta=c(0.001,0.01),
  mu_IR=c(0.5,2),
  rho = c(0.5,1)
)
@

\bi

\item  We are now ready to carry out likelihood maximizations from diverse starting points. To simplify the code, we can reset only the starting parameters from \code{mifs_global[[1]]} since the rest of the call to \code{mif2} can be read in from \code{mifs_global[[1]]}:

\ei

\end{frame}

\begin{frame}[fragile]

<<box_eval>>=
stew(file=sprintf("box_eval-%d.rda",run_level),{
  t_global <- system.time({
    mifs_global <- foreach(i=1:bsflu_Nglobal,.combine=c) %dopar% {
      mif2(
        mifs_local[[1]],
        params=c(
          apply(bsflu_box,1,function(x)runif(1,x[1],x[2])),
	  bsflu_fixed_params)
      )}
  })
},seed=1270401374,kind="L'Ecuyer")
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Repeated likelihood evaluations at each point estimate}

<<lik_global_eval>>=
stew(file=sprintf("lik_global_eval-%d.rda",run_level),{
  t_global_eval <- system.time({
    liks_global <- foreach(i=1:bsflu_Nglobal,
      .combine=rbind) %dopar% {
        evals <- replicate(bsflu_Neval,
          logLik(pfilter(bsflu2,
	    params=coef(mifs_global[[i]]),Np=bsflu_Np)))
        logmeanexp(evals, se=TRUE)
    }
  })
},seed=442141592,kind="L'Ecuyer")

results_global <- data.frame(
  logLik=liks_global[,1],
  logLik_se=liks_global[,2],t(sapply(mifs_global,coef)))
summary(results_global$logLik,digits=5)
@

\end{frame}

\begin{frame}[fragile]

\frametitle{Building up evidence about the high likelihood region}

\bi
\item  It is good practice to collect successful optimization results for subsequent investigation:
\ei

<<save_params,eval=FALSE>>=
if (run_level>2) 
  write.table(rbind(results_local,results_global),
    file="mif_bsflu_params.csv",
      append=TRUE,col.names=FALSE,row.names=FALSE)
@

\bi

\item  Evaluation of the best result of this search gives a likelihood of \code{r round(max(results_global$logLik),1)} with a standard error of \Sexpr{round(results_global$logLik_se[which.max(results_global$logLik)],1)}. This took in \Sexpr{round(t_global["elapsed"]/60,1)} minutes for the maximization and \Sexpr{round(t_global_eval["elapsed"]/60,1)} minutes for the evaluation.  Plotting these diverse parameter estimates can help to give a feel for the global geometry of the likelihood surface 

\ei

<<pairs_global_code,echo=TRUE,eval=FALSE>>=
pairs(~logLik+Beta+mu_IR+rho,
  data=subset(results_global,logLik>max(logLik)-250))
@

\end{frame}

\begin{frame}[fragile]

<<pairs_global,echo=FALSE,eval=TRUE,out.width="12cm">>=
<<pairs_global_code>>
@


\bi

\item  We see that optimization attempts from diverse remote starting points end up with comparable likelihoods, even when the parameter values are quite distinct. This gives us some confidence in our maximization procedure. 
\ei

\end{frame}   

\begin{frame}[fragile]

\frametitle{Diagnostic plots for the maximization procedure}
\bi

\item  The \code{plot} method for an object of class \code{mif2d.pomp} gives  graphical convergence and filtering diagnostics for the maximization procedure.

\item  Concatenating objects of class \code{mif2d.pomp} gives a list of class \code{mif2List}.
\item The \code{plot} method for a mif2List object gives us superimposed convergence diagnostic plots from different starting values, a useful tool.

\ei

<<class_mifs_global>>=
class(mifs_global)
class(mifs_global[[1]])
@

\end{frame}

\begin{frame}[fragile]

<<mifs_global_plot,out.width="8cm",echo=FALSE>>=
plot(mifs_global)
@

\end{frame}


\begin{frame}[fragile]

\frametitle{Interpreting the diagnostics}

\begin{enumerate}

\item What would the convergence plots look like if we cooled too quickly? Or too slowly? Can you find evidence for either of these above? (The algorithmic parameter \code{cooling.fraction.50} is the fraction by which we decrease the random walk standard deviation in 50 filtering iterations.)

\item Here, we did \Sexpr{bsflu_Nmif} \code{mif} iterations. Should we have done more? Could we have saved ourselves computational effort by doing less, without compromising our analysis?

\item Some parameter estimates show strong agreement between the different mif runs from different starting values. Others less so. How do you interpret this? Diversity in parameter estimates could be a signal of poor numerical maximization. It could signal a multi-modal likelhood surface. Or, it could simply correspond to a flat likelihood surface where the maximum is not precisely identifiable. Can we tell from the diagnostic plots which of these is going on here?

\end{enumerate}

\end{frame}

\begin{frame}

\frametitle{Effective sample size}

\bi
\item Maximization via particle filtering requires that the particle filter is working effectively. 
One way to monitor this is to pay attention to the \myemph{effective sample size} on the last filtering iteration. 

\item The effective sample size (ESS) is computed as
$$ \mathrm{ESS}_{n}= \frac{\left(\sum_{j=1}^J w_{n,j}\right)^2}{\sum_{j=1}^J w_{n,j}^2},$$
where $\{w_{n,j}\}$ are the weights defined in step 3 of the particle filter pseudo code.

\item
The ESS approximates the number of independent, equally weighted, samples from the filtering distribution that would be equally informative to the one weighted sample that we have obtained by the particle filter. 

\item For our example, do you have any concerns about the number of particles?

\ei

\end{frame} 

\begin{frame}[fragile]

\myquestion. {\bf Constructing a profile likelihood}. How strong is the evidence about the contact rate, $\beta$, given this model and data? Use \code{mif2} to construct a profile likelihood. Due to time constraints, you may be able to compute only a preliminary version.

\answer{\vspace{30mm}}{todo}

\bi
\item How would you profile over the basic reproduction number, $R_0=\beta P/\mu_{IR}$. 
Is this more or less well determined that $\beta$ for this model and data?
\ei

\end{frame}  


\begin{frame}[fragile]
\frametitle{Checking model source code}

It is surprisingly hard to ensure that written equations and code are perfectly matched. Here are some things to think about:

\begin{enumerate}

\item Papers should be written to be readable to as broad a community as possible. Code must be written to run successfully. People do not want to clutter papers with numerical details which they hope and belief are scientifically irrelevant. What problems can arise due to this, and what solutions are available?

\item Suppose that there is an error in the coding of \code{rprocess}. Suppose that plug-and-play statistical methodology is used to infer parameters.  A conscientious researcher carries out a simulation study, using \code{simulate} to generate some realizations from the fitted model and checking that the inference methodology can successfully recover the known parameters for this model, up to some statistical error. Will this procedure help to identify the error in \code{rprocess}? If not, how does one debug \code{rprocess}? What research practices help minimize the risk of errors in simulation code?

\end{enumerate}

\end{frame}  

 \begin{frame}[fragile]

\myquestion. {\bf Finding sharp peaks in the likelihood surface}. Even in this small, 3 parameter, example, it takes a considerable amount of computation to find the global maximum (with values of $\beta$ around 0.004) starting from uniform draws in the specified box. The problem is that, on the scale on which ``uniform'' is defined, the peak around $\beta\approx 0.004$ is very narrow. Propose and test a more favorable way to draw starting parameters for the global search, with better scale invariance properties.

\answer{\vspace{50mm}}{todo}

\end{frame}   

\begin{frame}[fragile]

\myquestion. {\bf Adding a latent class}. Modify the model to include a latent period between becoming exposed and becoming infectious. See what effect this has on the maximized likelihood.

\answer{\vspace{50mm}}{todo}

\end{frame}

\begin{frame}[fragile]
\frametitle{Acknowledgments and License}

\bi
\item Produced with R version \Sexpr{ getRversion()} and \package{pomp} version \Sexpr{ packageVersion("pomp")}.

\item These notes build on previous versions at \url{ionides.github.io/531w16} and \url{ionides.github.io/531w18}. 
\item Those notes draw on material developed for a short course on Simulation-based Inference for Epidemiological Dynamics (\url{http://kingaa.github.io/sbied/}) by Aaron King and Edward Ionides, taught at the University of Washington Summer Institute in Statistics and Modeling in Infectious Diseases, from 2015 through 2019.
\item
Licensed under the Creative Commons attribution-noncommercial license, \url{http://creativecommons.org/licenses/by-nc/3.0/}.
Please share and remix noncommercially, mentioning its origin.  
\includegraphics[width=2cm]{cc-by-nc.png}
\ei

\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliography{notes12.bib}
\end{frame}

\end{document}
