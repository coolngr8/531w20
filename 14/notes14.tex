%\documentclass[handout]{beamer}
\documentclass{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{natbib}
\bibliographystyle{dcu}
\input{../header.tex}

\newcommand\CHAPTER{14}

\setbeamertemplate{footline}[frame number]

\newcommand\eqspace{\quad\quad\quad}
\newcommand\eqskip{\vspace{2.5mm}}

\newcommand\dist[2]{\mathrm{#1}\left(#2\right)}
\newcommand\dlta{\Delta}

\newcommand\myeq{\hspace{10mm}}

% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{{\color{blue}{#2}}} % to show answers
\newcommand\answer[2]{#1} % to show blank space



\usepackage{bbm} % for blackboard bold 1
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

% knitr set up









\begin{frame}[fragile]
\frametitle{Chapter \CHAPTER. Case study: POMP modeling to investigate financial volatility}

\hspace{3cm} {\large \bf Objectives}

\vspace{3mm}

\begin{enumerate}

\item Introduce financial volatility and some models used to study it: ARCH, GARCH, and stochastic volatility models. 

\item Provide a case study in using \package{pomp} to study a stochastic volatility model.

\item Discuss this case study as an example of a broader class of nonlinear POMP models, derived from adding stochastically time varying parameters to a linear or nonlinear time series model.

\item Discuss this case study as an example of an extension of the POMP framework which allows feedback from the observation process to the state process.

\end{enumerate}

\end{frame}

\begin{frame}[fragile]

\frametitle{Introduction}

\bi

\item Returns on investments in stock market indices or large companies are often found to be approximately uncorrelated. 

\item If investment returns are substantially correlated, investors can study their time series behavior and make money. 

\item If the investment is non-liquid (i.e., not reliably tradeable), or expensive to trade, then it might be hard to make money even if you can statistically predict a positive expected return.

\item Otherwise, the market may notice a favorable investment opportunity. More buyers will lead to higher prices, and the opportunity will disappear.

\item Consequently, most readily traded investments (e.g., stock market indices, or stock of large companies) have close to uncorrelated returns.

\item The variability of the returns (called the volatility) can fluctuate considerably. Understanding this volatility is important for quantifying and managing the risk of investments. 

\ei

\end{frame}

\begin{frame}[fragile]

\bi

\item Recall the daily S\&P 500 data that we saw earlier, in Chapter 3.

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dat} \hlkwb{<-} \hlkwd{read.table}\hlstd{(}\hlstr{"sp500.csv"}\hlstd{,}\hlkwc{sep}\hlstd{=}\hlstr{","}\hlstd{,}\hlkwc{header}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlkwd{plot}\hlstd{(}\hlkwd{as.Date}\hlstd{(dat}\hlopt{$}\hlstd{Date),dat}\hlopt{$}\hlstd{Close,}
  \hlkwc{xlab}\hlstd{=}\hlstr{"date"}\hlstd{,}\hlkwc{ylab}\hlstd{=}\hlstr{"S&P 500"}\hlstd{,}\hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{)}
\hlkwd{plot}\hlstd{(}\hlkwd{as.Date}\hlstd{(dat}\hlopt{$}\hlstd{Date),dat}\hlopt{$}\hlstd{Close,} \hlkwc{log}\hlstd{=}\hlstr{"y"}\hlstd{,}
  \hlkwc{xlab}\hlstd{=}\hlstr{"date"}\hlstd{,}\hlkwc{ylab}\hlstd{=}\hlstr{"S&P 500"}\hlstd{,}\hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\vspace{-2mm}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=10cm]{tmp/figuresp500_plot-1} 

}



\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\frametitle{Returns, absolute returns, and autocorrelation}

\bi

\item We write $\{\data{z}_n,n=1,\dots,N\}$ for the data.

\item We write the return on the S\&P 500 index, i.e., the difference of the log of the index, as
$$ \data{y_n}=\log(\data{z_n})-\log(\data{z_{n-1}}).$$

* We saw in Chapter 3 that $\data{y_{2:N}}$ has negligible sample autocorrelation.

\item However, the absolute deviations from average, 
$$ \data{a_n} = \left| \data{y_n} - \frac{1}{N-1}\sum_{k=2}^N \data{y_k} \right|$$
have considerable sample autocorrelation.

\ei

\end{frame}

\begin{frame}[fragile]




\bi

\item We fit models to the demeaned daily returns for the S\&P 500 index for 2002-2012, to compare with \citet{breto14}. 

\vspace{-2mm}

\ei
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{load}\hlstd{(}\hlkwc{file}\hlstd{=}\hlstr{"sp500-2002-2012.rda"}\hlstd{)}
\hlkwd{plot}\hlstd{(sp500.ret.demeaned,} \hlkwc{type}\hlstd{=}\hlstr{"l"}\hlstd{,}
  \hlkwc{xlab}\hlstd{=}\hlstr{"business day (2002-2012)"}\hlstd{,}\hlkwc{ylab}\hlstd{=}\hlstr{"demeaned S&P 500 return"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\vspace{-2mm}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=8cm]{tmp/figuredata_rda_plot-1} 

}



\end{knitrout}

\myquestion. Is it appropriate to fit a stationary model to this series, or do we have evidence for violation of stationarity? Explain.

\answer{\vspace{30mm}}{todo}

\end{frame}


\begin{frame}[fragile]

\frametitle{The ARCH model}

\bi

\item The ARCH and GARCH models have become widely used for financial time series modeling. Here, we follow \citet{cowpertwait09} to introduce these models; see also Section 5.4 of Shumway and Stoffer (3rd edition). 

\item An order $p$ \myemph{autoregressive conditional heteroskedasticity} model, known as ARCH(p), has the form
$$ Y_n = \epsilon_n \sqrt{V_n},$$
where $\epsilon_{1:N}$ is white noise and
$$ V_n = \alpha_0 + \sum_{j=1}^p \alpha_j Y_{n-j}^2.$$

\item If $\epsilon_{1:N}$ is Gaussian, then $Y_{1:N}$ is called a Gaussian ARCH(p). Note, however, that a Gaussian ARCH model is not a Gaussian process, just a process driven by Gaussian noise.

\item If  $Y_{1:N}$ is a Gaussian ARCH(p), then  $Y_{1:N}^2$ is AR(p), but not Gaussian AR(p).

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{The GARCH model}

\bi

\item The \myemph{generalized ARCH} model, known as GARCH(p,q), has the form
$$ Y_n = \epsilon_n \sqrt{V_n},$$
where
$$ V_n = \alpha_0 + \sum_{j=1}^p \alpha_j Y_{n-j}^2 + \sum_{k=1}^q \beta_k V_{n-k}$$
and $\epsilon_{1:N}$ is white noise.


\item The GARCH(1.1) model is a popular choice \citep{cowpertwait09} which can be fitted using \code{garch()} in the \code{tseries} R package.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Fitting a GARCH model}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(tseries)}
\hlstd{fit.garch} \hlkwb{<-} \hlkwd{garch}\hlstd{(sp500.ret.demeaned,}\hlkwc{grad} \hlstd{=} \hlstr{"numerical"}\hlstd{,} \hlkwc{trace} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlstd{L.garch} \hlkwb{<-} \hlstd{tseries}\hlopt{:::}\hlkwd{logLik.garch}\hlstd{(fit.garch)}
\end{alltt}
\end{kframe}
\end{knitrout}

\bi

\item This 3-parameter model has a maximized log likelihood of $-4019.7$.

\item It appears that a bug in this version of \code{tseries} means that simply \code{logLik(fit.garch)} does not work.

\item From \code{?garch} we learn this is actually a conditional log likelihood given the first $\max(p,q)$ values.

\ei

\vspace{3mm}

\myquestion. It is usually inappropriate to present numerical results to five significant figures. Is this appropriate for reporting the log likelihood here? Why?

\answer{\vspace{30mm}}{todo}

\end{frame}

\begin{frame}[fragile]

\bi

\item We are now in a position to employ the framework of likelihood-based inference for GARCH models. In particular, profile likelihood, likelihood ratio tests, and AIC are available.

\item We can readily simulate from a fitted GARCH model, if we want to investigate properties of a fitted model that we don't know how to compute analytically.

\item However, GARCH is a black-box model, in the sense that the parameters don't have clear interpretation. We can develop an appropriate GARCH(p,q) model, and that may be useful for forecasting, but it won't help us understand more about how financial markets work. 

\item To develop and test a hypothesis that goes beyond the class of GARCH models, it is useful to have the POMP framework available.

\ei

\end{frame}


\begin{frame}[fragile]

\frametitle{Financial leverage}

\bi

\item It is a fairly well established empirical observation that negative shocks to a stockmarket index are associated with a subsequent increase in volatility. 

\item This phenomenon is called \myemph{leverage}.

\item Here, we formally define leverage, $R_n$ on day $n$ as the correlation between index return on day $n-1$ and the increase in the log volatility from day $n-1$ to day $n$.

\item Models have been proposed which incorporate leverage into the dynamics \citep{breto14}.

\item We present a pomp implementation of @breto14, which models $R_n$ as a random walk on a transformed scale,
$$R_n= \frac{\exp\{2G_n\} -1}{\exp\{2G_n\}+1},$$
where $\{G_n\}$ is the usual, Gaussian random walk.

\ei

\end{frame}

\begin{frame}[fragile]


\frametitle{Time-varying parameters}

\bi

\item A special case of this model, with the Gaussian random walk having standard deviation zero, is a fixed leverage model.

\item The POMP framework provides a general approach to time-varying parameters. Considering a parameter as a latent, unobserved random process that can progressively change its value over time (following a random walk, or some other stochastic process) leads to a POMP model. The resulting POMP model is usually nonlinear.

\item Many real-world systems are non-stationary and could be investigated using models with time-varying parameters. 

\item Some of the midterm projects discovered examples of this.

\item This is one possible thing to explore in your final project.

\ei

\end{frame}

\begin{frame}[fragile]

\bi

\item Following the notation and model representation in equation (4) of \citet{breto14}, we propose a model,
\begin{align} 
Y_n &= \exp\{H_n/2\} \epsilon_n, \\
H_n &= \mu_h(1-\phi) + \phi H_{n-1} +
\beta_{n-1}R_n\exp\{-H_{n-1}/2\} + \omega_n,\\
G_n &= G_{n-1}+\nu_n,
\end{align}
where $\beta_n=Y_n\sigma_\eta\sqrt{1-\phi^2}$, $\{\epsilon_n\}$ is an iid $N(0,1)$ sequence, $\{\nu_n\}$ is an iid $N(0,\sigma_{\nu}^2)$ sequence, and $\{\omega_n\}$ is an iid $N(0,\sigma_\omega^2)$ sequence.

\item Here, $H_n$ is the log volatility.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Building a POMP model}

\vspace{-2mm}

\bi

\item A complication is that transitions of the latent variables from $(G_n,H_n)$ to $(G_{n+1},H_{n+1})$ depends on the observable variable $Y_{n}$. 

\item Formally, therefore, we use the state variable $X_n=(G_{n},H_{n},Y_{n})$ and model the measurement process as a perfect observation of the $Y_n$ component of the state space. 

\item To define a recursive particle filter, we write the filtered particle $j$ at time $n-1$ as 
$$ X^F_{n-1,j}=(G^F_{n-1,j},H^F_{n-1,j},\data{y}_{n-1}).$$

\item Now we can construct prediction particles at time $n$,
$$(G^P_{n,j},H^P_{n,j})\sim f_{G_{n},H_n|G_{n-1},H_{n-1},Y_{n-1}}(g_n|G^F_{n-1,j},H^F_{n-1,j},\data{y}_{n-1})$$
with corresponding weight 
$$w_{n,j}=f_{Y_n|G_n,H_n}(\data{y}_n|G^P_{n,j},H^P_{n,j}).$$

\item Resampling with probability proportional to these weights gives an SMC representation of the filtering distribution at time $n$.

\item A derivation of this is given as an Appendix. 


\ei


\end{frame}

\begin{frame}[fragile]

\bi

\item We can coerce the basic sequential Monte Carlo algorithm, implemented as \code{pfilter} in \package{pomp}, into carrying out this calculation by building two different \code{pomp} objects, one to do filtering and another to do simulation.

\item For the implementation in \package{pomp}, we proceed to write Csnippet code for the two versions of \code{rprocess}. 

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sp500_statenames} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"H"}\hlstd{,}\hlstr{"G"}\hlstd{,}\hlstr{"Y_state"}\hlstd{)}
\hlstd{sp500_rp_names} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"sigma_nu"}\hlstd{,}\hlstr{"mu_h"}\hlstd{,}\hlstr{"phi"}\hlstd{,}\hlstr{"sigma_eta"}\hlstd{)}
\hlstd{sp500_ivp_names} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"G_0"}\hlstd{,}\hlstr{"H_0"}\hlstd{)}
\hlstd{sp500_paramnames} \hlkwb{<-} \hlkwd{c}\hlstd{(sp500_rp_names,sp500_ivp_names)}
\end{alltt}
\end{kframe}
\end{knitrout}


\end{frame}

\begin{frame}[fragile]

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{rproc1} \hlkwb{<-} \hlstr{"
  double beta,omega,nu;
  omega = rnorm(0,sigma_eta * sqrt( 1- phi*phi ) * sqrt(1-tanh(G)*tanh(G)));
  nu = rnorm(0, sigma_nu);
  G += nu;
  beta = Y_state * sigma_eta * sqrt( 1- phi*phi );
  H = mu_h*(1 - phi) + phi*H + beta * tanh( G ) * exp(-H/2) + omega;
"}
\hlstd{rproc2.sim} \hlkwb{<-} \hlstr{"
  Y_state = rnorm( 0,exp(H/2) );
 "}

\hlstd{rproc2.filt} \hlkwb{<-} \hlstr{"
  Y_state = covaryt;
 "}
\hlstd{sp500_rproc.sim} \hlkwb{<-} \hlkwd{paste}\hlstd{(rproc1,rproc2.sim)}
\hlstd{sp500_rproc.filt} \hlkwb{<-} \hlkwd{paste}\hlstd{(rproc1,rproc2.filt)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sp500_rinit} \hlkwb{<-} \hlstr{"
  G = G_0;
  H = H_0;
  Y_state = rnorm( 0,exp(H/2) );
"}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sp500_rmeasure} \hlkwb{<-} \hlstr{"
   y=Y_state;
"}

\hlstd{sp500_dmeasure} \hlkwb{<-} \hlstr{"
   lik=dnorm(y,0,exp(H/2),give_log);
"}
\end{alltt}
\end{kframe}
\end{knitrout}


\end{frame}

\begin{frame}[fragile]

\frametitle{Parameter transformations}

\bi

\item For optimization procedures such as iterated filtering, it is convenient to transform parameters to be defined on the whole real line. 

\item We therefore write transformation functions for $\sigma_\eta$, $\sigma_\nu$ and $\phi$,

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sp500_partrans} \hlkwb{<-} \hlkwd{parameter_trans}\hlstd{(}
  \hlkwc{log}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"sigma_eta"}\hlstd{,}\hlstr{"sigma_nu"}\hlstd{),}
  \hlkwc{logit}\hlstd{=}\hlstr{"phi"}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\bi

\item We can now build a pomp object suitable for filtering, and parameter estimation by iterated filtering or particle MCMC. 

\item Note that the data are also placed in a covariate slot. 

\item This is a devise to allow the state process evolution to depend on the data. In a POMP model, the latent process evolution depends only on the current latent state. In \package{pomp}, the consequence of this structure is that \code{rprocess} doesn't have access to the observation process. 

\item However, a POMP model does allow for the possibility for the basic elements to depend on arbitrary covariates. In \package{pomp}, this means \code{rprocess} has access to a covariate slot.

\item The code below gives an example of how to fill the covariate slot and how to use it in \code{rprocess}.


\ei

\end{frame}

\begin{frame}[fragile]

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sp500.filt} \hlkwb{<-} \hlkwd{pomp}\hlstd{(}\hlkwc{data}\hlstd{=}\hlkwd{data.frame}\hlstd{(}
    \hlkwc{y}\hlstd{=sp500.ret.demeaned,}\hlkwc{time}\hlstd{=}\hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(sp500.ret.demeaned)),}
  \hlkwc{statenames}\hlstd{=sp500_statenames,}
  \hlkwc{paramnames}\hlstd{=sp500_paramnames,}
  \hlkwc{times}\hlstd{=}\hlstr{"time"}\hlstd{,}
  \hlkwc{t0}\hlstd{=}\hlnum{0}\hlstd{,}
  \hlkwc{covar}\hlstd{=}\hlkwd{covariate_table}\hlstd{(}
    \hlkwc{time}\hlstd{=}\hlnum{0}\hlopt{:}\hlkwd{length}\hlstd{(sp500.ret.demeaned),}
    \hlkwc{covaryt}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,sp500.ret.demeaned),}
    \hlkwc{times}\hlstd{=}\hlstr{"time"}\hlstd{),}
  \hlkwc{rmeasure}\hlstd{=}\hlkwd{Csnippet}\hlstd{(sp500_rmeasure),}
  \hlkwc{dmeasure}\hlstd{=}\hlkwd{Csnippet}\hlstd{(sp500_dmeasure),}
  \hlkwc{rprocess}\hlstd{=}\hlkwd{discrete_time}\hlstd{(}\hlkwc{step.fun}\hlstd{=}\hlkwd{Csnippet}\hlstd{(sp500_rproc.filt),}\hlkwc{delta.t}\hlstd{=}\hlnum{1}\hlstd{),}
  \hlkwc{rinit}\hlstd{=}\hlkwd{Csnippet}\hlstd{(sp500_rinit),}
  \hlkwc{partrans}\hlstd{=sp500_partrans}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\bi

\item Simulating from the model is convenient for developing and testing the code, as well as to investigate a fitted model:

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{params_test} \hlkwb{<-} \hlkwd{c}\hlstd{(}
  \hlkwc{sigma_nu} \hlstd{=} \hlkwd{exp}\hlstd{(}\hlopt{-}\hlnum{4.5}\hlstd{),}
  \hlkwc{mu_h} \hlstd{=} \hlopt{-}\hlnum{0.25}\hlstd{,}
  \hlkwc{phi} \hlstd{=} \hlkwd{expit}\hlstd{(}\hlnum{4}\hlstd{),}
  \hlkwc{sigma_eta} \hlstd{=} \hlkwd{exp}\hlstd{(}\hlopt{-}\hlnum{0.07}\hlstd{),}
  \hlkwc{G_0} \hlstd{=} \hlnum{0}\hlstd{,}
  \hlkwc{H_0}\hlstd{=}\hlnum{0}
\hlstd{)}

\hlstd{sim1.sim} \hlkwb{<-} \hlkwd{pomp}\hlstd{(sp500.filt,}
  \hlkwc{statenames}\hlstd{=sp500_statenames,}
  \hlkwc{paramnames}\hlstd{=sp500_paramnames,}
  \hlkwc{rprocess}\hlstd{=}\hlkwd{discrete_time}\hlstd{(}
    \hlkwc{step.fun}\hlstd{=}\hlkwd{Csnippet}\hlstd{(sp500_rproc.sim),}\hlkwc{delta.t}\hlstd{=}\hlnum{1}\hlstd{)}
\hlstd{)}

\hlstd{sim1.sim} \hlkwb{<-} \hlkwd{simulate}\hlstd{(sim1.sim,}\hlkwc{seed}\hlstd{=}\hlnum{1}\hlstd{,}\hlkwc{params}\hlstd{=params_test)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\bi

\item Now. to build the filtering object from \code{sim1.sim}, we need to copy the new simulated data into the covariate slot, and put back the appropriate version of \code{rprocess}.


\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sim1.filt} \hlkwb{<-} \hlkwd{pomp}\hlstd{(sim1.sim,}
  \hlkwc{covar}\hlstd{=}\hlkwd{covariate_table}\hlstd{(}
    \hlkwc{time}\hlstd{=}\hlkwd{c}\hlstd{(}\hlkwd{timezero}\hlstd{(sim1.sim),}\hlkwd{time}\hlstd{(sim1.sim)),}
    \hlkwc{covaryt}\hlstd{=}\hlkwd{c}\hlstd{(}\hlkwd{obs}\hlstd{(sim1.sim),}\hlnum{NA}\hlstd{),}
    \hlkwc{times}\hlstd{=}\hlstr{"time"}\hlstd{),}
  \hlkwc{statenames}\hlstd{=sp500_statenames,}
  \hlkwc{paramnames}\hlstd{=sp500_paramnames,}
  \hlkwc{rprocess}\hlstd{=}\hlkwd{discrete_time}\hlstd{(}
    \hlkwc{step.fun}\hlstd{=}\hlkwd{Csnippet}\hlstd{(sp500_rproc.filt),}\hlkwc{delta.t}\hlstd{=}\hlnum{1}\hlstd{)}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\frametitle{Filtering on simulated data}

\bi

\item Let's check that we can indeed filter and re-estimate parameters successfully for this simulated data.  

\item As in previous case studies, we set up different run levels:

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{run_level} \hlkwb{<-} \hlnum{2}
\hlstd{sp500_Np} \hlkwb{<-}           \hlkwd{switch}\hlstd{(run_level,}\hlnum{100}\hlstd{,}\hlnum{1e3}\hlstd{,}\hlnum{2e3}\hlstd{)}
\hlstd{sp500_Nmif} \hlkwb{<-}         \hlkwd{switch}\hlstd{(run_level,}\hlnum{10}\hlstd{,} \hlnum{100}\hlstd{,}\hlnum{200}\hlstd{)}
\hlstd{sp500_Nreps_eval} \hlkwb{<-}   \hlkwd{switch}\hlstd{(run_level,}\hlnum{4}\hlstd{,}  \hlnum{10}\hlstd{,}  \hlnum{20}\hlstd{)}
\hlstd{sp500_Nreps_local} \hlkwb{<-}  \hlkwd{switch}\hlstd{(run_level,}\hlnum{10}\hlstd{,} \hlnum{20}\hlstd{,} \hlnum{20}\hlstd{)}
\hlstd{sp500_Nreps_global} \hlkwb{<-} \hlkwd{switch}\hlstd{(run_level,}\hlnum{10}\hlstd{,} \hlnum{20}\hlstd{,} \hlnum{100}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\bi

\item We carry out replications in parallel to assess Monte Carlo error.

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(doParallel)}
\hlkwd{registerDoParallel}\hlstd{()}
\hlkwd{library}\hlstd{(doRNG)}
\hlkwd{registerDoRNG}\hlstd{(}\hlnum{34118892}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{stew}\hlstd{(}\hlkwc{file}\hlstd{=}\hlkwd{sprintf}\hlstd{(}\hlstr{"pf1-%d.rda"}\hlstd{,run_level),\{}
  \hlstd{t.pf1} \hlkwb{<-} \hlkwd{system.time}\hlstd{(}
    \hlstd{pf1} \hlkwb{<-} \hlkwd{foreach}\hlstd{(}\hlkwc{i}\hlstd{=}\hlnum{1}\hlopt{:}\hlstd{sp500_Nreps_eval,}
      \hlkwc{.packages}\hlstd{=}\hlstr{'pomp'}\hlstd{)} \hlopt{%dopar%} \hlkwd{pfilter}\hlstd{(sim1.filt,}\hlkwc{Np}\hlstd{=sp500_Np))}
\hlstd{\},}\hlkwc{seed}\hlstd{=}\hlnum{493536993}\hlstd{,}\hlkwc{kind}\hlstd{=}\hlstr{"L'Ecuyer"}\hlstd{)}
\hlstd{(L.pf1} \hlkwb{<-} \hlkwd{logmeanexp}\hlstd{(}\hlkwd{sapply}\hlstd{(pf1,logLik),}\hlkwc{se}\hlstd{=}\hlnum{TRUE}\hlstd{))}
\end{alltt}
\begin{verbatim}
##                          se 
## -3658.8574140     0.1938912
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\bi

\item In  10.5 seconds, we obtain a log likelihood estimate of -3658.86 with a Monte standard error of 0.19. Notice that the replications are averaged using the \code{logmeanexp} function, since the likelihood estimate is unbiased on the natural scale but not the log scale.

\item We could test the numerical performance of an iterated filtering likelihood maximization algorithm on simulated data. 

\item We could also study the statistical performance of maximum likelihood estimators and profile likelihood confidence intervals on simulated data. 

\item However, here we are going to cut to the chase and start fitting models to data. 

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Fitting the stochastic leverage model to S\&P500 data}

\bi

\item We are now ready to try out iterated filtering on the S\&P500 data. We will use the IF2 algorithm of @ionides15, implemented by \code{mif2}.

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sp500_rw.sd_rp} \hlkwb{<-} \hlnum{0.02}
\hlstd{sp500_rw.sd_ivp} \hlkwb{<-} \hlnum{0.1}
\hlstd{sp500_cooling.fraction.50} \hlkwb{<-} \hlnum{0.5}
\hlstd{sp500_rw.sd} \hlkwb{<-} \hlkwd{rw.sd}\hlstd{(}
  \hlkwc{sigma_nu}  \hlstd{= sp500_rw.sd_rp,}
  \hlkwc{mu_h}      \hlstd{= sp500_rw.sd_rp,}
  \hlkwc{phi}       \hlstd{= sp500_rw.sd_rp,}
  \hlkwc{sigma_eta} \hlstd{= sp500_rw.sd_rp,}
  \hlkwc{G_0}       \hlstd{=} \hlkwd{ivp}\hlstd{(sp500_rw.sd_ivp),}
  \hlkwc{H_0}       \hlstd{=} \hlkwd{ivp}\hlstd{(sp500_rw.sd_ivp)}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]
\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{stew}\hlstd{(}\hlkwc{file}\hlstd{=}\hlkwd{sprintf}\hlstd{(}\hlstr{"mif1-%d.rda"}\hlstd{,run_level),\{}
  \hlstd{t.if1} \hlkwb{<-} \hlkwd{system.time}\hlstd{(\{}
  \hlstd{if1} \hlkwb{<-} \hlkwd{foreach}\hlstd{(}\hlkwc{i}\hlstd{=}\hlnum{1}\hlopt{:}\hlstd{sp500_Nreps_local,}
    \hlkwc{.packages}\hlstd{=}\hlstr{'pomp'}\hlstd{,} \hlkwc{.combine}\hlstd{=c)} \hlopt{%dopar%} \hlkwd{mif2}\hlstd{(sp500.filt,}
      \hlkwc{params}\hlstd{=params_test,}
      \hlkwc{Np}\hlstd{=sp500_Np,}
      \hlkwc{Nmif}\hlstd{=sp500_Nmif,}
      \hlkwc{cooling.fraction.50}\hlstd{=sp500_cooling.fraction.50,}
      \hlkwc{rw.sd} \hlstd{= sp500_rw.sd)}
  \hlstd{L.if1} \hlkwb{<-} \hlkwd{foreach}\hlstd{(}\hlkwc{i}\hlstd{=}\hlnum{1}\hlopt{:}\hlstd{sp500_Nreps_local,}
    \hlkwc{.packages}\hlstd{=}\hlstr{'pomp'}\hlstd{,} \hlkwc{.combine}\hlstd{=rbind)} \hlopt{%dopar%} \hlkwd{logmeanexp}\hlstd{(}
      \hlkwd{replicate}\hlstd{(sp500_Nreps_eval,} \hlkwd{logLik}\hlstd{(}\hlkwd{pfilter}\hlstd{(sp500.filt,}
        \hlkwc{params}\hlstd{=}\hlkwd{coef}\hlstd{(if1[[i]]),}\hlkwc{Np}\hlstd{=sp500_Np))),} \hlkwc{se}\hlstd{=}\hlnum{TRUE}\hlstd{)}
  \hlstd{\})}
\hlstd{\},}\hlkwc{seed}\hlstd{=}\hlnum{318817883}\hlstd{,}\hlkwc{kind}\hlstd{=}\hlstr{"L'Ecuyer"}\hlstd{)}

\hlstd{r.if1} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{logLik}\hlstd{=L.if1[,}\hlnum{1}\hlstd{],}\hlkwc{logLik_se}\hlstd{=L.if1[,}\hlnum{2}\hlstd{],}\hlkwd{t}\hlstd{(}\hlkwd{sapply}\hlstd{(if1,coef)))}
\hlkwa{if} \hlstd{(run_level}\hlopt{>}\hlnum{1}\hlstd{)} \hlkwd{write.table}\hlstd{(r.if1,}\hlkwc{file}\hlstd{=}\hlstr{"sp500_params.csv"}\hlstd{,}
  \hlkwc{append}\hlstd{=}\hlnum{TRUE}\hlstd{,}\hlkwc{col.names}\hlstd{=}\hlnum{FALSE}\hlstd{,}\hlkwc{row.names}\hlstd{=}\hlnum{FALSE}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\bi

\item This investigation took  189 minutes. 

\item The repeated stochastic maximizations can also show us the geometry of the likelihood surface in a neighborhood of this point estimate:

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pairs}\hlstd{(}\hlopt{~}\hlstd{logLik}\hlopt{+}\hlstd{sigma_nu}\hlopt{+}\hlstd{mu_h}\hlopt{+}\hlstd{phi}\hlopt{+}\hlstd{sigma_eta,}
  \hlkwc{data}\hlstd{=}\hlkwd{subset}\hlstd{(r.if1,logLik}\hlopt{>}\hlkwd{max}\hlstd{(logLik)}\hlopt{-}\hlnum{20}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

\vspace{-5mm}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=11cm]{tmp/figurepairs_plot-1} 

}



\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\frametitle{Likelihood maximization using randomized starting values}

\bi

\item As for our other case studies, carrying out searches starting randomly throughout a large box can lead to reasonble evidence for successful global maximization.

\item For our volatility model, a box containing plausible parameter values might be

\ei

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{sp500_box} \hlkwb{<-} \hlkwd{rbind}\hlstd{(}
 \hlkwc{sigma_nu}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0.005}\hlstd{,}\hlnum{0.05}\hlstd{),}
 \hlkwc{mu_h}    \hlstd{=}\hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{1}\hlstd{,}\hlnum{0}\hlstd{),}
 \hlkwc{phi} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.95}\hlstd{,}\hlnum{0.99}\hlstd{),}
 \hlkwc{sigma_eta} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0.5}\hlstd{,}\hlnum{1}\hlstd{),}
 \hlkwc{G_0} \hlstd{=} \hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{),}
 \hlkwc{H_0} \hlstd{=} \hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{)}
\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{stew}\hlstd{(}\hlkwc{file}\hlstd{=}\hlkwd{sprintf}\hlstd{(}\hlstr{"box_eval-%d.rda"}\hlstd{,run_level),\{}
  \hlstd{t.box} \hlkwb{<-} \hlkwd{system.time}\hlstd{(\{}
    \hlstd{if.box} \hlkwb{<-} \hlkwd{foreach}\hlstd{(}\hlkwc{i}\hlstd{=}\hlnum{1}\hlopt{:}\hlstd{sp500_Nreps_global,}
      \hlkwc{.packages}\hlstd{=}\hlstr{'pomp'}\hlstd{,}\hlkwc{.combine}\hlstd{=c)} \hlopt{%dopar%} \hlkwd{mif2}\hlstd{(if1[[}\hlnum{1}\hlstd{]],}
        \hlkwc{params}\hlstd{=}\hlkwd{apply}\hlstd{(sp500_box,}\hlnum{1}\hlstd{,}\hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{)}\hlkwd{runif}\hlstd{(}\hlnum{1}\hlstd{,x)))}
    \hlstd{L.box} \hlkwb{<-} \hlkwd{foreach}\hlstd{(}\hlkwc{i}\hlstd{=}\hlnum{1}\hlopt{:}\hlstd{sp500_Nreps_global,}
      \hlkwc{.packages}\hlstd{=}\hlstr{'pomp'}\hlstd{,}\hlkwc{.combine}\hlstd{=rbind)} \hlopt{%dopar%} \hlstd{\{}
         \hlkwd{logmeanexp}\hlstd{(}\hlkwd{replicate}\hlstd{(sp500_Nreps_eval,} \hlkwd{logLik}\hlstd{(}\hlkwd{pfilter}\hlstd{(}
             \hlstd{sp500.filt,}\hlkwc{params}\hlstd{=}\hlkwd{coef}\hlstd{(if.box[[i]]),}\hlkwc{Np}\hlstd{=sp500_Np))),}
           \hlkwc{se}\hlstd{=}\hlnum{TRUE}\hlstd{)}
       \hlstd{\}}
  \hlstd{\})}
\hlstd{\},}\hlkwc{seed}\hlstd{=}\hlnum{290860873}\hlstd{,}\hlkwc{kind}\hlstd{=}\hlstr{"L'Ecuyer"}\hlstd{)}

\hlstd{r.box} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{logLik}\hlstd{=L.box[,}\hlnum{1}\hlstd{],}\hlkwc{logLik_se}\hlstd{=L.box[,}\hlnum{2}\hlstd{],}
  \hlkwd{t}\hlstd{(}\hlkwd{sapply}\hlstd{(if.box,coef)))}
\hlkwa{if}\hlstd{(run_level}\hlopt{>}\hlnum{1}\hlstd{)} \hlkwd{write.table}\hlstd{(r.box,}\hlkwc{file}\hlstd{=}\hlstr{"sp500_params.csv"}\hlstd{,}
  \hlkwc{append}\hlstd{=}\hlnum{TRUE}\hlstd{,}\hlkwc{col.names}\hlstd{=}\hlnum{FALSE}\hlstd{,}\hlkwc{row.names}\hlstd{=}\hlnum{FALSE}\hlstd{)}
\hlkwd{summary}\hlstd{(r.box}\hlopt{$}\hlstd{logLik,}\hlkwc{digits}\hlstd{=}\hlnum{5}\hlstd{)}
\end{alltt}
\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   -3998   -3986   -3964   -3971   -3960   -3955
\end{verbatim}
\end{kframe}
\end{knitrout}

\end{frame}

\begin{frame}[fragile]

\bi

\item This search took  117.9 minutes. 

\item The best likelihood found was -3954.9 with a standard error of 0.4. 

\item We see that optimization attempts from diverse remote starting points can approach our MLE, but do not exceed it. This gives us some reasonable confidence in our MLE. 

\item Plotting these diverse parameter estimates can help to give a feel for the global geometry of the likelihood surface 

\ei

\end{frame}

\begin{frame}[fragile]

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pairs}\hlstd{(}\hlopt{~}\hlstd{logLik}\hlopt{+}\hlkwd{log}\hlstd{(sigma_nu)}\hlopt{+}\hlstd{mu_h}\hlopt{+}\hlstd{phi}\hlopt{+}\hlstd{sigma_eta}\hlopt{+}\hlstd{H_0,}
  \hlkwc{data}\hlstd{=}\hlkwd{subset}\hlstd{(r.box,logLik}\hlopt{>}\hlkwd{max}\hlstd{(logLik)}\hlopt{-}\hlnum{10}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

\vspace{-4mm}

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=11cm]{tmp/figurepairs_global_plot-1} 

}



\end{knitrout}
\end{frame}

\begin{frame}[fragile]

\bi

\item This preliminary analysis does not show clear evidence for the hypothesis that $\sigma_\nu > 0$. 

\item That is likely because we are studying only a subset of the 1988 to 2012 dataset analyzed by \citet{breto14}. 

\item Also, it might help to refine our inference be computing a likelihood profile over $\sigma_\nu$.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Benchmark likelihoods for alternative models}

\bi

\item To assess the overall success of the model, it is helpful to put the log likelihoods in the context of simpler models, called \myemph{benchmarks}.

\item Benchmarks provide a complementary approach to residual analysis and the investigation of simulations from the fitted model.





\item The GARCH(1,1) model for this dataset has a maximized likelihood of -4019.7 with 3 fitted parameters.

\item Our stochastic volatility model, with time-varying leverage, model has a maximized log likelihood of -3954.9 with 6 fitted parameters.  AIC favors the stochastic volatility model.

\item A model which both fits better and has meaningful interpretation has clear advantages over a simple statistical model. 

\item The disadvantage of the sophisticated modeling and inference is the extra effort required.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Can a mechanistic model be helpful if it loses to a non-mechanistic alternative?}

\bi

\item Sometimes, the mechanistic model does not beat simple benchmark models. That does not necessarily mean the mechanistic model is entirely useless. 

\item We may be able to learn about the system under investigation from what a scientifically interpretable model fails to explain.

\item We may be able to use preliminary results to improve the model, and subsequently beat the benchmarks.

\item If the mechanistic model fits disastrously compared to the benchmark, our model is probably missing something important. We must reconsider the model, based on clues we might obtain by carrying out residual analysis and looking at simulations from the fitted model.

\ei

\end{frame}

\begin{frame}[fragile]

\frametitle{Appendix: Proper weighting for a partially plug-and-play algorithm with a perfectly observed state space component}

\bi

\item Suppose a POMP model with latent variable $X_n=(U_n,V_n)$ and observed varaible $Y_n$ has conditional density $f_{Y_n|V_n}(y_n|v_n)$ depending only on $V_n$. 

\item The proper weight for an SMC proposal density $q_n(x_n|x_{n-1})$ is
$$ 
w_{n}(x_n|x_{n-1}) = \frac{f_{Y_n|X_n}(\data{y}_n|x_n)f_{X_n|X_{n-1}}(x_n|x_{n-1})}{q_n(x_n|x_{n-1})}.
$$

\item Consider the  proposal
$q_n(u_n,v_n|x_{n-1}) = f_{U_n|X_{n-1}}(u_n|x_{n-1}) g_n(v_n)$.
This is partially plug-and-play, in the sense that the $U_n$ part of the proposal is drawn from a simulator of the dynamic system. 

\item Computing the weights, we see that the transition density for the $U_n$ component cancels out and does not have to be computed, i.e.,
\ei
$$\begin{aligned}
w_{n}(x_n|x_{n-1}) &= \frac{f_{Y_n|V_n}(\data{y}_n|v_n)f_{U_n|X_{n-1}}(u_n|x_{n-1})f_{V_n|U_n,X_{n-1}}(v_n|u_n,x_{n-1})}{f_{U_n|X_{n-1}}(u_n|x_{n-1}) g_n(v_n)} \\
&= \frac{f_{Y_n|V_n}(\data{y}_n|v_n)f_{V_n|U_n,X_{n-1}}(v_n|u_n,x_{n-1})}{ g_n(v_n)}.
\end{aligned}
$$

\end{frame}

\begin{frame}[fragile]

\bi

\item Now consider the case where the $V_n$ component of the state space is perfectly observed, i.e., $Y_n=V_n$. In this case, 
$$ 
f_{Y_n|V_n}(y_n|v_n) = \delta(y_n-v_n),
$$
interpreted as a point mass at $v_n$ in the discrete case and a singular density at $v_n$ in the continuous case. 

\item We can choose $g_n(v_n)$ to depend on the data, and a natural choice is 
$$
g_n(v_n)=\delta(\data{y}_n-v_n),
$$
for which the proper weight is
$$
w_{n}(x_n|x_{n-1}) = f_{Y_n|U_n,X_{n-1}}(\data{y}_n|u_n,x_{n-1}).
$$

\item This is the situation in the context of our case study, with $U_n=(G_n,H_n)$ and $V_n=Y_n$.


\ei

\end{frame}  

\begin{frame}[fragile]
\frametitle{Acknowledgments and License}

\bi
\item Produced with R version 3.6.2 and \package{pomp} version 2.7.

\item These notes build on previous versions at \url{ionides.github.io/531w16} and \url{ionides.github.io/531w18}. 
\item
Licensed under the Creative Commons attribution-noncommercial license, \url{http://creativecommons.org/licenses/by-nc/3.0/}.
Please share and remix noncommercially, mentioning its origin.  
\includegraphics[width=2cm]{cc-by-nc.png}
\ei

\end{frame}


\begin{frame}[allowframebreaks]
\frametitle{References}
\bibliography{notes14.bib}
\end{frame}

\end{document}
